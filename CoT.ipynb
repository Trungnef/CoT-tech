{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import torch\n",
    "import google.generativeai as genai\n",
    "import gc\n",
    "from accelerate import dispatch_model\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"GEMINI_API_KEY\"\n",
    "genai.api_key=os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Qwen ==========\n",
    "qwen_model_path = r\"cache/model/Qwen_Qwen2.5-72B-Instruct/models--Qwen--Qwen2.5-72B-Instruct/snapshots/495f39366efef23836d0cfae4fbe635880d2be31\"\n",
    "qwen_tokenizer_path = r\"cache/tokenizer/Qwen_Qwen2.5-72B-Instruct/models--Qwen--Qwen2.5-72B-Instruct/snapshots/495f39366efef23836d0cfae4fbe635880d2be31\"\n",
    "\n",
    "# ========== Llama ==========\n",
    "llama_model_path = r\"cache/model/meta-llama_Llama-3.3-70B-Instruct/models--meta-llama--Llama-3.3-70B-Instruct/snapshots/6f6073b423013f6a7d4d9f39144961bfbfbc386b\"\n",
    "llama_tokenizer_path = r\"cache/tokenizer/meta-llama_Llama-3.3-70B-Instruct/models--meta-llama--Llama-3.3-70B-Instruct/snapshots/6f6073b423013f6a7d4d9f39144961bfbfbc386b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def clear_memory():\n",
    "    \"\"\"Gi·∫£i ph√≥ng b·ªô nh·ªõ GPU v√† CPU\"\"\"\n",
    "    import gc\n",
    "    import torch\n",
    "    \n",
    "    # Gi·∫£i ph√≥ng b·ªô nh·ªõ GPU\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Gi·∫£i ph√≥ng b·ªô nh·ªõ CPU\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"üßπ ƒê√£ gi·∫£i ph√≥ng b·ªô nh·ªõ cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "INFO:accelerate.utils.modeling:Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:\n",
      "  - 0: 4982833152 bytes required\n",
      "  - 1: 2491416576 bytes required\n",
      "  - 2: 2491416576 bytes required\n",
      "These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ ƒêang load m√¥ h√¨nh Qwen...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5781f4045336409ea2f565b8fe491f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " #H√†m load m√¥ h√¨nh v√† tokenizer\n",
    "def load_model_and_tokenizer(model_path, tokenizer_path, model_name):\n",
    "    print(f\"\\nüîÑ ƒêang load m√¥ h√¨nh {model_name}...\")\n",
    "    try:\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            tokenizer_path,\n",
    "            trust_remote_code=True,\n",
    "            use_fast=False\n",
    "        )\n",
    "        \n",
    "        # Load model\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            device_map=\"balanced\",  # T·ª± ƒë·ªông ph√¢n ph·ªëi tr√™n c√°c GPU\n",
    "            torch_dtype=torch.bfloat16,  # S·ª≠ d·ª•ng half precision ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ ƒê√£ load th√†nh c√¥ng m√¥ h√¨nh {model_name}\")\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi load m√¥ h√¨nh {model_name}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Load Qwen\n",
    "qwen_model, qwen_tokenizer = load_model_and_tokenizer(qwen_model_path, qwen_tokenizer_path, \"Qwen\")\n",
    "\n",
    "# Load Llama\n",
    "llama_model, llama_tokenizer = load_model_and_tokenizer(llama_model_path, llama_tokenizer_path, \"Llama\")\n",
    "\n",
    "# Ki·ªÉm tra xem c√°c m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c load th√†nh c√¥ng ch∆∞a\n",
    "if qwen_model is None or qwen_tokenizer is None:\n",
    "    print(\"‚ö†Ô∏è Kh√¥ng th·ªÉ load m√¥ h√¨nh Qwen\")\n",
    "if llama_model is None or llama_tokenizer is None:\n",
    "    print(\"‚ö†Ô∏è Kh√¥ng th·ªÉ load m√¥ h√¨nh Llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_prompt(query, task_type=\"elementary_math\"):\n",
    "    \"\"\"\n",
    "    Create a standardized prompt for elementary school math problems.\n",
    "    \n",
    "    Parameters:\n",
    "    - query (str): The math problem statement\n",
    "    - task_type (str): Type of task (default: \"elementary_math\")\n",
    "    \n",
    "    Returns:\n",
    "    - str: Formatted prompt ready to be sent to an LLM\n",
    "    \"\"\"\n",
    "    if task_type == \"elementary_math\":\n",
    "        return f\"\"\"Please solve this elementary school math problem step by step:\n",
    "\n",
    "Problem: {query}\n",
    "\n",
    "Please follow these steps:\n",
    "1. Read the problem carefully\n",
    "2. Identify what is given and what needs to be found\n",
    "3. Write down the equation or method needed\n",
    "4. Solve step by step\n",
    "5. Check your answer\n",
    "6. Write the final answer clearly\n",
    "\n",
    "Solution:\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Please solve this problem:\n",
    "\n",
    "Problem: {query}\n",
    "\n",
    "Answers:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def clear_gpu_cache():\n",
    "    \"\"\"Clear GPU cache and garbage collection\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"üßπ ƒê√£ gi·∫£i ph√≥ng b·ªô nh·ªõ cache\")\n",
    "\n",
    "class InferenceContext:\n",
    "    \"\"\"Context manager for automatic memory management\"\"\"\n",
    "    def __enter__(self):\n",
    "        clear_gpu_cache()\n",
    "        return self\n",
    "        \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        clear_gpu_cache()\n",
    "\n",
    "def optimize_inference(model, tokenizer, problem_text, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Optimized inference function with performance improvements\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Optimize input processing\n",
    "        inputs = tokenizer(\n",
    "            problem_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(model.device)\n",
    "\n",
    "        # 2. Enable optimizations\n",
    "        with torch.inference_mode(), torch.amp.autocast('cuda'):\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                # 3. Optimize generation parameters\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                min_new_tokens=10,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                top_k=50,\n",
    "                num_beams=1,\n",
    "                do_sample=False,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=model.config.eos_token_id,\n",
    "                # 4. Enable optimization flags\n",
    "                use_cache=True,\n",
    "                repetition_penalty=1.0\n",
    "            )\n",
    "        \n",
    "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"L·ªói: {str(e)}\"\n",
    "\n",
    "def batch_inference(model, tokenizer, questions, batch_size=4):\n",
    "    \"\"\"\n",
    "    Process multiple questions in batches for better performance\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for i in range(0, len(questions), batch_size):\n",
    "        batch = questions[i:i + batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch, \n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(model.device)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(**inputs)\n",
    "            \n",
    "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        results.extend(decoded)\n",
    "    return results\n",
    "\n",
    "def fast_inference(model, tokenizer, question):\n",
    "    \"\"\"\n",
    "    Fast inference wrapper with automatic memory management\n",
    "    \"\"\"\n",
    "    with InferenceContext():\n",
    "        # Clear cache before starting\n",
    "        clear_gpu_cache()\n",
    "        \n",
    "        # Perform inference with optimizations\n",
    "        result = optimize_inference(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            question,\n",
    "            max_new_tokens=50\n",
    "        )\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call GPT API\n",
    "def gpt_inference(problem_text):\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": standard_prompt(problem_text)}],\n",
    "            max_tokens=50\n",
    "        )\n",
    "        return response.choices[0].message['content'].strip()\n",
    "    except Exception as e:\n",
    "        return f\"API Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gemini models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# ‚úÖ **Load m√¥ h√¨nh Gemini**\n",
    "try:\n",
    "    gemini_text_model = genai.GenerativeModel('gemini-1.5-flash-8b')  # Th·ª≠ model kh√°c n·∫øu l·ªói\n",
    "    gemini_vision_model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "    print(\"‚úÖ Gemini models loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói khi load Gemini: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemini_inference_improved(problem_text, model_type='text'):\n",
    "    \"\"\"H√†m inference c·∫£i ti·∫øn cho m√¥ h√¨nh Gemini\"\"\"\n",
    "    try:\n",
    "        prompt = standard_prompt(problem_text, \"math\")\n",
    "        \n",
    "        if model_type == 'text':\n",
    "            response = gemini_text_model.generate_content(\n",
    "                prompt,\n",
    "                generation_config={\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"max_output_tokens\": 512,\n",
    "                    \"top_p\": 0.95\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            # L·ªói: problem_image kh√¥ng ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n",
    "            # S·ª≠a: b·ªè tham chi·∫øu t·ªõi problem_image\n",
    "            response = gemini_vision_model.generate_content(\n",
    "                prompt,  # Ch·ªâ s·ª≠ d·ª•ng vƒÉn b·∫£n\n",
    "                generation_config={\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"max_output_tokens\": 512,\n",
    "                    \"top_p\": 0.95\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        return response.text.replace(\"**\", \"\").strip()  # Clean formatting\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è L·ªói Gemini: {str(e)}\")\n",
    "        return \"Kh√¥ng th·ªÉ t·∫°o ph·∫£n h·ªìi t·ª´ Gemini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qwen_inference_improved(problem_text):\n",
    "    \"\"\"Improved inference function for Qwen model\"\"\"\n",
    "    if qwen_model is None or qwen_tokenizer is None:\n",
    "        return \"L·ªói: Model ho·∫∑c tokenizer Qwen ch∆∞a ƒë∆∞·ª£c load\"\n",
    "    \n",
    "    try:\n",
    "        with InferenceContext():\n",
    "            result = optimize_inference(\n",
    "                qwen_model,\n",
    "                qwen_tokenizer,\n",
    "                problem_text,\n",
    "                max_new_tokens=50\n",
    "            )\n",
    "            return result\n",
    "    except Exception as e:\n",
    "        return f\"L·ªói Qwen: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def llama_inference_improved(problem_text):\n",
    "    \"\"\"Improved inference function for Llama model\"\"\"\n",
    "    if llama_model is None or llama_tokenizer is None:\n",
    "        return \"L·ªói: Model ho·∫∑c tokenizer Llama ch∆∞a ƒë∆∞·ª£c load\"\n",
    "    \n",
    "    try:\n",
    "        with InferenceContext():\n",
    "            result = optimize_inference(\n",
    "                llama_model,\n",
    "                llama_tokenizer,\n",
    "                problem_text,\n",
    "                max_new_tokens=50\n",
    "            )\n",
    "            return result\n",
    "    except Exception as e:\n",
    "        return f\"L·ªói Llama: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ B∆Ø·ªöC 2: TH·ª¨ INFERENCE V·ªöI M√î H√åNH ƒê√É C·∫¢I THI·ªÜN\n",
      "============================================================\n",
      "\n",
      "ü§ñ ƒêang ch·∫°y inference v·ªõi Gemini...\n",
      "\n",
      "üìù K·∫øt qu·∫£ Gemini:\n",
      "Let 'g' be the number of chickens and 'h' be the number of dogs.\n",
      "\n",
      "We have two equations:\n",
      "\n",
      "1) g + h = 36  (Total animals)\n",
      "2) 2g + 4h = 100 (Total legs)\n",
      "\n",
      "From equation 1, we can express g as:\n",
      "\n",
      "g = 36 - h\n",
      "\n",
      "Substitute this into equation 2:\n",
      "\n",
      "2(36 - h) + 4h = 100\n",
      "72 - 2h + 4h = 100\n",
      "2h = 28\n",
      "h = 14\n",
      "\n",
      "Now substitute the value of h back into the equation for g:\n",
      "\n",
      "g = 36 - 14\n",
      "g = 22\n",
      "\n",
      "Therefore, there are $\\boxed{22}$ chickens and $\\boxed{14}$ dogs.\n",
      "\n",
      "ü§ñ ƒêang ch·∫°y inference v·ªõi Llama c·∫£i ti·∫øn...\n",
      "üßπ ƒê√£ gi·∫£i ph√≥ng b·ªô nh·ªõ cache\n",
      "\n",
      "üìù K·∫øt qu·∫£ Llama:\n",
      "L·ªói: Model ho·∫∑c tokenizer Llama ch∆∞a ƒë∆∞·ª£c load\n",
      "\n",
      "ü§ñ ƒêang ch·∫°y inference v·ªõi Qwen c·∫£i ti·∫øn...\n",
      "üßπ ƒê√£ gi·∫£i ph√≥ng b·ªô nh·ªõ cache\n",
      "üßπ ƒê√£ gi·∫£i ph√≥ng b·ªô nh·ªõ cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Trune\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "g:\\Trune\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "g:\\Trune\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:677: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ ƒê√£ gi·∫£i ph√≥ng b·ªô nh·ªõ cache\n",
      "\n",
      "üìù K·∫øt qu·∫£ Qwen:\n",
      "L·ªói: CUDA out of memory. Tried to allocate 2.32 GiB. GPU 0 has a total capacity of 47.99 GiB of which 0 bytes is free. Of the allocated memory 45.53 GiB is allocated by PyTorch, and 13.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "============================================================\n",
      "‚úÖ HO√ÄN TH√ÄNH\n",
      "============================================================\n",
      "üìä T√≥m t·∫Øt k·∫øt qu·∫£:\n",
      "- Gemini: Th√†nh c√¥ng\n",
      "- Llama: Th√†nh c√¥ng\n",
      "- Qwen: Th√†nh c√¥ng\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ B∆Ø·ªöC 2: TH·ª¨ INFERENCE V·ªöI M√î H√åNH ƒê√É C·∫¢I THI·ªÜN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "#%% [Example Usage]\n",
    "problem1 = \"ƒê·ªÅ b√†i: M·ªôt con tr√¢u ƒÉn c·ªè trong 5 ng√†y th√¨ h·∫øt m·ªôt ƒë√°m c·ªè. N·∫øu hai con tr√¢u ƒÉn th√¨ ch·ªâ ƒë·ªß trong 3 ng√†y. H·ªèi n·∫øu ba con tr√¢u ƒÉn th√¨ ƒë√°m c·ªè ƒë√≥ ƒë·ªß trong m·∫•y ng√†y?\"\n",
    "problem2 = \"ƒê·ªÅ b√†i: C√≥ m·ªôt √¥ng l√£o mu·ªën chia b√≥ ƒë≈©a cho c√°c con sao cho: ‚Ä¢ N·∫øu chia cho 2 ng∆∞·ªùi th√¨ d∆∞ 1 chi·∫øc. ‚Ä¢ N·∫øu chia cho 3 ng∆∞·ªùi th√¨ d∆∞ 1 chi·∫øc. ‚Ä¢ N·∫øu chia cho 4, 5 ho·∫∑c 6 ng∆∞·ªùi th√¨ ƒë·ªÅu d∆∞ 1 chi·∫øc. ‚Ä¢ Nh∆∞ng n·∫øu chia cho 7 ng∆∞·ªùi th√¨ v·ª´a ƒë·ªß. H·ªèi b√≥ ƒë≈©a c√≥ bao nhi√™u chi·∫øc?\"\n",
    "problem3 = \"ƒê·ªÅ b√†i: M·ªôt ng∆∞·ªùi g√°nh cam ƒëi b√°n, khi qua ch·ª£, √¥ng b√°n m·ªôt n·ª≠a s·ªë cam v√† th√™m n·ª≠a qu·∫£. ƒêi ti·∫øp m·ªôt ƒëo·∫°n, √¥ng l·∫°i b√°n m·ªôt n·ª≠a s·ªë cam c√≤n l·∫°i v√† th√™m n·ª≠a qu·∫£. Cu·ªëi c√πng, khi ƒë·∫øn ch·ª£ cu·ªëi c√πng, √¥ng l·∫°i b√°n m·ªôt n·ª≠a s·ªë cam c√≤n l·∫°i v√† th√™m n·ª≠a qu·∫£, h·∫øt s·∫°ch cam. H·ªèi ban ƒë·∫ßu √¥ng c√≥ bao nhi√™u qu·∫£ cam?\"\n",
    "problem4 = \"ƒê·ªÅ b√†i: V·ª´a g√† v·ª´a ch√≥, B√≥ l·∫°i cho tr√≤n, Ba m∆∞∆°i s√°u con, M·ªôt trƒÉm ch√¢n ch·∫µn. H·ªèi c√≥ bao nhi√™u con g√†, bao nhi√™u con ch√≥?\"\n",
    "\n",
    "# Ch·∫°y inference v·ªõi m√¥ h√¨nh Gemini (th∆∞·ªùng ·ªïn ƒë·ªãnh nh·∫•t)\n",
    "print(\"\\nü§ñ ƒêang ch·∫°y inference v·ªõi Gemini...\")\n",
    "gemini_result = gemini_inference_improved(problem4)\n",
    "print(\"\\nüìù K·∫øt qu·∫£ Gemini:\")\n",
    "print(gemini_result)\n",
    "\n",
    "# Th·ª≠ v·ªõi Llama\n",
    "print(\"\\nü§ñ ƒêang ch·∫°y inference v·ªõi Llama c·∫£i ti·∫øn...\")\n",
    "# Gi·∫£i ph√≥ng b·ªô nh·ªõ tr∆∞·ªõc khi ch·∫°y\n",
    "clear_memory()\n",
    "try:\n",
    "    llama_result = llama_inference_improved(problem4)\n",
    "    print(\"\\nüìù K·∫øt qu·∫£ Llama:\")\n",
    "    print(llama_result)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Kh√¥ng th·ªÉ ch·∫°y inference v·ªõi Llama: {e}\")\n",
    "\n",
    "# Th·ª≠ v·ªõi Qwen\n",
    "print(\"\\nü§ñ ƒêang ch·∫°y inference v·ªõi Qwen c·∫£i ti·∫øn...\")\n",
    "# Gi·∫£i ph√≥ng b·ªô nh·ªõ tr∆∞·ªõc khi ch·∫°y\n",
    "clear_memory()\n",
    "try:\n",
    "    qwen_result = qwen_inference_improved(problem4)\n",
    "    print(\"\\nüìù K·∫øt qu·∫£ Qwen:\")\n",
    "    print(qwen_result)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Kh√¥ng th·ªÉ ch·∫°y inference v·ªõi Qwen: {e}\")\n",
    "\n",
    "# T·ªïng k·∫øt\n",
    "# Th√™m ki·ªÉm tra bi·∫øn\n",
    "llama_result = \"Kh√¥ng c√≥ k·∫øt qu·∫£\" if 'llama_result' not in locals() else llama_result\n",
    "qwen_result = \"Kh√¥ng c√≥ k·∫øt qu·∫£\" if 'qwen_result' not in locals() else qwen_result\n",
    "gemini_result = \"Kh√¥ng c√≥ k·∫øt qu·∫£\" if 'gemini_result' not in locals() else gemini_result\n",
    "\n",
    "# T·ªïng k·∫øt\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ HO√ÄN TH√ÄNH\")\n",
    "print(\"=\"*60)\n",
    "print(\"üìä T√≥m t·∫Øt k·∫øt qu·∫£:\")\n",
    "print(f\"- Gemini: {'Th√†nh c√¥ng' if 'Kh√¥ng th·ªÉ t·∫°o ph·∫£n h·ªìi t·ª´ Gemini' not in gemini_result else 'Th·∫•t b·∫°i'}\")\n",
    "print(f\"- Llama: {'Th√†nh c√¥ng' if 'L·ªói Llama' not in llama_result else 'Th·∫•t b·∫°i'}\")\n",
    "print(f\"- Qwen: {'Th√†nh c√¥ng' if 'L·ªói Qwen' not in qwen_result else 'Th·∫•t b·∫°i'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def safe_inference(model, tokenizer, text, model_name):\n",
    "    \"\"\"\n",
    "    Safe inference wrapper with error handling and logging\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if model is None or tokenizer is None:\n",
    "            logger.error(f\"{model_name}: Model or tokenizer not loaded\")\n",
    "            return f\"L·ªói {model_name}: Model ch∆∞a ƒë∆∞·ª£c load\"\n",
    "            \n",
    "        with InferenceContext():\n",
    "            result = optimize_inference(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                text,\n",
    "                max_new_tokens=50\n",
    "            )\n",
    "            logger.info(f\"{model_name}: Inference successful\")\n",
    "            return result\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"{model_name}: Error during inference - {str(e)}\")\n",
    "        return f\"L·ªói {model_name}: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu_memory():\n",
    "    \"\"\"Check GPU memory before inference\"\"\"\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        free = total - allocated\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Total: {total:.2f} GB\")\n",
    "        print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"  Free: {free:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cot_prompt(problem):\n",
    "    return f\"\"\"You are a helpful assistant tasked with solving the following problem step by step. Explain your reasoning clearly at each stage and provide the final answer.\n",
    "\n",
    "Problem: {problem}\n",
    "\n",
    "Follow these steps to solve the problem:\n",
    "1. **Restate the Problem**: Rephrase the problem in your own words to confirm understanding.\n",
    "2. **Identify Key Variables**: Define the unknowns (e.g., 'let x be...') and list what needs to be solved.\n",
    "3. **List Given Information**: Write down all provided data and any assumptions (e.g., units, implied conditions).\n",
    "4. **Recall Relevant Concepts**: Mention any formulas, rules, or strategies (e.g., algebra, geometry) that apply.\n",
    "5. **Break Down the Problem**: If complex, split it into smaller sub-problems or logical steps.\n",
    "6. **Solve Step by Step**: Work through each part, showing all calculations and explaining your logic.\n",
    "7. **Combine Results**: Integrate the solutions to sub-problems to find the overall answer.\n",
    "8. **Verify the Solution**: Check the answer by substituting it back into the problem or using an alternative method. Explain why it makes sense.\n",
    "9. **Present the Answer**: State the final answer clearly and concisely.\n",
    "\n",
    "Focus on clarity and logical progression. If unsure, explore multiple approaches and select the best one.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
