"""
Result Analyzer cho h·ªá th·ªëng ƒë√°nh gi√° LLM.
Ph√¢n t√≠ch c√°c k·∫øt qu·∫£ t·ª´ qu√° tr√¨nh ƒë√°nh gi√° v√† t√≠nh to√°n c√°c metrics.
H·ªó tr·ª£ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng suy lu·∫≠n qua API Groq v√† c√°c metrics kh√°c.
"""

import time
import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Union, Tuple
from collections import defaultdict
import traceback
import re
import random

# Import c√°c module c·∫ßn thi·∫øt
try:
    from ..core.model_interface import generate_text
except ImportError:
    # Fallback khi ch·∫°y module tr·ª±c ti·∫øp
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from core.model_interface import generate_text

# Thi·∫øt l·∫≠p logging
logger = logging.getLogger(__name__)

class ResultAnalyzer:
    """
    Ph√¢n t√≠ch k·∫øt qu·∫£ ƒë√°nh gi√° m√¥ h√¨nh ng√¥n ng·ªØ.
    T√≠nh to√°n c√°c metrics v√† ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng suy lu·∫≠n.
    """
    
    def __init__(self, 
                 results_df: Optional[pd.DataFrame] = None,
                 reasoning_evaluation_config: Optional[Dict[str, Any]] = None,
                 reasoning_model: str = "groq/llama3-70b-8192",
                 language: str = "vietnamese",
                 similarity_model: Optional[str] = None,
                 verbose: bool = True):
        """
        Kh·ªüi t·∫°o ResultAnalyzer.
        
        Args:
            results_df (pd.DataFrame, optional): DataFrame k·∫øt qu·∫£ ƒë·ªÉ ph√¢n t√≠ch
            reasoning_evaluation_config (Dict, optional): C·∫•u h√¨nh ƒë√°nh gi√° suy lu·∫≠n
            reasoning_model (str): M√¥ h√¨nh d√πng ƒë·ªÉ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng suy lu·∫≠n
            language (str): Ng√¥n ng·ªØ s·ª≠ d·ª•ng trong ƒë√°nh gi√°
            similarity_model (str, optional): M√¥ h√¨nh ƒë·ªÉ t√≠nh to√°n semantic similarity
            verbose (bool): Hi·ªÉn th·ªã th√¥ng tin chi ti·∫øt trong qu√° tr√¨nh ph√¢n t√≠ch
        """
        self.results_df = results_df
        self.reasoning_config = reasoning_evaluation_config or {}
        self.reasoning_model = self.reasoning_config.get("model", reasoning_model)
        self.language = language.lower()
        self.similarity_model = similarity_model
        self.verbose = verbose
        self.sample_size = self.reasoning_config.get("sample_size", 10)
        
        # C√°c ti√™u ch√≠ ƒë√°nh gi√° m·ªõi
        self.reasoning_criteria = {
            "accuracy": "ƒê·ªô ch√≠nh x√°c (Accuracy)",
            "reasoning_consistency": "ƒê·ªô suy lu·∫≠n h·ª£p l√Ω (Reasoning Consistency)",
            "consistency": "T√≠nh nh·∫•t qu√°n (Consistency)",
            "difficulty_performance": "Hi·ªáu su·∫•t tr√™n ƒë·ªô kh√≥ (Difficulty Performance)",
            "context_adherence": "ƒê·ªô ph√π h·ª£p ng·ªØ c·∫£nh (Context Adherence)"
        }

        # C·∫•u tr√∫c prompt ƒë√°nh gi√°
        self.reasoning_eval_template = """
# H∆Ø·ªöNG D·∫™N ƒê√ÅNH GI√Å CH·∫§T L∆Ø·ª¢NG ƒê·∫¶U RA C·ª¶A M√î H√åNH LLM

B·∫°n l√† m·ªôt chuy√™n gia ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng ƒë·∫ßu ra c·ªßa c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLMs). Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë√°nh gi√° c√¢u tr·∫£ l·ªùi c·ªßa m·ªôt m√¥ h√¨nh LLM cho m·ªôt b√†i to√°n c·ª• th·ªÉ d·ª±a tr√™n c√°c ti√™u ch√≠ kh√°ch quan v√† r√µ r√†ng.

## TI√äU CH√ç ƒê√ÅNH GI√Å

1. **ƒê·ªô ch√≠nh x√°c (Accuracy)**
   - C√¢u tr·∫£ l·ªùi c√≥ ƒë√∫ng v·ªÅ m·∫∑t n·ªôi dung v√† k·∫øt qu·∫£ so v·ªõi ƒë√°p √°n chu·∫©n kh√¥ng?
   - V·ªõi b√†i to√°n s·ªë h·ªçc: k·∫øt qu·∫£ cu·ªëi c√πng c√≥ ƒë√∫ng kh√¥ng?
   - V·ªõi b√†i to√°n l√Ω lu·∫≠n: k·∫øt lu·∫≠n c√≥ ch√≠nh x√°c kh√¥ng?
   - ƒêi·ªÉm 5: Ho√†n to√†n ch√≠nh x√°c
   - ƒêi·ªÉm 1: Ho√†n to√†n sai

2. **ƒê·ªô suy lu·∫≠n h·ª£p l√Ω (Reasoning Consistency)**
   - Qu√° tr√¨nh l·∫≠p lu·∫≠n c√≥ logic v√† c√≥ c·∫•u tr√∫c r√µ r√†ng kh√¥ng?
   - C√°c b∆∞·ªõc suy lu·∫≠n c√≥ th·ªÉ theo d√µi v√† ki·ªÉm ch·ª©ng ƒë∆∞·ª£c kh√¥ng?
   - C√≥ sai s√≥t logic trong c√°c b∆∞·ªõc l·∫≠p lu·∫≠n kh√¥ng?
   - ƒêi·ªÉm 5: L·∫≠p lu·∫≠n ho√†n h·∫£o, r√µ r√†ng, v√† ƒë·∫ßy ƒë·ªß
   - ƒêi·ªÉm 1: L·∫≠p lu·∫≠n r·ªùi r·∫°c, m√¢u thu·∫´n ho·∫∑c sai c∆° b·∫£n

3. **T√≠nh nh·∫•t qu√°n (Consistency)**
   - C√¢u tr·∫£ l·ªùi c√≥ nh·∫•t qu√°n t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi kh√¥ng?
   - Kh√¥ng c√≥ m√¢u thu·∫´n gi·ªØa c√°c ph·∫ßn trong c√¢u tr·∫£ l·ªùi?
   - C√°c ƒë·ªãnh nghƒ©a v√† k√Ω hi·ªáu ƒë∆∞·ª£c s·ª≠ d·ª•ng nh·∫•t qu√°n?
   - ƒêi·ªÉm 5: Ho√†n to√†n nh·∫•t qu√°n
   - ƒêi·ªÉm 1: Nhi·ªÅu m√¢u thu·∫´n n·ªôi b·ªô

4. **Hi·ªáu su·∫•t ph√π h·ª£p v·ªõi ƒë·ªô kh√≥ (Difficulty Performance)**
   - C√¢u tr·∫£ l·ªùi c√≥ ph√π h·ª£p v·ªõi ƒë·ªô kh√≥ c·ªßa b√†i to√°n kh√¥ng?
   - M√¥ h√¨nh c√≥ x·ª≠ l√Ω ƒë·∫ßy ƒë·ªß ƒë·ªô ph·ª©c t·∫°p c·ªßa b√†i to√°n kh√¥ng?
   - ƒêi·ªÉm 5: X·ª≠ l√Ω xu·∫•t s·∫Øc b√†i to√°n theo ƒë√∫ng ƒë·ªô kh√≥
   - ƒêi·ªÉm 1: Kh√¥ng ƒë√°p ·ª©ng ƒë∆∞·ª£c y√™u c·∫ßu c∆° b·∫£n c·ªßa b√†i to√°n

5. **ƒê·ªô ph√π h·ª£p ng·ªØ c·∫£nh (Context Adherence)**
   - C√¢u tr·∫£ l·ªùi c√≥ t·∫≠n d·ª•ng t·ªët ng·ªØ c·∫£nh/v√≠ d·ª• ƒë∆∞·ª£c cung c·∫•p kh√¥ng?
   - √Åp d·ª•ng ƒë√∫ng c√°c m·∫´u/c·∫•u tr√∫c t·ª´ ng·ªØ c·∫£nh v√†o b√†i gi·∫£i?
   - ƒêi·ªÉm 5: T·∫≠n d·ª•ng t·ªëi ƒëa ng·ªØ c·∫£nh m·ªôt c√°ch hi·ªáu qu·∫£
   - ƒêi·ªÉm 1: Ho√†n to√†n kh√¥ng s·ª≠ d·ª•ng ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p

## B√ÄI TO√ÅN C·∫¶N GI·∫¢I QUY·∫æT

{question}

## ƒê√ÅP √ÅN CHU·∫®N

{correct_answer}

## C√ÇU TR·∫¢ L·ªúI C·ª¶A M√î H√åNH C·∫¶N ƒê√ÅNH GI√Å

{model_answer}

## ƒê√ÅNH GI√Å THEO THANG ƒêI·ªÇM 5

H√£y ƒë√°nh gi√° v√† cho ƒëi·ªÉm t·ª´ 1-5 cho t·ª´ng ti√™u ch√≠, trong ƒë√≥ 1 l√† k√©m nh·∫•t v√† 5 l√† t·ªët nh·∫•t:

1. ƒê·ªô ch√≠nh x√°c (accuracy): ?/5
2. ƒê·ªô suy lu·∫≠n h·ª£p l√Ω (reasoning): ?/5
3. T√≠nh nh·∫•t qu√°n (completeness): ?/5
4. Hi·ªáu su·∫•t ph√π h·ª£p v·ªõi ƒë·ªô kh√≥ (explanation): ?/5
5. ƒê·ªô ph√π h·ª£p ng·ªØ c·∫£nh (cultural_context): ?/5

ƒêi·ªÉm trung b√¨nh (average): ?/5

## GI·∫¢I TH√çCH CHI TI·∫æT

- ƒê·ªô ch√≠nh x√°c: [gi·∫£i th√≠ch chi ti·∫øt]
- ƒê·ªô suy lu·∫≠n h·ª£p l√Ω: [gi·∫£i th√≠ch chi ti·∫øt]
- T√≠nh nh·∫•t qu√°n: [gi·∫£i th√≠ch chi ti·∫øt]
- Hi·ªáu su·∫•t ph√π h·ª£p v·ªõi ƒë·ªô kh√≥: [gi·∫£i th√≠ch chi ti·∫øt]
- ƒê·ªô ph√π h·ª£p ng·ªØ c·∫£nh: [gi·∫£i th√≠ch chi ti·∫øt]

## K·∫æT LU·∫¨N T·ªîNG TH·ªÇ

[nh·∫≠n x√©t t·ªïng quan v·ªÅ ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi]
"""

    def analyze(self) -> pd.DataFrame:
        """
        Ph√¢n t√≠ch k·∫øt qu·∫£ v√† th·ª±c hi·ªán ƒë√°nh gi√°.
        Ph∆∞∆°ng th·ª©c n√†y g·ªçi khi ResultAnalyzer ƒë∆∞·ª£c s·ª≠ d·ª•ng t·ª´ Evaluator.
        
        Returns:
            pd.DataFrame: DataFrame v·ªõi k·∫øt qu·∫£ ph√¢n t√≠ch
        """
        if self.results_df is None or len(self.results_df) == 0:
            logger.warning("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch")
            return pd.DataFrame()
        
        if self.verbose:
            logger.info(f"üîç B·∫Øt ƒë·∫ßu ph√¢n t√≠ch {len(self.results_df)} k·∫øt qu·∫£ ƒë√°nh gi√°")
        
        # Ki·ªÉm tra c√°c c·ªôt c·∫ßn thi·∫øt
        required_cols = ['model_name', 'prompt_type', 'question_text']
        missing_cols = [col for col in required_cols if col not in self.results_df.columns]
        if missing_cols:
            logger.warning(f"Thi·∫øu c√°c c·ªôt c·∫ßn thi·∫øt cho ph√¢n t√≠ch: {missing_cols}")
            logger.info(f"C√°c c·ªôt hi·ªán c√≥: {list(self.results_df.columns)}")
            return self.results_df
        
        # Kh·ªüi t·∫°o dictionary metrics ch√≠nh v·ªõi c√°c kh√≥a ch√≠nh l√† dict r·ªóng
        analysis_results = {
            'basic_metrics': {},
            'model_prompt_metrics': {},
            'question_type_metrics': {},
            'accuracy_metrics': {},
            'reasoning_metrics': {},
            'consistency_metrics': {},
            'difficulty_metrics': {},
            'context_metrics': {}
        }
        
        # T√≠nh to√°n metrics c∆° b·∫£n
        analysis_results['basic_metrics'] = self._compute_basic_metrics(self.results_df)
        
        # T√≠nh to√°n metrics theo model v√† prompt type
        analysis_results['model_prompt_metrics'] = self._compute_metrics_by_model_prompt(self.results_df)
        
        # T√≠nh to√°n metrics theo lo·∫°i c√¢u h·ªèi (n·∫øu c√≥ th√¥ng tin)
        if 'question_type' in self.results_df.columns:
            analysis_results['question_type_metrics'] = self._compute_metrics_by_question_type(self.results_df)
        
        # ƒê√°nh gi√° theo c√°c ti√™u ch√≠ m·ªõi
        # 1. Accuracy
        if 'is_correct' in self.results_df.columns:
            analysis_results['accuracy_metrics'] = self._compute_accuracy_metrics(self.results_df)
        
        # 2. Reasoning Consistency
        if any(col.startswith('reasoning_') and col != 'reasoning_scores_str' for col in self.results_df.columns):
            analysis_results['reasoning_metrics'] = self._compute_reasoning_metrics(self.results_df)
        
        # 3. Consistency
        if self.results_df['prompt_type'].str.contains('consistency|cot_self_consistency', case=False).any():
            if self.verbose:
                logger.info("ƒê√°nh gi√° t√≠nh nh·∫•t qu√°n trong c√°c self-consistency runs")
            
            try:
                self.results_df = self.evaluate_consistency(self.results_df)
                analysis_results["consistency_metrics"] = self._compute_consistency_metrics(self.results_df)
            except Exception as e:
                logger.error(f"L·ªói khi ƒë√°nh gi√° t√≠nh nh·∫•t qu√°n: {str(e)}")
                logger.debug(traceback.format_exc())
        
        # 4. Performance on different difficulty levels
        analysis_results['difficulty_metrics'] = self._compute_difficulty_metrics(self.results_df)
        
        # 5. Context Adherence
        analysis_results['context_metrics'] = self._compute_context_adherence_metrics(self.results_df)
        
        # L∆∞u k·∫øt qu·∫£ ph√¢n t√≠ch v√†o thu·ªôc t√≠nh
        self.analysis_results = analysis_results
        
        return self.results_df
    
    def analyze_errors(self, 
                   results_df: pd.DataFrame, 
                   sample_size: int = 50,
                   random_seed: int = 42) -> pd.DataFrame:
        """
        Ph√¢n t√≠ch v√† ph√¢n lo·∫°i c√°c l·ªói trong c√¢u tr·∫£ l·ªùi c·ªßa model.
        
        C√°c lo·∫°i l·ªói ƒë∆∞·ª£c ph√¢n lo·∫°i bao g·ªìm:
        - L·ªói ki·∫øn th·ª©c (Knowledge Error): Thi·∫øu th√¥ng tin ho·∫∑c ki·∫øn th·ª©c kh√¥ng ch√≠nh x√°c
        - L·ªói suy lu·∫≠n (Reasoning Error): L·ªói trong qu√° tr√¨nh suy lu·∫≠n, sai logic
        - L·ªói t√≠nh to√°n (Calculation Error): Sai s·ªë h·ªçc ho·∫∑c t√≠nh to√°n
        - L·ªói kh√¥ng tr·∫£ l·ªùi (Non-answer): T·ª´ ch·ªëi ho·∫∑c kh√¥ng tr·∫£ l·ªùi
        - L·ªói l·∫°c ƒë·ªÅ (Off-topic): Tr·∫£ l·ªùi kh√¥ng li√™n quan ƒë·∫øn c√¢u h·ªèi
        - L·ªói hi·ªÉu nh·∫ßm (Misunderstanding): Hi·ªÉu sai c√¢u h·ªèi
        - L·ªói kh√°c (Other): C√°c l·ªói kh√¥ng thu·ªôc c√°c lo·∫°i tr√™n
        
        Args:
            results_df (pd.DataFrame): DataFrame ch·ª©a k·∫øt qu·∫£ ƒë√°nh gi√°
            sample_size (int): S·ªë l∆∞·ª£ng m·∫´u c·∫ßn ph√¢n t√≠ch
            random_seed (int): Seed ng·∫´u nhi√™n cho vi·ªác l·∫•y m·∫´u
            
        Returns:
            pd.DataFrame: DataFrame ƒë√£ b·ªï sung ph√¢n lo·∫°i l·ªói
        """
        # Ki·ªÉm tra xem c√≥ c√°c c·ªôt c·∫ßn thi·∫øt kh√¥ng
        required_cols = ['question_text', 'response', 'is_correct']
        for col in required_cols:
            if col not in results_df.columns:
                logger.warning(f"Kh√¥ng th·ªÉ ph√¢n t√≠ch l·ªói: thi·∫øu c·ªôt '{col}'")
                return results_df
                
        # ƒê·∫£m b·∫£o c√≥ c√°c c·ªôt c·∫ßn thi·∫øt
        if 'error_type' not in results_df.columns:
            results_df['error_type'] = ''
            
        if 'error_explanation' not in results_df.columns:
            results_df['error_explanation'] = ''
        
        # L·ªçc c√°c c√¢u tr·∫£ l·ªùi sai
        if 'is_correct' not in results_df.columns:
            logger.warning("Kh√¥ng c√≥ c·ªôt 'is_correct' ƒë·ªÉ ph√¢n t√≠ch l·ªói")
            return results_df
        
        # L·ªçc c√°c c√¢u tr·∫£ l·ªùi sai ch∆∞a ƒë∆∞·ª£c ph√¢n t√≠ch l·ªói
        error_rows = (results_df['is_correct'] == False) & (results_df['error_type'] == '')
        
        # Ki·ªÉm tra xem c√≥ c√¢u tr·∫£ l·ªùi sai ƒë·ªÉ ph√¢n t√≠ch kh√¥ng
        if not error_rows.any():
            logger.info("Kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi sai ch∆∞a ƒë∆∞·ª£c ph√¢n t√≠ch")
            return results_df
        
        error_indices = results_df[error_rows].index.tolist()
        
        # L·∫•y m·∫´u ng·∫´u nhi√™n n·∫øu c·∫ßn
        np.random.seed(random_seed)
        if len(error_indices) > sample_size:
            sample_indices = np.random.choice(error_indices, size=sample_size, replace=False)
        else:
            sample_indices = error_indices
        
        logger.info(f"Ph√¢n t√≠ch l·ªói cho {len(sample_indices)}/{len(error_indices)} c√¢u tr·∫£ l·ªùi sai")
        
        # Ph√¢n t√≠ch l·ªói cho t·ª´ng m·∫´u
        for i, idx in enumerate(sample_indices):
            row = results_df.loc[idx]
            
            question = row['question_text']
            model_answer = row['response']
            correct_answer = row['correct_answer'] if 'correct_answer' in row else None
            
            if self.verbose:
                logger.info(f"Ph√¢n t√≠ch l·ªói m·∫´u {i+1}/{len(sample_indices)}: model={row['model_name']}, prompt={row['prompt_type']}")
            
            try:
                # Ph√¢n t√≠ch l·ªói
                error_result = self._analyze_single_error(question, model_answer, correct_answer)
                
                # C·∫≠p nh·∫≠t DataFrame
                results_df.at[idx, 'error_type'] = error_result.get('error_type', 'Unknown')
                results_df.at[idx, 'error_explanation'] = error_result.get('explanation', '')
            
            except Exception as e:
                logger.error(f"L·ªói khi ph√¢n t√≠ch l·ªói cho m·∫´u {idx}: {str(e)}")
                logger.error(traceback.format_exc())
                
                # G√°n gi√° tr·ªã m·∫∑c ƒë·ªãnh
                results_df.at[idx, 'error_type'] = 'Analysis Error'
                results_df.at[idx, 'error_explanation'] = f"L·ªói khi ph√¢n t√≠ch: {str(e)}"
        
        return results_df
    
    def _analyze_single_error(self, 
                            question: str, 
                            model_answer: str, 
                            correct_answer: str = None) -> Dict[str, Any]:
        """
        Ph√¢n t√≠ch l·ªói cho m·ªôt m·∫´u c√¢u h·ªèi/c√¢u tr·∫£ l·ªùi.
        
        Args:
            question (str): C√¢u h·ªèi
            model_answer (str): C√¢u tr·∫£ l·ªùi c·ªßa model
            correct_answer (str, optional): ƒê√°p √°n ƒë√∫ng n·∫øu c√≥
            
        Returns:
            Dict: K·∫øt qu·∫£ ph√¢n t√≠ch l·ªói
        """
        try:
            # C·∫Øt b·ªõt ƒë·ªô d√†i n·∫øu qu√° d√†i
            max_length = 4000  # Gi·ªõi h·∫°n ƒë·ªô d√†i ƒë·ªÉ tr√°nh v∆∞·ª£t qu√° context window
            if len(model_answer) > max_length:
                logger.warning(f"C·∫Øt b·ªõt c√¢u tr·∫£ l·ªùi ({len(model_answer)} -> {max_length} k√Ω t·ª±)")
                model_answer = model_answer[:max_length] + "..."
            
            # T·∫°o prompt ph√¢n t√≠ch l·ªói
            if self.language.lower() == "vietnamese":
                correct_answer_part = f"\nƒê√ÅP √ÅN ƒê√öNG:\n{correct_answer}" if correct_answer else ""
                
                error_analysis_prompt = f"""
B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch l·ªói trong c√¢u tr·∫£ l·ªùi c·ªßa m√¥ h√¨nh ng√¥n ng·ªØ. H√£y ph√¢n t√≠ch v√† ph√¢n lo·∫°i l·ªói trong c√¢u tr·∫£ l·ªùi d∆∞·ªõi ƒë√¢y.

C√ÇU H·ªéI:
{question}
{correct_answer_part}

C√ÇU TR·∫¢ L·ªúI C·ª¶A M√î H√åNH:
{model_answer}

Ph√¢n lo·∫°i l·ªói v√†o M·ªòT trong c√°c danh m·ª•c sau:
1. L·ªói ki·∫øn th·ª©c (Knowledge Error): Thi·∫øu th√¥ng tin ho·∫∑c ki·∫øn th·ª©c kh√¥ng ch√≠nh x√°c
2. L·ªói suy lu·∫≠n (Reasoning Error): L·ªói trong qu√° tr√¨nh suy lu·∫≠n, sai logic
3. L·ªói t√≠nh to√°n (Calculation Error): Sai s·ªë h·ªçc ho·∫∑c t√≠nh to√°n
4. L·ªói kh√¥ng tr·∫£ l·ªùi (Non-answer): T·ª´ ch·ªëi ho·∫∑c kh√¥ng tr·∫£ l·ªùi
5. L·ªói l·∫°c ƒë·ªÅ (Off-topic): Tr·∫£ l·ªùi kh√¥ng li√™n quan ƒë·∫øn c√¢u h·ªèi
6. L·ªói hi·ªÉu nh·∫ßm (Misunderstanding): Hi·ªÉu sai c√¢u h·ªèi
7. L·ªói kh√°c (Other): C√°c l·ªói kh√¥ng thu·ªôc c√°c lo·∫°i tr√™n

Lo·∫°i l·ªói: [Ch·ªçn M·ªòT lo·∫°i l·ªói t·ª´ danh s√°ch tr√™n]

Gi·∫£i th√≠ch ng·∫Øn g·ªçn:
[Gi·∫£i th√≠ch t·∫°i sao c√¢u tr·∫£ l·ªùi b·ªã coi l√† sai v√† thu·ªôc lo·∫°i l·ªói ƒë√£ ch·ªçn]
"""
            else:
                correct_answer_part = f"\nCORRECT ANSWER:\n{correct_answer}" if correct_answer else ""
                
                error_analysis_prompt = f"""
You are an expert analyzing errors in language model responses. Analyze and categorize the error in the response below.

QUESTION:
{question}
{correct_answer_part}

MODEL RESPONSE:
{model_answer}

Categorize the error into ONE of the following categories:
1. Knowledge Error: Missing information or incorrect knowledge
2. Reasoning Error: Errors in the reasoning process, logical fallacies
3. Calculation Error: Mathematical or computational mistakes
4. Non-answer: Refusing or failing to provide an answer
5. Off-topic: Response unrelated to the question
6. Misunderstanding: Misinterpreting the question
7. Other: Errors that don't fall into the above categories

Error Type: [Select ONE error type from the list above]

Brief Explanation:
[Explain why the answer is incorrect and why it belongs to the selected error type]
"""
            
            # S·ª≠ d·ª•ng model API ƒë·ªÉ ph√¢n t√≠ch l·ªói
            use_groq = self.reasoning_config.get("use_groq", True)
            if use_groq:
                # S·ª≠ d·ª•ng Groq API
                from core.model_interface import generate_text
                
                # L·∫•y t√™n model Groq
                model_name = "groq"
                config = {
                    "model": self.reasoning_config.get("models", {}).get(
                        "error_analysis", "llama3-70b-8192"
                    ),
                    "temperature": 0.1,  # Th·∫•p ƒë·ªÉ ƒë·∫£m b·∫£o ph√¢n lo·∫°i nh·∫•t qu√°n
                    "max_tokens": 1024
                }
                
                # G·ªçi API
                logger.debug("Ph√¢n t√≠ch l·ªói b·∫±ng Groq API")
                response_text, stats = generate_text(model_name, error_analysis_prompt, config)
                
                if stats.get("has_error", False):
                    logger.error(f"L·ªói khi g·ªçi Groq API: {stats.get('error_message')}")
                    # Fallback v·ªÅ gi√° tr·ªã m·∫∑c ƒë·ªãnh
                    return {
                        "error_type": "Unknown",
                        "explanation": f"[L·ªói ph√¢n t√≠ch: {stats.get('error_message')}]"
                    }
            else:
                # TODO: S·ª≠ d·ª•ng model kh√°c n·∫øu c·∫ßn
                logger.warning("Ch·ªâ h·ªó tr·ª£ Groq API ƒë·ªÉ ph√¢n t√≠ch l·ªói")
                response_text = ""
            
            # Ph√¢n t√≠ch k·∫øt qu·∫£ ph√¢n lo·∫°i l·ªói
            result = self._parse_error_analysis(response_text)
            return result
            
        except Exception as e:
            logger.error(f"L·ªói khi ph√¢n t√≠ch l·ªói: {str(e)}")
            logger.error(traceback.format_exc())
            
            # Tr·∫£ v·ªÅ gi√° tr·ªã m·∫∑c ƒë·ªãnh
            return {
                "error_type": "Analysis Error",
                "explanation": f"L·ªói khi ph√¢n t√≠ch: {str(e)}"
            }
    
    def _parse_error_analysis(self, analysis_response: str) -> Dict[str, Any]:
        """
        Ph√¢n t√≠ch k·∫øt qu·∫£ ph√¢n lo·∫°i l·ªói t·ª´ API.
        
        Args:
            analysis_response (str): Ph·∫£n h·ªìi t·ª´ API
            
        Returns:
            Dict: Dictionary ch·ª©a lo·∫°i l·ªói v√† gi·∫£i th√≠ch
        """
        result = {
            "error_type": "Unknown",
            "explanation": analysis_response
        }
        
        if not analysis_response:
            return result
        
        # X√°c ƒë·ªãnh lo·∫°i l·ªói t·ª´ ph·∫£n h·ªìi
        error_type_patterns = [
            r"(?:Lo·∫°i l·ªói|Error Type)[:]\s*(.*?)(?:\n|$)",
            r"(?:l·ªói|error)[:]\s*(.*?)(?:\n|$)"
        ]
        
        for pattern in error_type_patterns:
            match = re.search(pattern, analysis_response, re.IGNORECASE)
            if match:
                error_type = match.group(1).strip()
                
                # Chu·∫©n h√≥a lo·∫°i l·ªói
                error_type = self._normalize_error_type(error_type)
                result["error_type"] = error_type
                break
        
        # Tr√≠ch xu·∫•t ph·∫ßn gi·∫£i th√≠ch
        explanation_patterns = [
            r"(?:Gi·∫£i th√≠ch|Brief Explanation)[:]\s*([\s\S]*)",
            r"(?:gi·∫£i th√≠ch|explanation)[:]\s*([\s\S]*)"
        ]
        
        for pattern in explanation_patterns:
            match = re.search(pattern, analysis_response, re.IGNORECASE)
            if match:
                explanation = match.group(1).strip()
                result["explanation"] = explanation
                break
        
        return result
    
    def _normalize_error_type(self, error_type: str) -> str:
        """
        Chu·∫©n h√≥a lo·∫°i l·ªói v·ªÅ c√°c lo·∫°i chu·∫©n.
        
        Args:
            error_type (str): Lo·∫°i l·ªói ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ ph·∫£n h·ªìi
            
        Returns:
            str: Lo·∫°i l·ªói ƒë√£ chu·∫©n h√≥a
        """
        # C√°c t·ª´ kh√≥a ƒë·ªÉ ph√¢n lo·∫°i l·ªói
        error_types = {
            "Knowledge Error": ["knowledge", "ki·∫øn th·ª©c", "thi·∫øu th√¥ng tin", "incorrect knowledge"],
            "Reasoning Error": ["reasoning", "suy lu·∫≠n", "logic", "logical", "reasoning process"],
            "Calculation Error": ["calculation", "t√≠nh to√°n", "mathematical", "computational", "s·ªë h·ªçc"],
            "Non-answer": ["non-answer", "kh√¥ng tr·∫£ l·ªùi", "refusing", "failing", "t·ª´ ch·ªëi"],
            "Off-topic": ["off-topic", "l·∫°c ƒë·ªÅ", "unrelated", "kh√¥ng li√™n quan"],
            "Misunderstanding": ["misunderstanding", "hi·ªÉu nh·∫ßm", "misinterpreting", "misinterpretation"],
            "Other": ["other", "kh√°c"]
        }
        
        error_type_lower = error_type.lower()
        
        # T√¨m lo·∫°i l·ªói ph√π h·ª£p nh·∫•t
        for standard_type, keywords in error_types.items():
            if any(keyword.lower() in error_type_lower for keyword in keywords):
                return standard_type
        
        # Ki·ªÉm tra ch·ªâ s·ªë 1-7 n·∫øu c√≥
        if re.match(r"^[1-7][\.:]?", error_type_lower):
            index = int(error_type_lower[0]) - 1
            standard_types = list(error_types.keys())
            if 0 <= index < len(standard_types):
                return standard_types[index]
        
        return "Other"
    
    def _compute_error_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        T√≠nh to√°n c√°c metrics li√™n quan ƒë·∫øn ph√¢n t√≠ch l·ªói.
        
        Args:
            df (pd.DataFrame): DataFrame ƒë√£ c√≥ ph√¢n lo·∫°i l·ªói
            
        Returns:
            Dict: Metrics li√™n quan ƒë·∫øn ph√¢n t√≠ch l·ªói
        """
        metrics = {}
        
        # L·ªçc c√°c d√≤ng c√≥ ph√¢n lo·∫°i l·ªói
        error_df = df[df['error_type'] != '']
        
        if len(error_df) == 0:
            return metrics
        
        # 1. T·ª∑ l·ªá c√°c lo·∫°i l·ªói t·ªïng th·ªÉ
        error_counts = error_df['error_type'].value_counts()
        error_percentages = error_counts / len(error_df) * 100
        
        metrics["overall"] = {
            "error_counts": error_counts.to_dict(),
            "error_percentages": error_percentages.to_dict()
        }
        
        # 2. T·ª∑ l·ªá l·ªói theo model
        metrics["by_model"] = {}
        if 'model_name' in error_df.columns:
            for model in error_df['model_name'].unique():
                model_df = error_df[error_df['model_name'] == model]
                
                model_error_counts = model_df['error_type'].value_counts()
                model_error_percentages = model_error_counts / len(model_df) * 100
                
                metrics["by_model"][model] = {
                    "error_counts": model_error_counts.to_dict(),
                    "error_percentages": model_error_percentages.to_dict()
                }
        elif 'model' in error_df.columns:
            for model in error_df['model'].unique():
                model_df = error_df[error_df['model'] == model]
                
                model_error_counts = model_df['error_type'].value_counts()
                model_error_percentages = model_error_counts / len(model_df) * 100
                
                metrics["by_model"][model] = {
                    "error_counts": model_error_counts.to_dict(),
                    "error_percentages": model_error_percentages.to_dict()
                }
        
        # 3. T·ª∑ l·ªá l·ªói theo prompt type
        metrics["by_prompt_type"] = {}
        for prompt in error_df['prompt_type'].unique():
            prompt_df = error_df[error_df['prompt_type'] == prompt]
            
            prompt_error_counts = prompt_df['error_type'].value_counts()
            prompt_error_percentages = prompt_error_counts / len(prompt_df) * 100
            
            metrics["by_prompt_type"][prompt] = {
                "error_counts": prompt_error_counts.to_dict(),
                "error_percentages": prompt_error_percentages.to_dict()
            }
        
        # 4. T·ª∑ l·ªá l·ªói theo model v√† prompt type
        metrics["by_model_prompt"] = {}
        if 'model_name' in error_df.columns:
            for model in error_df['model_name'].unique():
                metrics["by_model_prompt"][model] = {}
                model_df = error_df[error_df['model_name'] == model]
                
                for prompt in model_df['prompt_type'].unique():
                    prompt_df = model_df[model_df['prompt_type'] == prompt]
                    
                    mp_error_counts = prompt_df['error_type'].value_counts()
                    mp_error_percentages = mp_error_counts / len(prompt_df) * 100
                    
                    metrics["by_model_prompt"][model][prompt] = {
                        "error_counts": mp_error_counts.to_dict(),
                        "error_percentages": mp_error_percentages.to_dict()
                    }
        elif 'model' in error_df.columns:
            for model in error_df['model'].unique():
                metrics["by_model_prompt"][model] = {}
                model_df = error_df[error_df['model'] == model]
                
                for prompt in model_df['prompt_type'].unique():
                    prompt_df = model_df[model_df['prompt_type'] == prompt]
                    
                    mp_error_counts = prompt_df['error_type'].value_counts()
                    mp_error_percentages = mp_error_counts / len(prompt_df) * 100
                    
                    metrics["by_model_prompt"][model][prompt] = {
                        "error_counts": mp_error_counts.to_dict(),
                        "error_percentages": mp_error_percentages.to_dict()
                    }
        
        return metrics

    def evaluate_reasoning_quality(self, 
                               results_df: pd.DataFrame, 
                               sample_size: int = 10,
                               random_seed: int = 42) -> pd.DataFrame:
        """
        ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng suy lu·∫≠n c·ªßa c√°c c√¢u tr·∫£ l·ªùi LLM.
        
        Args:
            results_df (pd.DataFrame): DataFrame k·∫øt qu·∫£ ƒë·ªÉ ƒë√°nh gi√°
            sample_size (int): S·ªë l∆∞·ª£ng m·∫´u ƒë·ªÉ ƒë√°nh gi√°
            random_seed (int): Seed ng·∫´u nhi√™n cho vi·ªác l·∫•y m·∫´u
            
        Returns:
            pd.DataFrame: DataFrame ƒë√£ b·ªï sung ƒë√°nh gi√° suy lu·∫≠n
        """
        # Ki·ªÉm tra xem c√≥ c√°c c·ªôt c·∫ßn thi·∫øt kh√¥ng
        required_cols = ['question_text', 'response', 'correct_answer']
        for col in required_cols:
            if col not in results_df.columns:
                logger.warning(f"Kh√¥ng th·ªÉ ƒë√°nh gi√° suy lu·∫≠n: thi·∫øu c·ªôt '{col}'")
                return results_df
                
        # ƒê·∫£m b·∫£o c√≥ c√°c c·ªôt c·∫ßn thi·∫øt cho ƒë√°nh gi√° suy lu·∫≠n
        if 'reasoning_avg_score' not in results_df.columns:
            results_df['reasoning_avg_score'] = np.nan
            
        if 'reasoning_evaluation' not in results_df.columns:
            results_df['reasoning_evaluation'] = ''
        
        # ƒê·∫£m b·∫£o c√≥ c·ªôt ƒë√°nh gi√° suy lu·∫≠n
        result_cols = [
            'reasoning_logical_flow', 
            'reasoning_mathematical_correctness', 
            'reasoning_clarity', 
            'reasoning_completeness', 
            'reasoning_relevance',
            'reasoning_avg_score',
            'reasoning_evaluation'
        ]
        
        for col in result_cols:
            if col not in results_df.columns:
                results_df[col] = np.nan
                
        # L·ªçc c√°c m·∫´u c√≥ ƒë√°p √°n ƒë√∫ng v√† c√≥ s·ª≠ d·ª•ng prompt y√™u c·∫ßu l·∫≠p lu·∫≠n
        has_reasoning = results_df['prompt_type'].str.contains('thought|cot|reasoning|react', case=False, na=False)
        
        # N·∫øu kh√¥ng ch·ªâ ƒë·ªãnh m·∫´u c·ª• th·ªÉ, ch√∫ng ta ch·ªçn ng·∫´u nhi√™n
        valid_indices = results_df.index[has_reasoning].tolist()
        
        if not valid_indices:
            logger.warning("Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi n√†o ph√π h·ª£p ƒë·ªÉ ƒë√°nh gi√° suy lu·∫≠n")
            return results_df
            
        # L·∫•y m·∫´u ng·∫´u nhi√™n t·ª´ c√°c ch·ªâ s·ªë h·ª£p l·ªá
        random.seed(random_seed)
        
        # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng m·∫´u ƒë·ªÉ ƒë√°nh gi√°
        sample_size = min(sample_size, len(valid_indices))
        sample_indices = random.sample(valid_indices, sample_size)
        
        if self.verbose:
            logger.info(f"ƒê√°nh gi√° suy lu·∫≠n cho {sample_size} m·∫´u ng·∫´u nhi√™n")
        
        # ƒê√°nh gi√° t·ª´ng m·∫´u
        for i, idx in enumerate(sample_indices):
            row = results_df.loc[idx]
            
            question = row['question_text']
            correct_answer = row['correct_answer']
            model_answer = row['response']
            
            if self.verbose:
                logger.info(f"ƒê√°nh gi√° m·∫´u {i+1}/{len(sample_indices)}: model={row['model_name']}, prompt={row['prompt_type']}")
                
            try:
                # ƒê√°nh gi√° suy lu·∫≠n
                eval_result = self._evaluate_single_reasoning(question, correct_answer, model_answer)
                
                # C·∫≠p nh·∫≠t DataFrame
                for criterion, score in eval_result.items():
                    if criterion != 'explanation':
                        col_name = f'reasoning_{criterion}'
                        if col_name in results_df.columns:
                            results_df.at[idx, col_name] = score
                
                # T√≠nh ƒëi·ªÉm trung b√¨nh
                criteria_scores = [v for k, v in eval_result.items() if k != 'explanation' and isinstance(v, (int, float))]
                avg_score = sum(criteria_scores) / len(criteria_scores) if criteria_scores else 0
                results_df.at[idx, 'reasoning_avg_score'] = avg_score
                
                # L∆∞u gi·∫£i th√≠ch ƒë√°nh gi√°
                if 'explanation' in eval_result:
                    results_df.at[idx, 'reasoning_evaluation'] = eval_result['explanation']
                    
            except Exception as e:
                logger.error(f"L·ªói khi ƒë√°nh gi√° suy lu·∫≠n cho m·∫´u {idx}: {str(e)}")
                logger.error(traceback.format_exc())
        
        return results_df
    
    def _evaluate_single_reasoning(self, question, correct_answer, model_answer):
        """
        ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng suy lu·∫≠n cho m·ªôt c·∫∑p c√¢u h·ªèi-c√¢u tr·∫£ l·ªùi.
        
        S·ª≠ d·ª•ng LLM (m·∫∑c ƒë·ªãnh l√† Llama 3 qua Groq API) ƒë·ªÉ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng suy lu·∫≠n
        d·ª±a tr√™n c√°c ti√™u ch√≠ nh∆∞ t√≠nh logic, t√≠nh to√°n ch√≠nh x√°c, r√µ r√†ng, ƒë·∫ßy ƒë·ªß v√† li√™n quan.
        
        Args:
            question (str): C√¢u h·ªèi/b√†i to√°n
            correct_answer (str): C√¢u tr·∫£ l·ªùi ƒë√∫ng/ƒë√°p √°n
            model_answer (str): C√¢u tr·∫£ l·ªùi c·ªßa m√¥ h√¨nh c·∫ßn ƒë√°nh gi√°
            
        Returns:
            Dict: K·∫øt qu·∫£ ƒë√°nh gi√° v·ªõi c√°c ti√™u ch√≠ v√† ƒëi·ªÉm s·ªë
        """
        # Import c·∫ßn thi·∫øt ch·ªâ trong h√†m n√†y ƒë·ªÉ tr√°nh import cycle
        try:
            from core.model_interface import generate_text
        except ImportError:
            import sys
            import os
            sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            from core.model_interface import generate_text
            
        # T·∫°o prompt ƒë√°nh gi√°
        evaluation_prompt = self.reasoning_eval_template.format(
            question=question,
            correct_answer=correct_answer,
            model_answer=model_answer
        )
        
        # L·∫•y ph·∫£n h·ªìi ƒë√°nh gi√° t·ª´ LLM
        if self.verbose:
            logger.info(f"G·ª≠i y√™u c·∫ßu ƒë√°nh gi√° reasoning ƒë·∫øn model: {self.reasoning_model}")
            
        try:
            eval_response = generate_text(
                model_name=self.reasoning_model,
                prompt=evaluation_prompt,
                generation_config={
                    "temperature": 0.1,  # Gi·∫£m temperature ƒë·ªÉ c√≥ k·∫øt qu·∫£ ·ªïn ƒë·ªãnh
                    "max_tokens": 1000    # ƒê·ªß d√†i cho ƒë√°nh gi√° chi ti·∫øt
                }
            )
            
            # N·∫øu response l√† tuple (text, stats), l·∫•y text
            if isinstance(eval_response, tuple) and len(eval_response) > 0:
                eval_response = eval_response[0]
                
            # Parse k·∫øt qu·∫£ ƒë√°nh gi√°
            return self._parse_reasoning_evaluation(eval_response)
            
        except Exception as e:
            logger.error(f"L·ªói khi ƒë√°nh gi√° suy lu·∫≠n v·ªõi LLM: {str(e)}")
            logger.error(traceback.format_exc())
            
            # Tr·∫£ v·ªÅ k·∫øt qu·∫£ m·∫∑c ƒë·ªãnh n·∫øu c√≥ l·ªói
            return {
                'logical_flow': 0,
                'mathematical_correctness': 0,
                'clarity': 0,
                'completeness': 0,
                'relevance': 0,
                'avg_score': 0,
                'explanation': f"L·ªói khi ƒë√°nh gi√°: {str(e)}"
            }
    
    def _parse_reasoning_evaluation(self, eval_response):
        """
        Ph√¢n t√≠ch k·∫øt qu·∫£ ƒë√°nh gi√° t·ª´ LLM ƒë·ªÉ tr√≠ch xu·∫•t ƒëi·ªÉm s·ªë v√† gi·∫£i th√≠ch.
        
        Args:
            eval_response (str): Ph·∫£n h·ªìi t·ª´ m√¥ h√¨nh ƒë√°nh gi√°
            
        Returns:
            Dict: K·∫øt qu·∫£ ƒë√°nh gi√° v·ªõi c√°c ti√™u ch√≠ v√† ƒëi·ªÉm s·ªë
        """
        # Ki·ªÉm tra xem eval_response c√≥ ph·∫£i l√† chu·ªói JSON h·ª£p l·ªá kh√¥ng
        import json
        
        # Kh·ªüi t·∫°o k·∫øt qu·∫£ m·∫∑c ƒë·ªãnh
        result = {
            'logical_flow': 0,
            'mathematical_correctness': 0,
            'clarity': 0,
            'completeness': 0,
            'relevance': 0,
            'avg_score': 0,
            'explanation': ''
        }
        
        # X·ª≠ l√Ω khi eval_response l√† dict (ƒë√£ ƒë∆∞·ª£c parse tr∆∞·ªõc ƒë√≥)
        if isinstance(eval_response, dict):
            # C·∫≠p nh·∫≠t k·∫øt qu·∫£ t·ª´ dict
            for key in result.keys():
                if key in eval_response:
                    result[key] = eval_response[key]
            return result
        
        # Th·ª≠ ph√¢n t√≠ch d∆∞·ªõi d·∫°ng JSON
        if eval_response and isinstance(eval_response, str):
            try:
                # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p nhi·ªÅu JSON objects b·ªã n·ªëi v·ªõi nhau
                if eval_response.count('{') > 1 and eval_response.count('}') > 1:
                    # T√¨m JSON object ƒë·∫ßu ti√™n
                    first_open = eval_response.find('{')
                    first_close = eval_response.find('}', first_open) + 1
                    
                    if first_open >= 0 and first_close > first_open:
                        clean_response = eval_response[first_open:first_close]
                        logger.debug(f"Ph√°t hi·ªán nhi·ªÅu JSON objects, ch·ªâ s·ª≠ d·ª•ng object ƒë·∫ßu ti√™n: {clean_response}")
                        try:
                            json_data = json.loads(clean_response)
                            logger.debug(f"ƒê√£ ph√¢n t√≠ch chu·ªói JSON ƒë·∫ßu ti√™n th√†nh c√¥ng: {json_data}")
                            
                            # C·∫≠p nh·∫≠t k·∫øt qu·∫£
                            for key in result.keys():
                                if key in json_data:
                                    result[key] = json_data[key]
                            
                            return result
                        except json.JSONDecodeError:
                            logger.debug(f"Kh√¥ng th·ªÉ ph√¢n t√≠ch JSON object ƒë·∫ßu ti√™n, ti·∫øp t·ª•c t√¨m ki·∫øm")
                
                # Th·ª≠ ph√¢n t√≠ch to√†n b·ªô chu·ªói nh∆∞ JSON
                try:
                    json_data = json.loads(eval_response)
                    logger.debug(f"ƒê√£ ph√¢n t√≠ch chu·ªói JSON th√†nh c√¥ng: {json_data}")
                    
                    # C·∫≠p nh·∫≠t k·∫øt qu·∫£ t·ª´ d·ªØ li·ªáu JSON
                    for key in result.keys():
                        if key in json_data:
                            result[key] = json_data[key]
                    
                    # Ho√†n t·∫•t v√† tr·∫£ v·ªÅ k·∫øt qu·∫£
                    return result
                    
                except json.JSONDecodeError:
                    # C·ªë g·∫Øng l√†m s·∫°ch chu·ªói v√† th·ª≠ l·∫°i
                    # T√¨m JSON object h·ª£p l·ªá trong chu·ªói
                    import re
                    json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
                    match = re.search(json_pattern, eval_response)
                    
                    if match:
                        potential_json = match.group(0)
                        try:
                            json_data = json.loads(potential_json)
                            logger.debug(f"ƒê√£ ph√¢n t√≠ch chu·ªói JSON ƒë∆∞·ª£c tr√≠ch xu·∫•t th√†nh c√¥ng: {json_data}")
                            
                            # C·∫≠p nh·∫≠t k·∫øt qu·∫£ t·ª´ d·ªØ li·ªáu JSON
                            for key in result.keys():
                                if key in json_data:
                                    result[key] = json_data[key]
                            
                            return result
                        except:
                            logger.debug("Kh√¥ng th·ªÉ ph√¢n t√≠ch JSON sau khi tr√≠ch xu·∫•t, ti·∫øp t·ª•c v·ªõi ph∆∞∆°ng ph√°p regex")
                    else:
                        logger.debug("Kh√¥ng t√¨m th·∫•y chu·ªói JSON h·ª£p l·ªá, ti·∫øp t·ª•c v·ªõi ph∆∞∆°ng ph√°p regex")
                    
            except Exception as e:
                logger.debug(f"L·ªói khi x·ª≠ l√Ω JSON: {str(e)}")
        
        # Ti·∫øp t·ª•c v·ªõi ph∆∞∆°ng ph√°p ph√¢n t√≠ch regex n·∫øu kh√¥ng ph·∫£i JSON
        # C√°c ti√™u ch√≠ c·∫ßn tr√≠ch xu·∫•t
        criteria = {
            'logical_flow': r'(?:T√≠nh h·ª£p l√Ω|ƒê·ªô h·ª£p l√Ω|Logical flow|T√≠nh logic).*?(\d+)[/\s]*5',
            'mathematical_correctness': r'(?:ƒê·ªô ch√≠nh x√°c v·ªÅ m·∫∑t to√°n h·ªçc|Mathematical correctness|T√≠nh to√°n ch√≠nh x√°c).*?(\d+)[/\s]*5',
            'clarity': r'(?:R√µ r√†ng|ƒê·ªô r√µ r√†ng|Clarity).*?(\d+)[/\s]*5',
            'completeness': r'(?:T√≠nh ƒë·∫ßy ƒë·ªß|ƒê·∫ßy ƒë·ªß|Completeness).*?(\d+)[/\s]*5',
            'relevance': r'(?:M·ª©c ƒë·ªô li√™n quan|T√≠nh li√™n quan|Relevance).*?(\d+)[/\s]*5'
        }
        
        # M·∫´u ƒë·ªÉ tr√≠ch xu·∫•t ƒëi·ªÉm trung b√¨nh
        avg_pattern = r'(?:ƒêi·ªÉm trung b√¨nh|Average score|Avg score).*?(\d+\.?\d*)[/\s]*5'
        
        # M·∫´u ƒë·ªÉ tr√≠ch xu·∫•t ph·∫ßn gi·∫£i th√≠ch
        explanation_pattern = r'(?:Gi·∫£i th√≠ch|Explanation)\s*:(.*?)(?:$|(?=\n\s*\d))'
        
        # Tr√≠ch xu·∫•t ƒëi·ªÉm s·ªë cho t·ª´ng ti√™u ch√≠
        import re
        for criterion, pattern in criteria.items():
            match = re.search(pattern, eval_response, re.IGNORECASE | re.DOTALL)
            if match:
                result[criterion] = int(match.group(1))
        
        # Tr√≠ch xu·∫•t ƒëi·ªÉm trung b√¨nh
        avg_match = re.search(avg_pattern, eval_response, re.IGNORECASE | re.DOTALL)
        if avg_match:
            try:
                result['avg_score'] = float(avg_match.group(1))
            except ValueError:
                # T√≠nh to√°n l·∫°i ƒëi·ªÉm trung b√¨nh n·∫øu kh√¥ng th·ªÉ tr√≠ch xu·∫•t
                scores = [result[c] for c in criteria.keys()]
                result['avg_score'] = sum(scores) / len(scores) if scores else 0
        else:
            # T√≠nh to√°n ƒëi·ªÉm trung b√¨nh
            scores = [result[c] for c in criteria.keys()]
            result['avg_score'] = sum(scores) / len(scores) if scores else 0
        
        # Tr√≠ch xu·∫•t ph·∫ßn gi·∫£i th√≠ch
        explanation_match = re.search(explanation_pattern, eval_response, re.IGNORECASE | re.DOTALL)
        if explanation_match:
            result['explanation'] = explanation_match.group(1).strip()
        else:
            # N·∫øu kh√¥ng t√¨m th·∫•y ph·∫ßn gi·∫£i th√≠ch theo m·∫´u,
            # l·∫•y ph·∫ßn cu·ªëi c·ªßa eval_response l√†m gi·∫£i th√≠ch
            lines = eval_response.strip().split('\n')
            for i, line in enumerate(lines):
                if 'gi·∫£i th√≠ch' in line.lower() or 'explanation' in line.lower():
                    result['explanation'] = '\n'.join(lines[i+1:]).strip()
                    break
        
        return result
    
    def _compute_reasoning_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        T√≠nh to√°n c√°c metrics ƒë√°nh gi√° suy lu·∫≠n.
        
        Args:
            df (pd.DataFrame): DataFrame ƒë√£ c√≥ k·∫øt qu·∫£ ƒë√°nh gi√° suy lu·∫≠n
            
        Returns:
            Dict: C√°c metrics c·ªßa ƒë√°nh gi√° suy lu·∫≠n
        """
        metrics = {}
        
        # Ki·ªÉm tra c√°c c·ªôt reasoning c√≥ t·ªìn t·∫°i kh√¥ng
        reasoning_cols = [col for col in df.columns if col.startswith('reasoning_') 
                        and col not in ['reasoning_evaluation', 'reasoning_scores', 'reasoning_scores_str']]
        
        if not reasoning_cols:
            logger.warning("Kh√¥ng t√¨m th·∫•y c√°c c·ªôt reasoning_ ƒë·ªÉ t√≠nh to√°n metrics")
            return metrics
        
        logger.debug(f"T√≠nh to√°n metrics t·ª´ c√°c c·ªôt reasoning: {reasoning_cols}")
        
        # ƒê·∫£m b·∫£o c√°c c·ªôt ch·ª©a d·ªØ li·ªáu s·ªë
        for col in reasoning_cols:
            try:
                # Ki·ªÉm tra xem c·ªôt c√≥ ch·ª©a d·ªØ li·ªáu kh√¥ng ph·∫£i s·ªë kh√¥ng
                if df[col].dtype == 'object':
                    logger.debug(f"Chuy·ªÉn ƒë·ªïi c·ªôt {col} th√†nh s·ªë")
                    # Th·ª≠ chuy·ªÉn ƒë·ªïi c·ªôt th√†nh s·ªë
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            except Exception as e:
                logger.error(f"L·ªói khi chuy·ªÉn ƒë·ªïi c·ªôt {col} th√†nh s·ªë: {e}")
                # Lo·∫°i b·ªè c·ªôt n√†y kh·ªèi danh s√°ch c·∫ßn t√≠nh to√°n
                reasoning_cols.remove(col)
        
        if not reasoning_cols:
            logger.warning("Kh√¥ng c√≤n c·ªôt reasoning_ n√†o ƒë·ªÉ t√≠nh to√°n sau khi chuy·ªÉn ƒë·ªïi")
            return metrics
        
        # 1. Metrics t·ªïng th·ªÉ
        metrics["overall"] = {}
        for col in reasoning_cols:
            criterion = col.replace('reasoning_', '')
            # S·ª≠ d·ª•ng mean tr√™n d·ªØ li·ªáu s·ªë, b·ªè qua gi√° tr·ªã NaN
            metrics["overall"][criterion] = df[col].mean(skipna=True)
        
        # 2. Metrics theo model
        metrics["by_model"] = {}
        for model in df['model_name'].unique():
            metrics["by_model"][model] = {}
            model_df = df[df['model_name'] == model]
            
            for col in reasoning_cols:
                criterion = col.replace('reasoning_', '')
                metrics["by_model"][model][criterion] = model_df[col].mean(skipna=True)
        
        # 3. Metrics theo prompt type
        metrics["by_prompt_type"] = {}
        for prompt in df['prompt_type'].unique():
            metrics["by_prompt_type"][prompt] = {}
            prompt_df = df[df['prompt_type'] == prompt]
            
            for col in reasoning_cols:
                criterion = col.replace('reasoning_', '')
                metrics["by_prompt_type"][prompt][criterion] = prompt_df[col].mean(skipna=True)
        
        # 4. Metrics theo model v√† prompt type
        metrics["by_model_prompt"] = {}
        for model in df['model_name'].unique():
            metrics["by_model_prompt"][model] = {}
            model_df = df[df['model_name'] == model]
            
            for prompt in model_df['prompt_type'].unique():
                metrics["by_model_prompt"][model][prompt] = {}
                prompt_df = model_df[model_df['prompt_type'] == prompt]
                
                for col in reasoning_cols:
                    criterion = col.replace('reasoning_', '')
                    metrics["by_model_prompt"][model][prompt][criterion] = prompt_df[col].mean(skipna=True)
        
        return metrics
    
    def calculate_similarity(self, 
                            df: pd.DataFrame,
                            reference_column: str = 'correct_answer',
                            response_column: str = 'response') -> pd.DataFrame:
        """
        T√≠nh to√°n semantic similarity gi·ªØa c√¢u tr·∫£ l·ªùi m√¥ h√¨nh v√† ƒë√°p √°n chu·∫©n.
        
        Args:
            df (pd.DataFrame): DataFrame k·∫øt qu·∫£
            reference_column (str): T√™n c·ªôt ch·ª©a tham chi·∫øu (th∆∞·ªùng l√† ƒë√°p √°n chu·∫©n)
            response_column (str): T√™n c·ªôt ch·ª©a c√¢u tr·∫£ l·ªùi m√¥ h√¨nh
            
        Returns:
            pd.DataFrame: DataFrame v·ªõi c·ªôt similarity b·ªï sung
        """
        # Ch·ªâ t√≠nh similarity khi c√≥ m√¥ h√¨nh ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
        if not self.similarity_model:
            logger.warning("Kh√¥ng c√≥ m√¥ h√¨nh similarity ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh")
            return df
        
        if self.verbose:
            logger.info(f"üìè T√≠nh to√°n semantic similarity cho {len(df)} m·ª•c")
        
        # Copy DataFrame ƒë·ªÉ kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn d·ªØ li·ªáu g·ªëc
        result_df = df.copy()
        
        # Chu·∫©n b·ªã c·ªôt similarity
        result_df['similarity_score'] = 0.0
        
        try:
            # T√≠nh similarity cho t·ª´ng d√≤ng
            for idx, row in result_df.iterrows():
                reference = row[reference_column]
                response = row[response_column]
                
                # T√≠nh similarity score
                similarity = self._calculate_text_similarity(reference, response)
                result_df.at[idx, 'similarity_score'] = similarity
                
        except Exception as e:
            logger.error(f"L·ªói khi t√≠nh to√°n similarity: {str(e)}")
        
        return result_df
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """
        T√≠nh to√°n similarity gi·ªØa hai ƒëo·∫°n vƒÉn b·∫£n.
        Ph∆∞∆°ng th·ª©c n√†y s·∫Ω ƒë∆∞·ª£c tri·ªÉn khai khi c√≥ m√¥ h√¨nh similarity c·ª• th·ªÉ.
        
        Args:
            text1 (str): VƒÉn b·∫£n th·ª© nh·∫•t
            text2 (str): VƒÉn b·∫£n th·ª© hai
            
        Returns:
            float: ƒêi·ªÉm similarity (0-1)
        """
        # Ch·ªâ d√πng khi c√≥ m√¥ h√¨nh
        if not self.similarity_model:
            return 0.0
        
        # TODO: Tri·ªÉn khai t√≠nh similarity khi c·∫ßn thi·∫øt
        # Hi·ªán t·∫°i, tr·∫£ v·ªÅ gi√° tr·ªã m·∫∑c ƒë·ªãnh
        return 0.0
    
    def export_summary(self, analysis_results: Dict[str, Any], format: str = 'text') -> str:
        """
        Xu·∫•t b·∫£n t√≥m t·∫Øt k·∫øt qu·∫£ ph√¢n t√≠ch theo ƒë·ªãnh d·∫°ng ch·ªâ ƒë·ªãnh.
        
        Args:
            analysis_results (Dict): K·∫øt qu·∫£ ph√¢n t√≠ch t·ª´ h√†m analyze_results
            format (str): ƒê·ªãnh d·∫°ng xu·∫•t ('text', 'markdown', 'json', 'html')
            
        Returns:
            str: T√≥m t·∫Øt k·∫øt qu·∫£ ph√¢n t√≠ch theo ƒë·ªãnh d·∫°ng ch·ªâ ƒë·ªãnh
        """
        if format == 'text':
            return self._export_text_summary(analysis_results)
        elif format == 'markdown':
            return self._export_markdown_summary(analysis_results)
        elif format == 'json':
            import json
            return json.dumps(analysis_results, indent=2, ensure_ascii=False)
        elif format == 'html':
            return self._export_html_summary(analysis_results)
        else:
            return self._export_text_summary(analysis_results)
    
    def _export_text_summary(self, analysis_results: Dict[str, Any]) -> str:
        """
        Xu·∫•t b·∫£n t√≥m t·∫Øt d·∫°ng text.
        
        Args:
            analysis_results (Dict): K·∫øt qu·∫£ ph√¢n t√≠ch
            
        Returns:
            str: T√≥m t·∫Øt d·∫°ng text
        """
        summary = []
        
        # Th√¥ng tin t·ªïng quan
        summary.append("=== K·∫æT QU·∫¢ PH√ÇN T√çCH ===")
        
        # Metrics c∆° b·∫£n
        basic = analysis_results.get('basic_metrics', {})
        summary.append("\n--- METRICS C∆† B·∫¢N ---")
        
        if 'overall_accuracy' in basic:
            summary.append(f"Accuracy t·ªïng th·ªÉ: {basic['overall_accuracy']:.4f}")
        
        if 'average_latency' in basic:
            summary.append(f"Th·ªùi gian trung b√¨nh: {basic['average_latency']:.2f}s")
        
        if 'average_response_length' in basic:
            summary.append(f"ƒê·ªô d√†i ph·∫£n h·ªìi trung b√¨nh: {basic['average_response_length']:.2f} tokens")
        
        # Metrics theo model v√† prompt type
        model_metrics = analysis_results.get('model_prompt_metrics', {})
        if model_metrics:
            summary.append("\n--- METRICS THEO MODEL & PROMPT TYPE ---")
            
            for model, prompts in model_metrics.items():
                summary.append(f"\nModel: {model}")
                
                for prompt, metrics in prompts.items():
                    summary.append(f"  Prompt: {prompt}")
                    
                    if 'accuracy' in metrics:
                        summary.append(f"    - Accuracy: {metrics['accuracy']:.4f}")
                    
                    if 'avg_latency' in metrics:
                        summary.append(f"    - Th·ªùi gian TB: {metrics['avg_latency']:.2f}s")
                    
                    if 'avg_response_length' in metrics:
                        summary.append(f"    - ƒê·ªô d√†i TB: {metrics['avg_response_length']:.2f} tokens")
        
        # Metrics theo lo·∫°i c√¢u h·ªèi
        q_metrics = analysis_results.get('question_type_metrics', {})
        if q_metrics:
            summary.append("\n--- METRICS THEO LO·∫†I C√ÇU H·ªéI ---")
            
            for q_type, metrics in q_metrics.items():
                summary.append(f"\nLo·∫°i: {q_type} (s·ªë l∆∞·ª£ng: {metrics.get('count', 0)})")
                
                if 'accuracy' in metrics:
                    summary.append(f"  - Accuracy: {metrics['accuracy']:.4f}")
                
                if 'avg_latency' in metrics:
                    summary.append(f"  - Th·ªùi gian TB: {metrics['avg_latency']:.2f}s")
        
        return "\n".join(summary)
    
    def _export_markdown_summary(self, analysis_results: Dict[str, Any]) -> str:
        """
        Xu·∫•t b·∫£n t√≥m t·∫Øt d·∫°ng markdown.
        
        Args:
            analysis_results (Dict): K·∫øt qu·∫£ ph√¢n t√≠ch
            
        Returns:
            str: T√≥m t·∫Øt d·∫°ng markdown
        """
        summary = []
        
        # Th√¥ng tin t·ªïng quan
        summary.append("# K·∫æT QU·∫¢ PH√ÇN T√çCH")
        
        # Metrics c∆° b·∫£n
        basic = analysis_results.get('basic_metrics', {})
        summary.append("\n## Metrics C∆° B·∫£n")
        
        if basic:
            summary.append("| Metric | Gi√° tr·ªã |")
            summary.append("|--------|--------|")
            
            if 'overall_accuracy' in basic:
                summary.append(f"| Accuracy t·ªïng th·ªÉ | {basic['overall_accuracy']:.4f} |")
            
            if 'average_latency' in basic:
                summary.append(f"| Th·ªùi gian trung b√¨nh | {basic['average_latency']:.2f}s |")
            
            if 'average_response_length' in basic:
                summary.append(f"| ƒê·ªô d√†i ph·∫£n h·ªìi trung b√¨nh | {basic['average_response_length']:.2f} tokens |")
        
        # Metrics theo model v√† prompt type
        model_metrics = analysis_results.get('model_prompt_metrics', {})
        if model_metrics:
            summary.append("\n## Metrics Theo Model & Prompt Type")
            
            for model, prompts in model_metrics.items():
                summary.append(f"\n### Model: {model}")
                
                summary.append("| Prompt Type | Accuracy | Th·ªùi gian TB | ƒê·ªô d√†i TB |")
                summary.append("|------------|----------|--------------|-----------|")
                
                for prompt, metrics in prompts.items():
                    acc = f"{metrics.get('accuracy', 'N/A'):.4f}" if 'accuracy' in metrics else 'N/A'
                    lat = f"{metrics.get('avg_latency', 'N/A'):.2f}s" if 'avg_latency' in metrics else 'N/A'
                    len_val = f"{metrics.get('avg_response_length', 'N/A'):.2f}" if 'avg_response_length' in metrics else 'N/A'
                    
                    summary.append(f"| {prompt} | {acc} | {lat} | {len_val} |")
        
        # Metrics theo lo·∫°i c√¢u h·ªèi
        q_metrics = analysis_results.get('question_type_metrics', {})
        if q_metrics:
            summary.append("\n## Metrics Theo Lo·∫°i C√¢u H·ªèi")
            
            summary.append("| Lo·∫°i | S·ªë l∆∞·ª£ng | Accuracy | Th·ªùi gian TB |")
            summary.append("|------|----------|----------|--------------|")
            
            for q_type, metrics in q_metrics.items():
                count = metrics.get('count', 'N/A')
                acc = f"{metrics.get('accuracy', 'N/A'):.4f}" if 'accuracy' in metrics else 'N/A'
                lat = f"{metrics.get('avg_latency', 'N/A'):.2f}s" if 'avg_latency' in metrics else 'N/A'
                
                summary.append(f"| {q_type} | {count} | {acc} | {lat} |")
        
        return "\n".join(summary)
    
    def _export_html_summary(self, analysis_results: Dict[str, Any]) -> str:
        """
        Xu·∫•t b·∫£n t√≥m t·∫Øt d·∫°ng HTML.
        
        Args:
            analysis_results (Dict): K·∫øt qu·∫£ ph√¢n t√≠ch
            
        Returns:
            str: T√≥m t·∫Øt d·∫°ng HTML
        """
        # Chuy·ªÉn ƒë·ªïi t·ª´ markdown sang HTML
        try:
            import markdown
            md_summary = self._export_markdown_summary(analysis_results)
            html = markdown.markdown(md_summary, extensions=['tables'])
            
            # B·ªçc trong template HTML c∆° b·∫£n
            return f"""
            <!DOCTYPE html>
            <html>
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>K·∫øt Qu·∫£ Ph√¢n T√≠ch LLM</title>
                <style>
                    body {{ font-family: Arial, sans-serif; line-height: 1.6; padding: 20px; max-width: 1200px; margin: 0 auto; }}
                    table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
                    th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                    th {{ background-color: #f2f2f2; }}
                    tr:nth-child(even) {{ background-color: #f9f9f9; }}
                    h1, h2, h3 {{ color: #333; }}
                </style>
            </head>
            <body>
                {html}
            </body>
            </html>
            """
        except ImportError:
            # Fallback n·∫øu kh√¥ng c√≥ th∆∞ vi·ªán markdown
            return f"<pre>{self._export_text_summary(analysis_results)}</pre>"

    def evaluate_consistency(self, results_df: pd.DataFrame) -> pd.DataFrame:
        """
        ƒê√°nh gi√° t√≠nh nh·∫•t qu√°n trong c√°c self-consistency runs.
        
        Args:
            results_df (pd.DataFrame): DataFrame ch·ª©a k·∫øt qu·∫£ ƒë√°nh gi√°
            
        Returns:
            pd.DataFrame: DataFrame ƒë√£ b·ªï sung ƒë√°nh gi√° t√≠nh nh·∫•t qu√°n
        """
        # Ki·ªÉm tra xem c√≥ c·ªôt response kh√¥ng
        if 'response' not in results_df.columns:
            logger.error("L·ªói khi ƒë√°nh gi√° t√≠nh nh·∫•t qu√°n: thi·∫øu c·ªôt 'response'")
            return results_df
            
        # ƒê·∫£m b·∫£o c√≥ c√°c c·ªôt c·∫ßn thi·∫øt
        if 'consistency_score' not in results_df.columns:
            results_df['consistency_score'] = np.nan
        
        if 'consistency_agreement_rate' not in results_df.columns:
            results_df['consistency_agreement_rate'] = np.nan
        
        if 'consistency_most_common' not in results_df.columns:
            results_df['consistency_most_common'] = ''
        
        # L·ªçc c√°c prompt c√≥ ch·ª©a self-consistency
        self_consistency_mask = results_df['prompt_type'].str.contains('consistency|cot_self_consistency', case=False)
        
        if not self_consistency_mask.any():
            logger.warning("Kh√¥ng t√¨m th·∫•y self-consistency runs ƒë·ªÉ ƒë√°nh gi√° t√≠nh nh·∫•t qu√°n")
            return results_df
        
        # Nh√≥m theo model, question v√† prompt type (lo·∫°i b·ªè ph·∫ßn s·ªë runs n·∫øu c√≥)
        # V√≠ d·ª•: cot_self_consistency_3 v√† cot_self_consistency_5 s·∫Ω ƒë∆∞·ª£c nh√≥m chung
        results_df['base_prompt_type'] = results_df['prompt_type'].str.replace(r'_\d+$', '', regex=True)
        
        # S·ª≠ d·ª•ng model_name thay v√¨ model n·∫øu c√≥
        model_col = 'model_name' if 'model_name' in results_df.columns else 'model'
        
        # X√°c ƒë·ªãnh c√°c nh√≥m ch·∫°y self-consistency
        groups = results_df[self_consistency_mask].groupby([model_col, 'question_id', 'base_prompt_type'])
        
        # X·ª≠ l√Ω t·ª´ng nh√≥m
        for (model, question_id, prompt_type), group in groups:
            # B·ªè qua n·∫øu ch·ªâ c√≥ m·ªôt k·∫øt qu·∫£
            if len(group) <= 1:
                continue
            
            # L·∫•y t·∫•t c·∫£ c√°c c√¢u tr·∫£ l·ªùi trong nh√≥m
            responses = group['response'].tolist()
            final_answers = group['final_answer'].tolist() if 'final_answer' in group.columns else responses
            
            # T√≠nh to√°n t·ª∑ l·ªá nh·∫•t qu√°n
            from collections import Counter
            answer_counts = Counter(final_answers)
            
            # X√°c ƒë·ªãnh c√¢u tr·∫£ l·ªùi ph·ªï bi·∫øn nh·∫•t
            most_common_answer, most_common_count = answer_counts.most_common(1)[0]
            agreement_rate = most_common_count / len(final_answers)
            
            # T√≠nh ƒëi·ªÉm nh·∫•t qu√°n: 1 n·∫øu ho√†n to√†n nh·∫•t qu√°n, gi·∫£m d·∫ßn khi c√≥ nhi·ªÅu c√¢u tr·∫£ l·ªùi kh√°c nhau
            unique_answers = len(answer_counts)
            consistency_score = 1.0 if unique_answers == 1 else (most_common_count / len(final_answers))
            
            # C·∫≠p nh·∫≠t ƒëi·ªÉm nh·∫•t qu√°n cho t·ª´ng d√≤ng trong nh√≥m
            for idx in group.index:
                results_df.at[idx, 'consistency_score'] = consistency_score
                results_df.at[idx, 'consistency_agreement_rate'] = agreement_rate
                results_df.at[idx, 'consistency_most_common'] = most_common_answer
        
        # X√≥a c·ªôt t·∫°m base_prompt_type
        results_df = results_df.drop('base_prompt_type', axis=1)
        
        return results_df
    
    def _compute_consistency_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        T√≠nh to√°n metrics cho ƒë√°nh gi√° t√≠nh nh·∫•t qu√°n.
        
        Args:
            df (pd.DataFrame): DataFrame ƒë√£ c√≥ ƒë√°nh gi√° t√≠nh nh·∫•t qu√°n
            
        Returns:
            Dict: Metrics li√™n quan ƒë·∫øn t√≠nh nh·∫•t qu√°n
        """
        metrics = {}
        
        # L·ªçc c√°c d√≤ng c√≥ ƒë√°nh gi√° t√≠nh nh·∫•t qu√°n
        consistency_df = df[~df['consistency_score'].isna()]
        
        if len(consistency_df) == 0:
            return metrics
        
        # 1. Metrics t·ªïng th·ªÉ
        metrics["overall"] = {
            "avg_consistency_score": consistency_df['consistency_score'].mean(),
            "avg_agreement_rate": consistency_df['consistency_agreement_rate'].mean()
        }
        
        # 2. Metrics theo model
        metrics["by_model"] = {}
        
        # S·ª≠ d·ª•ng model_name thay v√¨ model n·∫øu c√≥
        model_col = 'model_name' if 'model_name' in consistency_df.columns else 'model'
        
        if model_col in consistency_df.columns:
            for model in consistency_df[model_col].unique():
                model_df = consistency_df[consistency_df[model_col] == model]
                metrics["by_model"][model] = {
                    "avg_consistency_score": model_df['consistency_score'].mean(),
                    "avg_agreement_rate": model_df['consistency_agreement_rate'].mean()
                }
        
        # 3. Metrics theo prompt type
        metrics["by_prompt_type"] = {}
        if 'prompt_type' in consistency_df.columns:
            for prompt in consistency_df['prompt_type'].unique():
                prompt_df = consistency_df[consistency_df['prompt_type'] == prompt]
                metrics["by_prompt_type"][prompt] = {
                    "avg_consistency_score": prompt_df['consistency_score'].mean(),
                    "avg_agreement_rate": prompt_df['consistency_agreement_rate'].mean()
                }
        
        # 4. Metrics theo model v√† prompt type
        metrics["by_model_prompt"] = {}
        if model_col in consistency_df.columns and 'prompt_type' in consistency_df.columns:
            for model in consistency_df[model_col].unique():
                metrics["by_model_prompt"][model] = {}
                model_df = consistency_df[consistency_df[model_col] == model]
                
                for prompt in model_df['prompt_type'].unique():
                    prompt_df = model_df[model_df['prompt_type'] == prompt]
                    metrics["by_model_prompt"][model][prompt] = {
                        "avg_consistency_score": prompt_df['consistency_score'].mean(),
                        "avg_agreement_rate": prompt_df['consistency_agreement_rate'].mean()
                    }
        
        return metrics

    def evaluate_completeness(self, 
                          results_df: pd.DataFrame, 
                          sample_size: int = 50,
                          random_seed: int = 42) -> pd.DataFrame:
        """
        ƒê√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß c·ªßa c√°c c√¢u tr·∫£ l·ªùi.
        
        Args:
            results_df (pd.DataFrame): DataFrame ch·ª©a k·∫øt qu·∫£ ƒë√°nh gi√°
            sample_size (int): S·ªë l∆∞·ª£ng m·∫´u c·∫ßn ƒë√°nh gi√°
            random_seed (int): Seed ng·∫´u nhi√™n cho vi·ªác l·∫•y m·∫´u
            
        Returns:
            pd.DataFrame: DataFrame ƒë√£ b·ªï sung ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß
        """
        # Ki·ªÉm tra xem c√≥ c√°c c·ªôt c·∫ßn thi·∫øt kh√¥ng
        required_cols = ['question_text', 'response']
        missing_cols = [col for col in required_cols if col not in results_df.columns]
        if missing_cols:
            logger.warning(f"Kh√¥ng th·ªÉ ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß: thi·∫øu c√°c c·ªôt {missing_cols}")
            return results_df
                
        # ƒê·∫£m b·∫£o c√≥ c√°c c·ªôt c·∫ßn thi·∫øt
        if 'completeness_score' not in results_df.columns:
            results_df['completeness_score'] = np.nan
            
        if 'completeness_evaluation' not in results_df.columns:
            results_df['completeness_evaluation'] = ''
            
        # L·ªçc c√°c h√†ng c√≥ c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi
        valid_rows = (
            ~results_df['question_text'].isna() & 
            ~results_df['response'].isna() &
            (results_df['response'] != '') &  # Th√™m ƒëi·ªÅu ki·ªán check chu·ªói r·ªóng
            results_df['completeness_score'].isna()  # Ch∆∞a ƒë∆∞·ª£c ƒë√°nh gi√°
        )
        
        valid_indices = results_df[valid_rows].index.tolist()
        
        if not valid_indices:
            logger.warning("Kh√¥ng c√≥ m·∫´u ph√π h·ª£p ƒë·ªÉ ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß")
            return results_df
            
        # L·∫•y m·∫´u ng·∫´u nhi√™n n·∫øu c·∫ßn
        np.random.seed(random_seed)
        if len(valid_indices) > sample_size:
            sample_indices = np.random.choice(valid_indices, size=sample_size, replace=False)
        else:
            sample_indices = valid_indices
            
        logger.info(f"ƒê√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß cho {len(sample_indices)} m·∫´u")
        
        # ƒê√°nh gi√° t·ª´ng m·∫´u
        for i, idx in enumerate(sample_indices):
            row = results_df.loc[idx]
            
            question = row['question_text']
            model_answer = row['response']
            
            if self.verbose:
                model_name = row['model_name'] if 'model_name' in row else row.get('model', 'unknown')
                prompt_type = row['prompt_type'] if 'prompt_type' in row else 'unknown'
                logger.info(f"ƒê√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß m·∫´u {i+1}/{len(sample_indices)}: model={model_name}, prompt={prompt_type}")
                
            try:
                # ƒê√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß
                eval_result = self._evaluate_single_completeness(question, model_answer)
                
                # C·∫≠p nh·∫≠t DataFrame
                results_df.at[idx, 'completeness_score'] = eval_result.get('score', 0.0)
                results_df.at[idx, 'completeness_evaluation'] = eval_result.get('explanation', '')
                    
            except Exception as e:
                logger.error(f"L·ªói khi ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß cho m·∫´u {idx}: {str(e)}")
                logger.error(traceback.format_exc())
        
        return results_df
    
    def _evaluate_single_completeness(self, question: str, model_answer: str) -> Dict[str, Any]:
        """
        ƒê√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß cho m·ªôt m·∫´u c√¢u h·ªèi/c√¢u tr·∫£ l·ªùi.
        
        Args:
            question (str): C√¢u h·ªèi
            model_answer (str): C√¢u tr·∫£ l·ªùi c·ªßa model
            
        Returns:
            Dict: K·∫øt qu·∫£ ƒë√°nh gi√° (ƒëi·ªÉm v√† gi·∫£i th√≠ch)
        """
        try:
            # C·∫Øt b·ªõt ƒë·ªô d√†i n·∫øu qu√° d√†i
            max_length = 4000  # Gi·ªõi h·∫°n ƒë·ªô d√†i ƒë·ªÉ tr√°nh v∆∞·ª£t qu√° context window
            if len(model_answer) > max_length:
                logger.warning(f"C·∫Øt b·ªõt c√¢u tr·∫£ l·ªùi ({len(model_answer)} -> {max_length} k√Ω t·ª±)")
                model_answer = model_answer[:max_length] + "..."
            
            # T·∫°o prompt ƒë√°nh gi√°
            if self.language.lower() == "vietnamese":
                eval_prompt = """
B·∫°n l√† m·ªôt chuy√™n gia ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß c·ªßa c√¢u tr·∫£ l·ªùi. H√£y ƒë√°nh gi√° xem c√¢u tr·∫£ l·ªùi c√≥ gi·∫£i quy·∫øt t·∫•t c·∫£ c√°c kh√≠a c·∫°nh c·ªßa c√¢u h·ªèi hay kh√¥ng.

C√ÇU H·ªéI:
{question}

C√ÇU TR·∫¢ L·ªúI:
{answer}

H√£y ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß c·ªßa c√¢u tr·∫£ l·ªùi theo thang ƒëi·ªÉm t·ª´ 0-10 (10 l√† ho√†n to√†n ƒë·∫ßy ƒë·ªß). X√°c ƒë·ªãnh c√°c kh√≠a c·∫°nh c·ªßa c√¢u h·ªèi v√† ki·ªÉm tra xem c√¢u tr·∫£ l·ªùi ƒë√£ ƒë·ªÅ c·∫≠p ƒë·∫øn t·∫•t c·∫£ c√°c kh√≠a c·∫°nh ƒë√≥ ch∆∞a.

ƒêi·ªÉm t√≠nh ƒë·∫ßy ƒë·ªß: ?/10

Ph√¢n t√≠ch chi ti·∫øt:
1. C√°c kh√≠a c·∫°nh c·ªßa c√¢u h·ªèi:
2. C√°c kh√≠a c·∫°nh ƒë∆∞·ª£c tr·∫£ l·ªùi:
3. C√°c kh√≠a c·∫°nh ch∆∞a ƒë∆∞·ª£c tr·∫£ l·ªùi (n·∫øu c√≥):
"""
            else:
                eval_prompt = """
You are an expert evaluating the completeness of answers. Assess whether the answer addresses all aspects of the question.

QUESTION:
{question}

ANSWER:
{answer}

Evaluate the completeness of the answer on a scale of 0-10 (10 being completely comprehensive). Identify the aspects of the question and check if the answer addresses all of them.

Completeness score: ?/10

Detailed analysis:
1. Aspects of the question:
2. Aspects addressed in the answer:
3. Aspects not addressed (if any):
"""
            
            # Format prompt
            eval_prompt = eval_prompt.format(question=question, answer=model_answer)
            
            # S·ª≠ d·ª•ng model API ƒë·ªÉ ƒë√°nh gi√°
            use_groq = self.reasoning_config.get("use_groq", True)
            if use_groq:
                # S·ª≠ d·ª•ng Groq API
                from core.model_interface import generate_text
                
                # L·∫•y t√™n model Groq
                model_name = "groq"
                config = {
                    "model": self.reasoning_config.get("models", {}).get(
                        "completeness_evaluation", "llama3-70b-8192"
                    ),
                    "temperature": 0.2,  # Th·∫•p ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n
                    "max_tokens": 1024
                }
                
                # G·ªçi API
                logger.debug("ƒê√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß b·∫±ng Groq API")
                response_text, stats = generate_text(model_name, eval_prompt, config)
                
                if stats.get("has_error", False):
                    logger.error(f"L·ªói khi g·ªçi Groq API: {stats.get('error_message')}")
                    # Fallback v·ªÅ gi√° tr·ªã m·∫∑c ƒë·ªãnh
                    return {
                        "score": 5.0,
                        "explanation": f"[L·ªói ƒë√°nh gi√°: {stats.get('error_message')}]"
                    }
            else:
                # TODO: S·ª≠ d·ª•ng model kh√°c n·∫øu c·∫ßn
                logger.warning("Ch·ªâ h·ªó tr·ª£ Groq API ƒë·ªÉ ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß")
                response_text = ""
                
            # Ph√¢n t√≠ch k·∫øt qu·∫£ ƒë√°nh gi√°
            eval_result = self._parse_completeness_evaluation(response_text)
            
            return eval_result
                
        except Exception as e:
            logger.error(f"L·ªói khi ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß: {str(e)}")
            logger.error(traceback.format_exc())
            
            # Tr·∫£ v·ªÅ gi√° tr·ªã m·∫∑c ƒë·ªãnh
            return {
                "score": 5.0,
                "explanation": f"[L·ªói ƒë√°nh gi√°: {str(e)}]"
            }
    
    def _parse_completeness_evaluation(self, eval_response: str) -> Dict[str, Any]:
        """
        Ph√¢n t√≠ch k·∫øt qu·∫£ ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß t·ª´ API.
        
        Args:
            eval_response (str): Ph·∫£n h·ªìi t·ª´ API
            
        Returns:
            Dict: Dictionary ch·ª©a ƒëi·ªÉm s·ªë v√† gi·∫£i th√≠ch
        """
        result = {
            "score": 5.0,  # Gi√° tr·ªã m·∫∑c ƒë·ªãnh
            "explanation": eval_response
        }
        
        if not eval_response:
            return result
            
        # T√¨m ƒëi·ªÉm ƒë√°nh gi√° t·ª´ ph·∫£n h·ªìi
        score_patterns = [
            r"(?:ƒëi·ªÉm|score)[^\d]*(\d+(?:\.\d+)?)/10",
            r"(?:ƒëi·ªÉm|score)[^\d]*:?[^\d]*(\d+(?:\.\d+)?)"
        ]
        
        # T√¨m ƒëi·ªÉm s·ªë
        for pattern in score_patterns:
            match = re.search(pattern, eval_response, re.IGNORECASE)
            if match:
                try:
                    score = float(match.group(1))
                    # Chuy·ªÉn v·ªÅ thang ƒëi·ªÉm 0-1
                    normalized_score = score / 10.0
                    result["score"] = max(0.0, min(1.0, normalized_score))
                    break
                except (ValueError, IndexError):
                    continue
        
        return result
    
    def _compute_completeness_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        T√≠nh to√°n metrics cho ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß.
        
        Args:
            df (pd.DataFrame): DataFrame ƒë√£ c√≥ ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß
            
        Returns:
            Dict: Metrics li√™n quan ƒë·∫øn t√≠nh ƒë·∫ßy ƒë·ªß
        """
        metrics = {}
        
        # L·ªçc c√°c d√≤ng c√≥ ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß
        completeness_df = df[~df['completeness_score'].isna()]
        
        if len(completeness_df) == 0:
            return metrics
        
        # 1. Metrics t·ªïng th·ªÉ
        metrics["overall"] = {
            "avg_completeness_score": completeness_df['completeness_score'].mean(),
            "high_completeness_rate": (completeness_df['completeness_score'] >= 0.8).mean()
        }
        
        # 2. Metrics theo model
        metrics["by_model"] = {}
        
        # X√°c ƒë·ªãnh c·ªôt model (model_name ho·∫∑c model)
        model_col = 'model_name' if 'model_name' in completeness_df.columns else 'model'
        
        if model_col in completeness_df.columns:
            for model in completeness_df[model_col].unique():
                model_df = completeness_df[completeness_df[model_col] == model]
                metrics["by_model"][model] = {
                    "avg_completeness_score": model_df['completeness_score'].mean(),
                    "high_completeness_rate": (model_df['completeness_score'] >= 0.8).mean()
                }
        
        # 3. Metrics theo prompt type
        metrics["by_prompt_type"] = {}
        if 'prompt_type' in completeness_df.columns:
            for prompt in completeness_df['prompt_type'].unique():
                prompt_df = completeness_df[completeness_df['prompt_type'] == prompt]
                metrics["by_prompt_type"][prompt] = {
                    "avg_completeness_score": prompt_df['completeness_score'].mean(),
                    "high_completeness_rate": (prompt_df['completeness_score'] >= 0.8).mean()
                }
        
        # 4. Metrics theo model v√† prompt type
        metrics["by_model_prompt"] = {}
        if model_col in completeness_df.columns and 'prompt_type' in completeness_df.columns:
            for model in completeness_df[model_col].unique():
                metrics["by_model_prompt"][model] = {}
                model_df = completeness_df[completeness_df[model_col] == model]
                
                for prompt in model_df['prompt_type'].unique():
                    prompt_df = model_df[model_df['prompt_type'] == prompt]
                    metrics["by_model_prompt"][model][prompt] = {
                        "avg_completeness_score": prompt_df['completeness_score'].mean(),
                        "high_completeness_rate": (prompt_df['completeness_score'] >= 0.8).mean()
                    }
        
        return metrics

    def evaluate_similarity(self, results_df: pd.DataFrame) -> pd.DataFrame:
        """
        T√≠nh to√°n c√°c metrics ƒëo l∆∞·ªùng ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa c√¢u tr·∫£ l·ªùi c·ªßa model v√† ƒë√°p √°n chu·∫©n.
        S·ª≠ d·ª•ng c√°c ph∆∞∆°ng ph√°p:
        - ROUGE (ƒëo l∆∞·ªùng s·ª± tr√πng l·∫∑p n-gram)
        - BLEU (ƒëo l∆∞·ªùng ƒë·ªô ch√≠nh x√°c n-gram)
        - Cosine similarity c·ªßa embeddings (n·∫øu c√≥ similarity_model)
        
        Args:
            results_df (pd.DataFrame): DataFrame ch·ª©a k·∫øt qu·∫£ ƒë√°nh gi√°
            
        Returns:
            pd.DataFrame: DataFrame ƒë√£ b·ªï sung c√°c metrics ƒëo l∆∞·ªùng ƒë·ªô t∆∞∆°ng ƒë·ªìng
        """
        # ƒê·∫£m b·∫£o c√≥ c·ªôt ƒë√°p √°n chu·∫©n v√† c√¢u tr·∫£ l·ªùi
        if 'correct_answer' not in results_df.columns or 'response' not in results_df.columns:
            logger.warning("Thi·∫øu c·ªôt 'correct_answer' ho·∫∑c 'response' ƒë·ªÉ t√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng")
            return results_df
        
        # Th√™m c√°c c·ªôt c·∫ßn thi·∫øt n·∫øu ch∆∞a c√≥
        for col in ['rouge_score', 'bleu_score', 'embedding_similarity']:
            if col not in results_df.columns:
                results_df[col] = np.nan
        
        # L·ªçc c√°c h√†ng c√≥ ƒë√°p √°n chu·∫©n v√† c√¢u tr·∫£ l·ªùi
        valid_rows = ~results_df['correct_answer'].isna() & ~results_df['response'].isna()
        
        if not valid_rows.any():
            logger.warning("Kh√¥ng c√≥ m·∫´u ph√π h·ª£p ƒë·ªÉ t√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng")
            return results_df
        
        # T√≠nh to√°n ROUGE v√† BLEU scores
        try:
            # N·∫øu ch∆∞a c√≥ th∆∞ vi·ªán ROUGE ho·∫∑c BLEU, c·∫ßn th√™m v√†o requirements.txt
            from rouge import Rouge
            import nltk
            from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
            
            # Download NLTK data n·∫øu c·∫ßn
            try:
                nltk.data.find('tokenizers/punkt')
            except LookupError:
                nltk.download('punkt')
            
            # Kh·ªüi t·∫°o ROUGE
            rouge = Rouge()
            smoothing = SmoothingFunction().method1
            
            # X·ª≠ l√Ω t·ª´ng h√†ng
            for idx in results_df[valid_rows].index:
                reference = results_df.at[idx, 'correct_answer']
                hypothesis = results_df.at[idx, 'response']
                
                # T√≠nh ROUGE score
                try:
                    rouge_scores = rouge.get_scores(hypothesis, reference)
                    # L·∫•y trung b√¨nh rouge-1, rouge-2, rouge-l f1-scores
                    rouge_f1 = (rouge_scores[0]['rouge-1']['f'] + 
                               rouge_scores[0]['rouge-2']['f'] + 
                               rouge_scores[0]['rouge-l']['f']) / 3
                    results_df.at[idx, 'rouge_score'] = rouge_f1
                except Exception as e:
                    logger.debug(f"L·ªói khi t√≠nh ROUGE score: {str(e)}")
                
                # T√≠nh BLEU score
                try:
                    # Tokenize c√¢u
                    reference_tokens = nltk.word_tokenize(reference.lower())
                    hypothesis_tokens = nltk.word_tokenize(hypothesis.lower())
                    
                    # T√≠nh BLEU (s·ª≠ d·ª•ng smoothing ƒë·ªÉ tr√°nh l·ªói khi kh√¥ng c√≥ n-gram kh·ªõp)
                    bleu_score = sentence_bleu([reference_tokens], hypothesis_tokens, 
                                              smoothing_function=smoothing)
                    results_df.at[idx, 'bleu_score'] = bleu_score
                except Exception as e:
                    logger.debug(f"L·ªói khi t√≠nh BLEU score: {str(e)}")
                
        except ImportError as e:
            logger.warning(f"Kh√¥ng th·ªÉ t√≠nh ROUGE/BLEU scores do thi·∫øu th∆∞ vi·ªán: {str(e)}")
        
        # T√≠nh to√°n embedding similarity n·∫øu c√≥ similarity model
        if self.similarity_model:
            # Th·ª±c hi·ªán t√≠nh to√°n embedding similarity
            try:
                for idx in results_df[valid_rows].index:
                    reference = results_df.at[idx, 'correct_answer']
                    hypothesis = results_df.at[idx, 'response']
                    
                    similarity = self._calculate_embedding_similarity(reference, hypothesis)
                    results_df.at[idx, 'embedding_similarity'] = similarity
            except Exception as e:
                logger.warning(f"L·ªói khi t√≠nh embedding similarity: {str(e)}")
        
        return results_df
    
    def _calculate_embedding_similarity(self, text1: str, text2: str) -> float:
        """
        T√≠nh to√°n cosine similarity gi·ªØa embeddings c·ªßa hai ƒëo·∫°n vƒÉn b·∫£n.
        
        Args:
            text1 (str): VƒÉn b·∫£n th·ª© nh·∫•t
            text2 (str): VƒÉn b·∫£n th·ª© hai
            
        Returns:
            float: Cosine similarity (0-1)
        """
        if not self.similarity_model:
            return 0.0
            
        try:
            # ƒê√¢y l√† ph·∫ßn tri·ªÉn khai t√πy thu·ªôc v√†o model v√† framework s·ª≠ d·ª•ng
            # V√≠ d·ª• v·ªõi sentence-transformers
            from sentence_transformers import SentenceTransformer
            import numpy as np
            from sklearn.metrics.pairwise import cosine_similarity
            
            # Ki·ªÉm tra xem similarity_model c√≥ ph·∫£i l√† ƒë∆∞·ªùng d·∫´n ho·∫∑c t√™n model kh√¥ng
            if isinstance(self.similarity_model, str):
                # Lazy loading model
                if not hasattr(self, '_embedding_model'):
                    self._embedding_model = SentenceTransformer(self.similarity_model)
                
                # T√≠nh embeddings
                embedding1 = self._embedding_model.encode([text1])[0]
                embedding2 = self._embedding_model.encode([text2])[0]
                
                # T√≠nh cosine similarity
                similarity = cosine_similarity([embedding1], [embedding2])[0][0]
                return float(similarity)
            else:
                # N·∫øu similarity_model ƒë√£ l√† instance c·ªßa model
                embedding1 = self.similarity_model.encode([text1])[0]
                embedding2 = self.similarity_model.encode([text2])[0]
                
                similarity = cosine_similarity([embedding1], [embedding2])[0][0]
                return float(similarity)
                
        except Exception as e:
            logger.error(f"L·ªói khi t√≠nh embedding similarity: {str(e)}")
            return 0.0
    
    def _compute_similarity_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        T√≠nh to√°n c√°c metrics li√™n quan ƒë·∫øn ƒë·ªô t∆∞∆°ng ƒë·ªìng v·ªõi ƒë√°p √°n chu·∫©n.
        
        Args:
            df (pd.DataFrame): DataFrame ƒë√£ c√≥ c√°c metrics ƒëo l∆∞·ªùng ƒë·ªô t∆∞∆°ng ƒë·ªìng
            
        Returns:
            Dict: Metrics li√™n quan ƒë·∫øn ƒë·ªô t∆∞∆°ng ƒë·ªìng
        """
        metrics = {}
        
        # Danh s√°ch c√°c c·ªôt similarity metrics
        similarity_cols = ['rouge_score', 'bleu_score', 'embedding_similarity']
        
        # L·ªçc c√°c h√†ng c√≥ √≠t nh·∫•t m·ªôt metric similarity ƒë∆∞·ª£c t√≠nh to√°n
        similarity_df = df[df[similarity_cols].notna().any(axis=1)]
        
        if len(similarity_df) == 0:
            return metrics
        
        # 1. Metrics t·ªïng th·ªÉ
        metrics["overall"] = {}
        for col in similarity_cols:
            if col in similarity_df.columns and similarity_df[col].notna().any():
                metrics["overall"][col] = similarity_df[col].mean()
        
        # 2. Metrics theo model
        metrics["by_model"] = {}
        for model in similarity_df['model'].unique():
            metrics["by_model"][model] = {}
            model_df = similarity_df[similarity_df['model'] == model]
            
            for col in similarity_cols:
                if col in model_df.columns and model_df[col].notna().any():
                    metrics["by_model"][model][col] = model_df[col].mean()
        
        # 3. Metrics theo prompt type
        metrics["by_prompt_type"] = {}
        for prompt in similarity_df['prompt_type'].unique():
            metrics["by_prompt_type"][prompt] = {}
            prompt_df = similarity_df[similarity_df['prompt_type'] == prompt]
            
            for col in similarity_cols:
                if col in prompt_df.columns and prompt_df[col].notna().any():
                    metrics["by_prompt_type"][prompt][col] = prompt_df[col].mean()
        
        # 4. Metrics theo model v√† prompt type
        metrics["by_model_prompt"] = {}
        for model in similarity_df['model'].unique():
            metrics["by_model_prompt"][model] = {}
            model_df = similarity_df[similarity_df['model'] == model]
            
            for prompt in model_df['prompt_type'].unique():
                metrics["by_model_prompt"][model][prompt] = {}
                prompt_df = model_df[model_df['prompt_type'] == prompt]
                
                for col in similarity_cols:
                    if col in prompt_df.columns and prompt_df[col].notna().any():
                        metrics["by_model_prompt"][model][prompt][col] = prompt_df[col].mean()
        
        # 5. T∆∞∆°ng quan gi·ªØa accuracy v√† c√°c metrics similarity
        if 'is_correct' in similarity_df.columns:
            metrics["correlation"] = {}
            
            for col in similarity_cols:
                if col in similarity_df.columns and similarity_df[col].notna().any():
                    corr = similarity_df[['is_correct', col]].corr().iloc[0, 1]
                    metrics["correlation"][f"{col}_vs_accuracy"] = corr
        
        return metrics

    def _compute_accuracy_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        T√≠nh to√°n c√°c metrics li√™n quan ƒë·∫øn ƒë·ªô ch√≠nh x√°c (Accuracy).
        
        Args:
            df (pd.DataFrame): DataFrame ch·ª©a k·∫øt qu·∫£ ƒë√°nh gi√°
            
        Returns:
            Dict[str, Any]: C√°c metrics li√™n quan ƒë·∫øn accuracy
        """
        metrics = {}
        
        if 'is_correct' not in df.columns:
            logger.warning("Kh√¥ng th·ªÉ t√≠nh accuracy metrics: thi·∫øu c·ªôt is_correct")
            return metrics
        
        # T√≠nh overall accuracy
        metrics['overall_accuracy'] = df['is_correct'].mean()
        
        # X√°c ƒë·ªãnh c·ªôt model (c√≥ th·ªÉ l√† 'model_name' ho·∫∑c 'model')
        model_col = 'model_name' if 'model_name' in df.columns else 'model'
        
        # T√≠nh accuracy theo model v√† prompt type
        if model_col in df.columns:
            accuracy_by_model = df.groupby(model_col)['is_correct'].mean().to_dict()
            accuracy_by_model_prompt = df.groupby([model_col, 'prompt_type'])['is_correct'].mean().unstack().to_dict('index')
            
            metrics['accuracy_by_model'] = accuracy_by_model
            metrics['accuracy_by_model_prompt'] = accuracy_by_model_prompt
        
        if 'prompt_type' in df.columns:
            accuracy_by_prompt = df.groupby('prompt_type')['is_correct'].mean().to_dict()
            metrics['accuracy_by_prompt'] = accuracy_by_prompt
        
        # T√≠nh F1 score n·∫øu c√≥ th·ªÉ
        try:
            from sklearn.metrics import f1_score
            if 'is_correct' in df.columns and 'expected_answer' in df.columns and 'response' in df.columns:
                # Th·ª±c hi·ªán t√≠nh to√°n F1 score cho t·ª´ng model/prompt
                f1_scores = {}
                for (model, prompt), group in df.groupby(['model_name', 'prompt_type']):
                    if len(group) > 0:
                        f1 = self._calculate_f1_score(group)
                        f1_scores[(model, prompt)] = f1
                
                metrics['f1_scores'] = f1_scores
        except (ImportError, Exception) as e:
            logger.warning(f"Kh√¥ng th·ªÉ t√≠nh F1 score: {str(e)}")
        
        return metrics
    
    def _calculate_f1_score(self, group_df: pd.DataFrame) -> float:
        """
        T√≠nh F1 score cho m·ªôt nh√≥m k·∫øt qu·∫£.
        
        Args:
            group_df (pd.DataFrame): DataFrame ch·ª©a m·ªôt nh√≥m k·∫øt qu·∫£
            
        Returns:
            float: F1 score
        """
        # ƒê∆°n gi·∫£n h√≥a: coi is_correct nh∆∞ true positive/negative
        try:
            from sklearn.metrics import f1_score
            return f1_score([1] * len(group_df), group_df['is_correct'])
        except Exception:
            # Fallback: t√≠nh th·ªß c√¥ng
            tp = group_df['is_correct'].sum()
            total = len(group_df)
            precision = tp / total if total > 0 else 0
            recall = tp / total if total > 0 else 0
            
            return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    def _compute_difficulty_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Ph√¢n t√≠ch hi·ªáu su·∫•t d·ª±a tr√™n c√°c m·ª©c ƒë·ªô kh√≥ kh√°c nhau.
        
        Args:
            df (pd.DataFrame): DataFrame ch·ª©a k·∫øt qu·∫£ ƒë√°nh gi√°
            
        Returns:
            Dict[str, Any]: C√°c metrics v·ªÅ hi·ªáu su·∫•t theo ƒë·ªô kh√≥
        """
        metrics = {}
        
        if 'difficulty' not in df.columns or 'is_correct' not in df.columns:
            logger.warning("Kh√¥ng th·ªÉ t√≠nh difficulty metrics: thi·∫øu c·ªôt difficulty ho·∫∑c is_correct")
            return metrics
        
        # ƒê·∫£m b·∫£o c·ªôt difficulty c√≥ gi√° tr·ªã
        df_valid = df.dropna(subset=['difficulty', 'is_correct'])
        
        if len(df_valid) == 0:
            logger.warning("Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá ƒë·ªÉ t√≠nh difficulty metrics")
            return metrics
        
        # T√≠nh accuracy theo ƒë·ªô kh√≥
        accuracy_by_difficulty = df_valid.groupby('difficulty')['is_correct'].mean().to_dict()
        metrics['accuracy_by_difficulty'] = accuracy_by_difficulty
        
        # T√≠nh accuracy theo model v√† ƒë·ªô kh√≥
        accuracy_by_model_difficulty = df_valid.groupby(['model_name', 'difficulty'])['is_correct'].mean().unstack().to_dict('index')
        metrics['accuracy_by_model_difficulty'] = accuracy_by_model_difficulty
        
        # T√≠nh accuracy theo prompt v√† ƒë·ªô kh√≥
        accuracy_by_prompt_difficulty = df_valid.groupby(['prompt_type', 'difficulty'])['is_correct'].mean().unstack().to_dict('index')
        metrics['accuracy_by_prompt_difficulty'] = accuracy_by_prompt_difficulty
        
        # Ph√¢n t√≠ch m·ª©c ƒë·ªô c·∫£i thi·ªán gi·ªØa c√°c ƒë·ªô kh√≥
        difficulty_levels = ['D·ªÖ', 'Trung b√¨nh', 'Kh√≥']
        valid_levels = [level for level in difficulty_levels if level in df_valid['difficulty'].unique()]
        
        if len(valid_levels) > 1:
            improvements = {}
            for model in df_valid['model_name'].unique():
                model_improvements = {}
                for i in range(len(valid_levels)-1):
                    easier = valid_levels[i]
                    harder = valid_levels[i+1]
                    
                    easier_acc = df_valid[(df_valid['model_name'] == model) & (df_valid['difficulty'] == easier)]['is_correct'].mean()
                    harder_acc = df_valid[(df_valid['model_name'] == model) & (df_valid['difficulty'] == harder)]['is_correct'].mean()
                    
                    # T√≠nh s·ª± suy gi·∫£m hi·ªáu su·∫•t
                    if not np.isnan(easier_acc) and not np.isnan(harder_acc):
                        diff = harder_acc - easier_acc
                        model_improvements[f'{easier}_to_{harder}'] = diff
                
                improvements[model] = model_improvements
            
            metrics['difficulty_improvements'] = improvements
        
        return metrics
    
    def _compute_context_adherence_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        T√≠nh to√°n metrics cho ƒë√°nh gi√° context adherence.
        
        Args:
            df (pd.DataFrame): DataFrame ƒë√£ c√≥ ƒë√°nh gi√° context adherence
            
        Returns:
            Dict: Metrics li√™n quan ƒë·∫øn context adherence
        """
        metrics = {}
        
        # L·ªçc c√°c lo·∫°i prompt li√™n quan t·ªõi context (few-shot, react)
        context_prompt_mask = df['prompt_type'].str.contains('few-shot|react', case=False, na=False)
        df_context = df[context_prompt_mask]
        
        if len(df_context) == 0:
            return metrics
        
        # 1. T·ª∑ l·ªá c√¢u tr·∫£ l·ªùi ƒë√∫ng cho prompts li√™n quan context
        if 'is_correct' in df_context.columns:
            metrics['context_accuracy'] = df_context['is_correct'].mean()
        
        # 2. So s√°nh v·ªõi non-context prompts
        non_context_mask = ~df['prompt_type'].str.contains('few-shot|react', case=False, na=False)
        df_non_context = df[non_context_mask]
        
        if len(df_non_context) > 0 and 'is_correct' in df_non_context.columns:
            metrics['non_context_accuracy'] = df_non_context['is_correct'].mean()
            
            # T√≠nh delta accuracy
            context_acc = metrics.get('context_accuracy', 0)
            non_context_acc = metrics.get('non_context_accuracy', 0)
            metrics['context_accuracy_delta'] = context_acc - non_context_acc
        
        # Ph√¢n t√≠ch reasoning_cultural_context n·∫øu c√≥
        if 'reasoning_cultural_context' in df_context.columns:
            try:
                context_scores = df_context['reasoning_cultural_context'].dropna().tolist()
                
                if context_scores:
                    metrics['avg_context_adherence_score'] = sum(context_scores) / len(context_scores)
                    metrics['max_context_adherence_score'] = max(context_scores)
                    metrics['min_context_adherence_score'] = min(context_scores)
            except Exception as e:
                logger.error(f"L·ªói khi t√≠nh to√°n context adherence score: {str(e)}")
                logger.error(traceback.format_exc())
        
        return metrics

    def _compute_basic_metrics(self, df: pd.DataFrame) -> Dict[str, float]:
        """
        T√≠nh to√°n c√°c metrics c∆° b·∫£n tr√™n to√†n b·ªô dataset.
        
        Args:
            df (pd.DataFrame): DataFrame k·∫øt qu·∫£
            
        Returns:
            Dict[str, float]: C√°c metrics c∆° b·∫£n
        """
        metrics = {}
        
        # T√≠nh to√°n accuracy t·ªïng th·ªÉ
        if 'is_correct' in df.columns:
            metrics['overall_accuracy'] = df['is_correct'].mean()
        
        # T√≠nh to√°n th·ªùi gian trung b√¨nh
        if 'latency' in df.columns:
            metrics['average_latency'] = df['latency'].mean()
            metrics['max_latency'] = df['latency'].max()
            metrics['min_latency'] = df['latency'].min()
        
        # T√≠nh to√°n ƒë·ªô d√†i ph·∫£n h·ªìi trung b√¨nh
        if 'response_length' in df.columns:
            metrics['average_response_length'] = df['response_length'].mean()
        
        return metrics
    
    def _compute_metrics_by_model_prompt(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        T√≠nh to√°n c√°c metrics theo t·ª´ng c·∫∑p model-prompt.
        
        Args:
            df (pd.DataFrame): DataFrame k·∫øt qu·∫£
            
        Returns:
            Dict[str, Dict[str, Any]]: C√°c metrics theo t·ª´ng c·∫∑p model-prompt
        """
        metrics = {}
        
        # X√°c ƒë·ªãnh c·ªôt model (c√≥ th·ªÉ l√† 'model_name' ho·∫∑c 'model')
        model_col = 'model_name' if 'model_name' in df.columns else 'model'
        
        if model_col not in df.columns or 'prompt_type' not in df.columns:
            return metrics
        
        # L·∫•y danh s√°ch models v√† prompt types
        models = df[model_col].unique()
        prompt_types = df['prompt_type'].unique()
        
        # T√≠nh metrics cho t·ª´ng c·∫∑p model-prompt
        for model in models:
            metrics[model] = {}
            for prompt_type in prompt_types:
                mp_df = df[(df[model_col] == model) & (df['prompt_type'] == prompt_type)]
                
                if len(mp_df) == 0:
                    continue
                
                mp_metrics = {}
                
                # Accuracy (n·∫øu c√≥)
                if 'is_correct' in mp_df.columns:
                    mp_metrics['accuracy'] = mp_df['is_correct'].mean()
                
                # Latency (n·∫øu c√≥)
                if 'latency' in mp_df.columns:
                    mp_metrics['average_latency'] = mp_df['latency'].mean()
                    mp_metrics['max_latency'] = mp_df['latency'].max()
                    mp_metrics['min_latency'] = mp_df['latency'].min()
                
                # Token count (n·∫øu c√≥)
                if 'token_count' in mp_df.columns:
                    mp_metrics['average_token_count'] = mp_df['token_count'].mean()
                
                # Th√™m c√°c metrics kh√°c n·∫øu c·∫ßn
                
                # L∆∞u metrics cho c·∫∑p model-prompt
                metrics[model][prompt_type] = mp_metrics
        
        return metrics
    
    def _compute_metrics_by_question_type(self, df: pd.DataFrame) -> Dict[str, Dict[str, float]]:
        """
        T√≠nh to√°n metrics theo lo·∫°i c√¢u h·ªèi.
        
        Args:
            df (pd.DataFrame): DataFrame k·∫øt qu·∫£
            
        Returns:
            Dict: Metrics theo lo·∫°i c√¢u h·ªèi
        """
        metrics = {}
        
        if 'question_type' not in df.columns:
            return metrics
        
        # L·∫∑p qua t·ª´ng lo·∫°i c√¢u h·ªèi
        for q_type in df['question_type'].unique():
            metrics[q_type] = {}
            type_df = df[df['question_type'] == q_type]
            
            # T√≠nh accuracy cho lo·∫°i c√¢u h·ªèi n√†y
            if 'is_correct' in df.columns:
                metrics[q_type]['accuracy'] = type_df['is_correct'].mean()
            
            # T√≠nh th·ªùi gian trung b√¨nh
            if 'latency' in df.columns:
                metrics[q_type]['avg_latency'] = type_df['latency'].mean()
            
            # T√≠nh s·ªë l∆∞·ª£ng c√¢u h·ªèi
            metrics[q_type]['count'] = len(type_df)
        
        return metrics

    def _deep_merge(self, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
        """
        G·ªôp hai dict m·ªôt c√°ch ƒë·ªá quy.
        
        Args:
            base: Dict c∆° s·ªü
            override: Dict ghi ƒë√®
            
        Returns:
            Dict m·ªõi sau khi g·ªôp
        """
        result = base.copy()
        
        for key, value in override.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = self._deep_merge(result[key], value)
            else:
                result[key] = value
        
        return result
