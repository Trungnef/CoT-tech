"""
Result Analyzer cho h·ªá th·ªëng ƒë√°nh gi√° LLM.
Ph√¢n t√≠ch c√°c k·∫øt qu·∫£ t·ª´ qu√° tr√¨nh ƒë√°nh gi√° v√† t√≠nh to√°n c√°c metrics.
H·ªó tr·ª£ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng suy lu·∫≠n qua API Groq v√† c√°c metrics kh√°c.
"""

import time
import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Union, Tuple
from collections import defaultdict
import traceback
import re

# Import c√°c module c·∫ßn thi·∫øt
try:
    from ..core.model_interface import generate_text
except ImportError:
    # Fallback khi ch·∫°y module tr·ª±c ti·∫øp
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from core.model_interface import generate_text

# Thi·∫øt l·∫≠p logging
logger = logging.getLogger(__name__)

class ResultAnalyzer:
    """
    Ph√¢n t√≠ch k·∫øt qu·∫£ ƒë√°nh gi√° m√¥ h√¨nh ng√¥n ng·ªØ.
    T√≠nh to√°n c√°c metrics v√† ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng suy lu·∫≠n.
    """
    
    def __init__(self, 
                 results_df: Optional[pd.DataFrame] = None,
                 reasoning_evaluation_config: Optional[Dict[str, Any]] = None,
                 reasoning_model: str = "groq/llama3-70b-8192",
                 language: str = "vietnamese",
                 similarity_model: Optional[str] = None,
                 verbose: bool = True):
        """
        Kh·ªüi t·∫°o ResultAnalyzer.
        
        Args:
            results_df (pd.DataFrame, optional): DataFrame k·∫øt qu·∫£ ƒë·ªÉ ph√¢n t√≠ch
            reasoning_evaluation_config (Dict, optional): C·∫•u h√¨nh ƒë√°nh gi√° suy lu·∫≠n
            reasoning_model (str): M√¥ h√¨nh d√πng ƒë·ªÉ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng suy lu·∫≠n
            language (str): Ng√¥n ng·ªØ s·ª≠ d·ª•ng trong ƒë√°nh gi√°
            similarity_model (str, optional): M√¥ h√¨nh ƒë·ªÉ t√≠nh to√°n semantic similarity
            verbose (bool): Hi·ªÉn th·ªã th√¥ng tin chi ti·∫øt trong qu√° tr√¨nh ph√¢n t√≠ch
        """
        self.results_df = results_df
        self.reasoning_config = reasoning_evaluation_config or {}
        self.reasoning_model = self.reasoning_config.get("model", reasoning_model)
        self.language = language.lower()
        self.similarity_model = similarity_model
        self.verbose = verbose
        self.sample_size = self.reasoning_config.get("sample_size", 10)
        
        # C√°c ti√™u ch√≠ ƒë√°nh gi√° suy lu·∫≠n (ƒëi·ªÉm t·ª´ 1-5)
        self.reasoning_criteria = {
            "logical_flow": "T√≠nh h·ª£p l√Ω v√† m·∫°ch l·∫°c c·ªßa l·∫≠p lu·∫≠n (1-5)",
            "mathematical_correctness": "ƒê·ªô ch√≠nh x√°c v·ªÅ m·∫∑t to√°n h·ªçc (1-5)",
            "clarity": "R√µ r√†ng v√† d·ªÖ hi·ªÉu (1-5)",
            "completeness": "ƒê·∫ßy ƒë·ªß c√°c b∆∞·ªõc c·∫ßn thi·∫øt (1-5)",
            "relevance": "M·ª©c ƒë·ªô li√™n quan ƒë·∫øn c√¢u h·ªèi (1-5)"
        }

        # C·∫•u tr√∫c prompt ƒë√°nh gi√°
        if language.lower() == "vietnamese":
            self.reasoning_eval_template = """
B·∫°n l√† m·ªôt chuy√™n gia ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng suy lu·∫≠n. H√£y ƒë√°nh gi√° c√¢u tr·∫£ l·ªùi cho b√†i to√°n d∆∞·ªõi ƒë√¢y theo 5 ti√™u ch√≠, ƒë∆∞a ra ƒëi·ªÉm s·ªë t·ª´ 1-5 (5 l√† t·ªët nh·∫•t).

B√ÄI TO√ÅN:
{question}

ƒê√ÅP √ÅN CHU·∫®N:
{correct_answer}

C√ÇU TR·∫¢ L·ªúI C·∫¶N ƒê√ÅNH GI√Å:
{model_answer}

H√ÉY ƒê√ÅNH GI√Å THEO C√ÅC TI√äU CH√ç SAU (ƒëi·ªÉm t·ª´ 1-5):
1. T√≠nh h·ª£p l√Ω v√† m·∫°ch l·∫°c c·ªßa l·∫≠p lu·∫≠n (logical_flow): ?/5
2. ƒê·ªô ch√≠nh x√°c v·ªÅ m·∫∑t to√°n h·ªçc (mathematical_correctness): ?/5
3. R√µ r√†ng v√† d·ªÖ hi·ªÉu (clarity): ?/5
4. ƒê·∫ßy ƒë·ªß c√°c b∆∞·ªõc c·∫ßn thi·∫øt (completeness): ?/5
5. M·ª©c ƒë·ªô li√™n quan ƒë·∫øn c√¢u h·ªèi (relevance): ?/5

ƒêi·ªÉm trung b√¨nh: ?/5

Gi·∫£i th√≠ch ng·∫Øn g·ªçn cho m·ªói ƒëi·ªÉm:
"""
        else:
            self.reasoning_eval_template = """
You are an expert evaluating reasoning quality. Please evaluate the answer to the following problem according to 5 criteria, giving scores from 1-5 (5 being the best).

PROBLEM:
{question}

CORRECT ANSWER:
{correct_answer}

ANSWER TO EVALUATE:
{model_answer}

EVALUATE ACCORDING TO THESE CRITERIA (score 1-5):
1. Logical flow and coherence (logical_flow): ?/5
2. Mathematical correctness (mathematical_correctness): ?/5
3. Clarity and understandability (clarity): ?/5
4. Completeness of necessary steps (completeness): ?/5
5. Relevance to the question (relevance): ?/5

Average score: ?/5

Brief explanation for each score:
"""

    def analyze(self) -> pd.DataFrame:
        """
        Ph√¢n t√≠ch k·∫øt qu·∫£ v√† th·ª±c hi·ªán ƒë√°nh gi√°.
        Ph∆∞∆°ng th·ª©c n√†y g·ªçi khi ResultAnalyzer ƒë∆∞·ª£c s·ª≠ d·ª•ng t·ª´ Evaluator.
        
        Returns:
            pd.DataFrame: DataFrame v·ªõi k·∫øt qu·∫£ ph√¢n t√≠ch
        """
        if self.results_df is None or len(self.results_df) == 0:
            logger.warning("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch")
            return pd.DataFrame()
        
        if self.verbose:
            logger.info(f"üîç B·∫Øt ƒë·∫ßu ph√¢n t√≠ch {len(self.results_df)} k·∫øt qu·∫£ ƒë√°nh gi√°")
        
        # Ki·ªÉm tra c√°c c·ªôt c·∫ßn thi·∫øt
        required_cols = ['model_name', 'prompt_type', 'question_text']
        missing_cols = [col for col in required_cols if col not in self.results_df.columns]
        if missing_cols:
            logger.warning(f"Thi·∫øu c√°c c·ªôt c·∫ßn thi·∫øt cho ph√¢n t√≠ch: {missing_cols}")
            logger.info(f"C√°c c·ªôt hi·ªán c√≥: {list(self.results_df.columns)}")
            return self.results_df
        
        # T√≠nh to√°n metrics c∆° b·∫£n
        analysis_results = self.analyze_results(self.results_df)
        
        # ƒê√°nh gi√° t√≠nh nh·∫•t qu√°n trong self-consistency runs
        if self.results_df['prompt_type'].str.contains('consistency|cot_self_consistency', case=False).any():
            if self.verbose:
                logger.info("ƒê√°nh gi√° t√≠nh nh·∫•t qu√°n trong c√°c self-consistency runs")
            
            try:
                self.results_df = self.evaluate_consistency(self.results_df)
                analysis_results["consistency_metrics"] = self._compute_consistency_metrics(self.results_df)
            except Exception as e:
                logger.error(f"L·ªói khi ƒë√°nh gi√° t√≠nh nh·∫•t qu√°n: {str(e)}")
                logger.debug(traceback.format_exc())
        
        # ƒê√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß c·ªßa c√¢u tr·∫£ l·ªùi
        if self.reasoning_config.get("evaluate_completeness", True):
            if self.verbose:
                logger.info("ƒê√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß c·ªßa c√¢u tr·∫£ l·ªùi")
                
            try:
                completeness_sample_size = self.reasoning_config.get("completeness_sample_size", self.sample_size)
                self.results_df = self.evaluate_completeness(
                    self.results_df,
                    sample_size=completeness_sample_size
                )
                
                analysis_results["completeness_metrics"] = self._compute_completeness_metrics(self.results_df)
            except Exception as e:
                logger.error(f"L·ªói khi ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß: {str(e)}")
                logger.debug(traceback.format_exc())
        
        # T√≠nh to√°n metrics ƒëo l∆∞·ªùng ƒë·ªô t∆∞∆°ng ƒë·ªìng v·ªõi ƒë√°p √°n chu·∫©n
        if 'correct_answer' in self.results_df.columns and self.reasoning_config.get("evaluate_similarity", True):
            if self.verbose:
                logger.info("T√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng v·ªõi ƒë√°p √°n chu·∫©n")
                
            try:
                self.results_df = self.evaluate_similarity(self.results_df)
                analysis_results["similarity_metrics"] = self._compute_similarity_metrics(self.results_df)
            except Exception as e:
                logger.error(f"L·ªói khi t√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng: {str(e)}")
                logger.debug(traceback.format_exc())
        
        # Ph√¢n t√≠ch l·ªói n·∫øu c√≥ c√°c c√¢u tr·∫£ l·ªùi sai
        if 'is_correct' in self.results_df.columns and self.reasoning_config.get("error_analysis", True):
            try:
                error_sample_size = self.reasoning_config.get("error_sample_size", min(50, self.sample_size))
                
                if self.verbose:
                    logger.info(f"Th·ª±c hi·ªán ph√¢n t√≠ch l·ªói v·ªõi sample_size={error_sample_size}")
                
                self.results_df = self.analyze_errors(
                    self.results_df,
                    sample_size=error_sample_size
                )
                
                analysis_results["error_analysis"] = self._compute_error_metrics(self.results_df)
            except Exception as e:
                logger.error(f"L·ªói khi ph√¢n t√≠ch l·ªói: {str(e)}")
                logger.debug(traceback.format_exc())
        
        # ƒê√°nh gi√° kh·∫£ nƒÉng suy lu·∫≠n n·∫øu ƒë∆∞·ª£c b·∫≠t
        if self.reasoning_config.get("enabled", True) and 'correct_answer' in self.results_df.columns:
            try:
                if self.verbose:
                    logger.info(f"ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng suy lu·∫≠n v·ªõi sample_size={self.sample_size}")
                
                # Th·ª±c hi·ªán ƒë√°nh gi√° suy lu·∫≠n cho m·∫´u ng·∫´u nhi√™n
                self.results_df = self.evaluate_reasoning_quality(
                    self.results_df, 
                    sample_size=self.sample_size
                )
                
                # T√≠nh metrics cho k·∫øt qu·∫£ ƒë√°nh gi√° suy lu·∫≠n
                reasoning_metrics = self._compute_reasoning_metrics(self.results_df)
                analysis_results["reasoning_metrics"] = reasoning_metrics
            except Exception as e:
                logger.error(f"L·ªói khi ƒë√°nh gi√° kh·∫£ nƒÉng suy lu·∫≠n: {str(e)}")
                logger.debug(traceback.format_exc())
        
        # L∆∞u k·∫øt qu·∫£ ph√¢n t√≠ch v√†o instance ƒë·ªÉ c√≥ th·ªÉ truy c·∫≠p sau
        self.analysis_results = analysis_results
        
        # Hi·ªÉn th·ªã t√≥m t·∫Øt n·∫øu ·ªü ch·∫ø ƒë·ªô verbose
        if self.verbose:
            logger.info("\n" + self.export_summary(analysis_results))
        
        return self.results_df
    
    def analyze_errors(self, 
                   results_df: pd.DataFrame, 
                   sample_size: int = 50,
                   random_seed: int = 42) -> pd.DataFrame:
        """
        Ph√¢n t√≠ch v√† ph√¢n lo·∫°i c√°c l·ªói trong c√¢u tr·∫£ l·ªùi c·ªßa model.
        
        C√°c lo·∫°i l·ªói ƒë∆∞·ª£c ph√¢n lo·∫°i bao g·ªìm:
        - L·ªói ki·∫øn th·ª©c (Knowledge Error): Thi·∫øu th√¥ng tin ho·∫∑c ki·∫øn th·ª©c kh√¥ng ch√≠nh x√°c
        - L·ªói suy lu·∫≠n (Reasoning Error): L·ªói trong qu√° tr√¨nh suy lu·∫≠n, sai logic
        - L·ªói t√≠nh to√°n (Calculation Error): Sai s·ªë h·ªçc ho·∫∑c t√≠nh to√°n
        - L·ªói kh√¥ng tr·∫£ l·ªùi (Non-answer): T·ª´ ch·ªëi ho·∫∑c kh√¥ng tr·∫£ l·ªùi
        - L·ªói l·∫°c ƒë·ªÅ (Off-topic): Tr·∫£ l·ªùi kh√¥ng li√™n quan ƒë·∫øn c√¢u h·ªèi
        - L·ªói hi·ªÉu nh·∫ßm (Misunderstanding): Hi·ªÉu sai c√¢u h·ªèi
        - L·ªói kh√°c (Other): C√°c l·ªói kh√¥ng thu·ªôc c√°c lo·∫°i tr√™n
        
        Args:
            results_df (pd.DataFrame): DataFrame ch·ª©a k·∫øt qu·∫£ ƒë√°nh gi√°
            sample_size (int): S·ªë l∆∞·ª£ng m·∫´u c·∫ßn ph√¢n t√≠ch
            random_seed (int): Seed ng·∫´u nhi√™n cho vi·ªác l·∫•y m·∫´u
            
        Returns:
            pd.DataFrame: DataFrame ƒë√£ b·ªï sung ph√¢n lo·∫°i l·ªói
        """
        # Ki·ªÉm tra xem c√≥ c√°c c·ªôt c·∫ßn thi·∫øt kh√¥ng
        required_cols = ['question_text', 'response', 'is_correct']
        for col in required_cols:
            if col not in results_df.columns:
                logger.warning(f"Kh√¥ng th·ªÉ ph√¢n t√≠ch l·ªói: thi·∫øu c·ªôt '{col}'")
                return results_df
                
        # ƒê·∫£m b·∫£o c√≥ c√°c c·ªôt c·∫ßn thi·∫øt
        if 'error_type' not in results_df.columns:
            results_df['error_type'] = ''
            
        if 'error_explanation' not in results_df.columns:
            results_df['error_explanation'] = ''
        
        # L·ªçc c√°c c√¢u tr·∫£ l·ªùi sai
        if 'is_correct' not in results_df.columns:
            logger.warning("Kh√¥ng c√≥ c·ªôt 'is_correct' ƒë·ªÉ ph√¢n t√≠ch l·ªói")
            return results_df
        
        # L·ªçc c√°c c√¢u tr·∫£ l·ªùi sai ch∆∞a ƒë∆∞·ª£c ph√¢n t√≠ch l·ªói
        error_rows = (results_df['is_correct'] == False) & (results_df['error_type'] == '')
        
        # Ki·ªÉm tra xem c√≥ c√¢u tr·∫£ l·ªùi sai ƒë·ªÉ ph√¢n t√≠ch kh√¥ng
        if not error_rows.any():
            logger.info("Kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi sai ch∆∞a ƒë∆∞·ª£c ph√¢n t√≠ch")
            return results_df
        
        error_indices = results_df[error_rows].index.tolist()
        
        # L·∫•y m·∫´u ng·∫´u nhi√™n n·∫øu c·∫ßn
        np.random.seed(random_seed)
        if len(error_indices) > sample_size:
            sample_indices = np.random.choice(error_indices, size=sample_size, replace=False)
        else:
            sample_indices = error_indices
        
        logger.info(f"Ph√¢n t√≠ch l·ªói cho {len(sample_indices)}/{len(error_indices)} c√¢u tr·∫£ l·ªùi sai")
        
        # Ph√¢n t√≠ch l·ªói cho t·ª´ng m·∫´u
        for i, idx in enumerate(sample_indices):
            row = results_df.loc[idx]
            
            question = row['question_text']
            model_answer = row['response']
            correct_answer = row['correct_answer'] if 'correct_answer' in row else None
            
            if self.verbose:
                logger.info(f"Ph√¢n t√≠ch l·ªói m·∫´u {i+1}/{len(sample_indices)}: model={row['model_name']}, prompt={row['prompt_type']}")
            
            try:
                # Ph√¢n t√≠ch l·ªói
                error_result = self._analyze_single_error(question, model_answer, correct_answer)
                
                # C·∫≠p nh·∫≠t DataFrame
                results_df.at[idx, 'error_type'] = error_result.get('error_type', 'Unknown')
                results_df.at[idx, 'error_explanation'] = error_result.get('explanation', '')
            
            except Exception as e:
                logger.error(f"L·ªói khi ph√¢n t√≠ch l·ªói cho m·∫´u {idx}: {str(e)}")
                logger.error(traceback.format_exc())
                
                # G√°n gi√° tr·ªã m·∫∑c ƒë·ªãnh
                results_df.at[idx, 'error_type'] = 'Analysis Error'
                results_df.at[idx, 'error_explanation'] = f"L·ªói khi ph√¢n t√≠ch: {str(e)}"
        
        return results_df
    
    def _analyze_single_error(self, 
                            question: str, 
                            model_answer: str, 
                            correct_answer: str = None) -> Dict[str, Any]:
        """
        Ph√¢n t√≠ch l·ªói cho m·ªôt m·∫´u c√¢u h·ªèi/c√¢u tr·∫£ l·ªùi.
        
        Args:
            question (str): C√¢u h·ªèi
            model_answer (str): C√¢u tr·∫£ l·ªùi c·ªßa model
            correct_answer (str, optional): ƒê√°p √°n ƒë√∫ng n·∫øu c√≥
            
        Returns:
            Dict: K·∫øt qu·∫£ ph√¢n t√≠ch l·ªói
        """
        try:
            # C·∫Øt b·ªõt ƒë·ªô d√†i n·∫øu qu√° d√†i
            max_length = 4000  # Gi·ªõi h·∫°n ƒë·ªô d√†i ƒë·ªÉ tr√°nh v∆∞·ª£t qu√° context window
            if len(model_answer) > max_length:
                logger.warning(f"C·∫Øt b·ªõt c√¢u tr·∫£ l·ªùi ({len(model_answer)} -> {max_length} k√Ω t·ª±)")
                model_answer = model_answer[:max_length] + "..."
            
            # T·∫°o prompt ph√¢n t√≠ch l·ªói
            if self.language.lower() == "vietnamese":
                correct_answer_part = f"\nƒê√ÅP √ÅN ƒê√öNG:\n{correct_answer}" if correct_answer else ""
                
                error_analysis_prompt = f"""
B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch l·ªói trong c√¢u tr·∫£ l·ªùi c·ªßa m√¥ h√¨nh ng√¥n ng·ªØ. H√£y ph√¢n t√≠ch v√† ph√¢n lo·∫°i l·ªói trong c√¢u tr·∫£ l·ªùi d∆∞·ªõi ƒë√¢y.

C√ÇU H·ªéI:
{question}
{correct_answer_part}

C√ÇU TR·∫¢ L·ªúI C·ª¶A M√î H√åNH:
{model_answer}

Ph√¢n lo·∫°i l·ªói v√†o M·ªòT trong c√°c danh m·ª•c sau:
1. L·ªói ki·∫øn th·ª©c (Knowledge Error): Thi·∫øu th√¥ng tin ho·∫∑c ki·∫øn th·ª©c kh√¥ng ch√≠nh x√°c
2. L·ªói suy lu·∫≠n (Reasoning Error): L·ªói trong qu√° tr√¨nh suy lu·∫≠n, sai logic
3. L·ªói t√≠nh to√°n (Calculation Error): Sai s·ªë h·ªçc ho·∫∑c t√≠nh to√°n
4. L·ªói kh√¥ng tr·∫£ l·ªùi (Non-answer): T·ª´ ch·ªëi ho·∫∑c kh√¥ng tr·∫£ l·ªùi
5. L·ªói l·∫°c ƒë·ªÅ (Off-topic): Tr·∫£ l·ªùi kh√¥ng li√™n quan ƒë·∫øn c√¢u h·ªèi
6. L·ªói hi·ªÉu nh·∫ßm (Misunderstanding): Hi·ªÉu sai c√¢u h·ªèi
7. L·ªói kh√°c (Other): C√°c l·ªói kh√¥ng thu·ªôc c√°c lo·∫°i tr√™n

Lo·∫°i l·ªói: [Ch·ªçn M·ªòT lo·∫°i l·ªói t·ª´ danh s√°ch tr√™n]

Gi·∫£i th√≠ch ng·∫Øn g·ªçn:
[Gi·∫£i th√≠ch t·∫°i sao c√¢u tr·∫£ l·ªùi b·ªã coi l√† sai v√† thu·ªôc lo·∫°i l·ªói ƒë√£ ch·ªçn]
"""
            else:
                correct_answer_part = f"\nCORRECT ANSWER:\n{correct_answer}" if correct_answer else ""
                
                error_analysis_prompt = f"""
You are an expert analyzing errors in language model responses. Analyze and categorize the error in the response below.

QUESTION:
{question}
{correct_answer_part}

MODEL RESPONSE:
{model_answer}

Categorize the error into ONE of the following categories:
1. Knowledge Error: Missing information or incorrect knowledge
2. Reasoning Error: Errors in the reasoning process, logical fallacies
3. Calculation Error: Mathematical or computational mistakes
4. Non-answer: Refusing or failing to provide an answer
5. Off-topic: Response unrelated to the question
6. Misunderstanding: Misinterpreting the question
7. Other: Errors that don't fall into the above categories

Error Type: [Select ONE error type from the list above]

Brief Explanation:
[Explain why the answer is incorrect and why it belongs to the selected error type]
"""
            
            # S·ª≠ d·ª•ng model API ƒë·ªÉ ph√¢n t√≠ch l·ªói
            use_groq = self.reasoning_config.get("use_groq", True)
            if use_groq:
                # S·ª≠ d·ª•ng Groq API
                from core.model_interface import generate_text
                
                # L·∫•y t√™n model Groq
                model_name = "groq"
                config = {
                    "model": self.reasoning_config.get("models", {}).get(
                        "error_analysis", "llama3-70b-8192"
                    ),
                    "temperature": 0.1,  # Th·∫•p ƒë·ªÉ ƒë·∫£m b·∫£o ph√¢n lo·∫°i nh·∫•t qu√°n
                    "max_tokens": 1024
                }
                
                # G·ªçi API
                logger.debug("Ph√¢n t√≠ch l·ªói b·∫±ng Groq API")
                response_text, stats = generate_text(model_name, error_analysis_prompt, config)
                
                if stats.get("has_error", False):
                    logger.error(f"L·ªói khi g·ªçi Groq API: {stats.get('error_message')}")
                    # Fallback v·ªÅ gi√° tr·ªã m·∫∑c ƒë·ªãnh
                    return {
                        "error_type": "Unknown",
                        "explanation": f"[L·ªói ph√¢n t√≠ch: {stats.get('error_message')}]"
                    }
            else:
                # TODO: S·ª≠ d·ª•ng model kh√°c n·∫øu c·∫ßn
                logger.warning("Ch·ªâ h·ªó tr·ª£ Groq API ƒë·ªÉ ph√¢n t√≠ch l·ªói")
                response_text = ""
            
            # Ph√¢n t√≠ch k·∫øt qu·∫£ ph√¢n lo·∫°i l·ªói
            result = self._parse_error_analysis(response_text)
            return result
            
        except Exception as e:
            logger.error(f"L·ªói khi ph√¢n t√≠ch l·ªói: {str(e)}")
            logger.error(traceback.format_exc())
            
            # Tr·∫£ v·ªÅ gi√° tr·ªã m·∫∑c ƒë·ªãnh
            return {
                "error_type": "Analysis Error",
                "explanation": f"L·ªói khi ph√¢n t√≠ch: {str(e)}"
            }
    
    def _parse_error_analysis(self, analysis_response: str) -> Dict[str, Any]:
        """
        Ph√¢n t√≠ch k·∫øt qu·∫£ ph√¢n lo·∫°i l·ªói t·ª´ API.
        
        Args:
            analysis_response (str): Ph·∫£n h·ªìi t·ª´ API
            
        Returns:
            Dict: Dictionary ch·ª©a lo·∫°i l·ªói v√† gi·∫£i th√≠ch
        """
        result = {
            "error_type": "Unknown",
            "explanation": analysis_response
        }
        
        if not analysis_response:
            return result
        
        # X√°c ƒë·ªãnh lo·∫°i l·ªói t·ª´ ph·∫£n h·ªìi
        error_type_patterns = [
            r"(?:Lo·∫°i l·ªói|Error Type)[:]\s*(.*?)(?:\n|$)",
            r"(?:l·ªói|error)[:]\s*(.*?)(?:\n|$)"
        ]
        
        for pattern in error_type_patterns:
            match = re.search(pattern, analysis_response, re.IGNORECASE)
            if match:
                error_type = match.group(1).strip()
                
                # Chu·∫©n h√≥a lo·∫°i l·ªói
                error_type = self._normalize_error_type(error_type)
                result["error_type"] = error_type
                break
        
        # Tr√≠ch xu·∫•t ph·∫ßn gi·∫£i th√≠ch
        explanation_patterns = [
            r"(?:Gi·∫£i th√≠ch|Brief Explanation)[:]\s*([\s\S]*)",
            r"(?:gi·∫£i th√≠ch|explanation)[:]\s*([\s\S]*)"
        ]
        
        for pattern in explanation_patterns:
            match = re.search(pattern, analysis_response, re.IGNORECASE)
            if match:
                explanation = match.group(1).strip()
                result["explanation"] = explanation
                break
        
        return result
    
    def _normalize_error_type(self, error_type: str) -> str:
        """
        Chu·∫©n h√≥a lo·∫°i l·ªói v·ªÅ c√°c lo·∫°i chu·∫©n.
        
        Args:
            error_type (str): Lo·∫°i l·ªói ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ ph·∫£n h·ªìi
            
        Returns:
            str: Lo·∫°i l·ªói ƒë√£ chu·∫©n h√≥a
        """
        # C√°c t·ª´ kh√≥a ƒë·ªÉ ph√¢n lo·∫°i l·ªói
        error_types = {
            "Knowledge Error": ["knowledge", "ki·∫øn th·ª©c", "thi·∫øu th√¥ng tin", "incorrect knowledge"],
            "Reasoning Error": ["reasoning", "suy lu·∫≠n", "logic", "logical", "reasoning process"],
            "Calculation Error": ["calculation", "t√≠nh to√°n", "mathematical", "computational", "s·ªë h·ªçc"],
            "Non-answer": ["non-answer", "kh√¥ng tr·∫£ l·ªùi", "refusing", "failing", "t·ª´ ch·ªëi"],
            "Off-topic": ["off-topic", "l·∫°c ƒë·ªÅ", "unrelated", "kh√¥ng li√™n quan"],
            "Misunderstanding": ["misunderstanding", "hi·ªÉu nh·∫ßm", "misinterpreting", "misinterpretation"],
            "Other": ["other", "kh√°c"]
        }
        
        error_type_lower = error_type.lower()
        
        # T√¨m lo·∫°i l·ªói ph√π h·ª£p nh·∫•t
        for standard_type, keywords in error_types.items():
            if any(keyword.lower() in error_type_lower for keyword in keywords):
                return standard_type
        
        # Ki·ªÉm tra ch·ªâ s·ªë 1-7 n·∫øu c√≥
        if re.match(r"^[1-7][\.:]?", error_type_lower):
            index = int(error_type_lower[0]) - 1
            standard_types = list(error_types.keys())
            if 0 <= index < len(standard_types):
                return standard_types[index]
        
        return "Other"
    
    def _compute_error_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        T√≠nh to√°n c√°c metrics li√™n quan ƒë·∫øn ph√¢n t√≠ch l·ªói.
        
        Args:
            df (pd.DataFrame): DataFrame ƒë√£ c√≥ ph√¢n lo·∫°i l·ªói
            
        Returns:
            Dict: Metrics li√™n quan ƒë·∫øn ph√¢n t√≠ch l·ªói
        """
        metrics = {}
        
        # L·ªçc c√°c d√≤ng c√≥ ph√¢n lo·∫°i l·ªói
        error_df = df[df['error_type'] != '']
        
        if len(error_df) == 0:
            return metrics
        
        # 1. T·ª∑ l·ªá c√°c lo·∫°i l·ªói t·ªïng th·ªÉ
        error_counts = error_df['error_type'].value_counts()
        error_percentages = error_counts / len(error_df) * 100
        
        metrics["overall"] = {
            "error_counts": error_counts.to_dict(),
            "error_percentages": error_percentages.to_dict()
        }
        
        # 2. T·ª∑ l·ªá l·ªói theo model
        metrics["by_model"] = {}
        if 'model_name' in error_df.columns:
            for model in error_df['model_name'].unique():
                model_df = error_df[error_df['model_name'] == model]
                
                model_error_counts = model_df['error_type'].value_counts()
                model_error_percentages = model_error_counts / len(model_df) * 100
                
                metrics["by_model"][model] = {
                    "error_counts": model_error_counts.to_dict(),
                    "error_percentages": model_error_percentages.to_dict()
                }
        elif 'model' in error_df.columns:
            for model in error_df['model'].unique():
                model_df = error_df[error_df['model'] == model]
                
                model_error_counts = model_df['error_type'].value_counts()
                model_error_percentages = model_error_counts / len(model_df) * 100
                
                metrics["by_model"][model] = {
                    "error_counts": model_error_counts.to_dict(),
                    "error_percentages": model_error_percentages.to_dict()
                }
        
        # 3. T·ª∑ l·ªá l·ªói theo prompt type
        metrics["by_prompt_type"] = {}
        for prompt in error_df['prompt_type'].unique():
            prompt_df = error_df[error_df['prompt_type'] == prompt]
            
            prompt_error_counts = prompt_df['error_type'].value_counts()
            prompt_error_percentages = prompt_error_counts / len(prompt_df) * 100
            
            metrics["by_prompt_type"][prompt] = {
                "error_counts": prompt_error_counts.to_dict(),
                "error_percentages": prompt_error_percentages.to_dict()
            }
        
        # 4. T·ª∑ l·ªá l·ªói theo model v√† prompt type
        metrics["by_model_prompt"] = {}
        for model in error_df['model'].unique():
            metrics["by_model_prompt"][model] = {}
            model_df = error_df[error_df['model'] == model]
            
            for prompt in model_df['prompt_type'].unique():
                prompt_df = model_df[model_df['prompt_type'] == prompt]
                
                mp_error_counts = prompt_df['error_type'].value_counts()
                mp_error_percentages = mp_error_counts / len(prompt_df) * 100
                
                metrics["by_model_prompt"][model][prompt] = {
                    "error_counts": mp_error_counts.to_dict(),
                    "error_percentages": mp_error_percentages.to_dict()
                }
        
        return metrics

    def analyze_results(self, results_df: pd.DataFrame) -> Dict[str, Any]:
        """
        Ph√¢n t√≠ch DataFrame k·∫øt qu·∫£ v√† t√≠nh to√°n t·∫•t c·∫£ c√°c metrics.
        
        Args:
            results_df (pd.DataFrame): DataFrame ch·ª©a k·∫øt qu·∫£ ƒë√°nh gi√°
            
        Returns:
            Dict[str, Any]: Dictionary ch·ª©a t·∫•t c·∫£ metrics v√† k·∫øt qu·∫£ ph√¢n t√≠ch
        """
        if self.verbose:
            logger.info(f"üîç Ph√¢n t√≠ch k·∫øt qu·∫£ cho {len(results_df)} m·ª•c")
        
        # 1. T√≠nh to√°n metrics c∆° b·∫£n
        basic_metrics = self._compute_basic_metrics(results_df)
        
        # 2. T√≠nh c√°c metrics theo model v√† prompt type
        model_prompt_metrics = self._compute_metrics_by_model_prompt(results_df)
        
        # 3. T√≠nh to√°n metrics theo lo·∫°i c√¢u h·ªèi (n·∫øu c√≥ th√¥ng tin)
        question_type_metrics = {}
        if 'question_type' in results_df.columns:
            question_type_metrics = self._compute_metrics_by_question_type(results_df)
        
        # 4. K·∫øt qu·∫£ c·ªßa ph√¢n t√≠ch
        analysis_results = {
            "basic_metrics": basic_metrics,
            "model_prompt_metrics": model_prompt_metrics,
            "question_type_metrics": question_type_metrics,
            "raw_results": results_df
        }
        
        return analysis_results
    
    def _compute_basic_metrics(self, df: pd.DataFrame) -> Dict[str, float]:
        """
        T√≠nh to√°n c√°c metrics c∆° b·∫£n tr√™n to√†n b·ªô dataset.
        
        Args:
            df (pd.DataFrame): DataFrame k·∫øt qu·∫£
            
        Returns:
            Dict[str, float]: C√°c metrics c∆° b·∫£n
        """
        metrics = {}
        
        # T√≠nh to√°n accuracy t·ªïng th·ªÉ
        if 'is_correct' in df.columns:
            metrics['overall_accuracy'] = df['is_correct'].mean()
        
        # T√≠nh to√°n th·ªùi gian trung b√¨nh
        if 'latency' in df.columns:
            metrics['average_latency'] = df['latency'].mean()
            metrics['max_latency'] = df['latency'].max()
            metrics['min_latency'] = df['latency'].min()
        
        # T√≠nh to√°n ƒë·ªô d√†i ph·∫£n h·ªìi trung b√¨nh
        if 'response_length' in df.columns:
            metrics['average_response_length'] = df['response_length'].mean()
        
        return metrics
    
    def _compute_metrics_by_model_prompt(self, df: pd.DataFrame) -> Dict[str, Dict[str, Dict[str, float]]]:
        """
        T√≠nh to√°n metrics theo t·ª´ng model v√† prompt type.
        
        Args:
            df (pd.DataFrame): DataFrame k·∫øt qu·∫£
            
        Returns:
            Dict: Metrics theo model v√† prompt type
        """
        metrics = {}
        
        # Ki·ªÉm tra c√°c c·ªôt c·∫ßn thi·∫øt
        model_col = 'model_name' if 'model_name' in df.columns else ('model' if 'model' in df.columns else None)
        prompt_col = 'prompt_type' if 'prompt_type' in df.columns else None
        
        if not model_col or not prompt_col:
            logger.warning(f"DataFrame thi·∫øu c·ªôt c·∫ßn thi·∫øt ƒë·ªÉ t√≠nh metrics theo model/prompt. "
                         f"C·∫ßn c√≥ 'model_name'/'model' v√† 'prompt_type', hi·ªán c√≥: {list(df.columns)}")
            
            # T·∫°o m·ªôt b·ªô metrics ƒë∆°n gi·∫£n v·ªõi d·ªØ li·ªáu kh·∫£ d·ª•ng
            if len(df) > 0:
                # T·∫°o c√°c c·ªôt gi·∫£ n·∫øu kh√¥ng c√≥
                if not model_col:
                    df['model_name'] = 'unknown_model'
                    model_col = 'model_name'
                if not prompt_col:
                    df['prompt_type'] = 'unknown_prompt'
                    prompt_col = 'prompt_type'
            else:
                return metrics
        
        # L·∫∑p qua t·ª´ng model
        for model in df[model_col].unique():
            metrics[model] = {}
            model_df = df[df[model_col] == model]
            
            # L·∫∑p qua t·ª´ng prompt type
            for prompt_type in model_df[prompt_col].unique():
                metrics[model][prompt_type] = {}
                prompt_df = model_df[model_df[prompt_col] == prompt_type]
                
                # T√≠nh accuracy n·∫øu c√≥ c·ªôt is_correct
                if 'is_correct' in df.columns:
                    metrics[model][prompt_type]['accuracy'] = prompt_df['is_correct'].mean()
                
                # T√≠nh th·ªùi gian trung b√¨nh - ki·ªÉm tra c·∫£ hai t√™n c·ªôt c√≥ th·ªÉ c√≥
                latency_col = None
                for col_name in ['latency', 'elapsed_time']:
                    if col_name in df.columns:
                        latency_col = col_name
                        break
                        
                if latency_col:
                    metrics[model][prompt_type]['avg_latency'] = prompt_df[latency_col].mean()
                
                # T√≠nh ƒë·ªô d√†i ph·∫£n h·ªìi trung b√¨nh
                if 'response_length' in df.columns:
                    metrics[model][prompt_type]['avg_response_length'] = prompt_df['response_length'].mean()
                elif 'token_count' in df.columns:
                    metrics[model][prompt_type]['avg_token_count'] = prompt_df['token_count'].mean()
                
                # Th√™m s·ªë l∆∞·ª£ng m·∫´u cho m·ªói t·ªï h·ª£p model/prompt
                metrics[model][prompt_type]['sample_count'] = len(prompt_df)
        
        return metrics
    
    def _compute_metrics_by_question_type(self, df: pd.DataFrame) -> Dict[str, Dict[str, float]]:
        """
        T√≠nh to√°n metrics theo lo·∫°i c√¢u h·ªèi.
        
        Args:
            df (pd.DataFrame): DataFrame k·∫øt qu·∫£
            
        Returns:
            Dict: Metrics theo lo·∫°i c√¢u h·ªèi
        """
        metrics = {}
        
        if 'question_type' not in df.columns:
            return metrics
        
        # L·∫∑p qua t·ª´ng lo·∫°i c√¢u h·ªèi
        for q_type in df['question_type'].unique():
            metrics[q_type] = {}
            type_df = df[df['question_type'] == q_type]
            
            # T√≠nh accuracy cho lo·∫°i c√¢u h·ªèi n√†y
            if 'is_correct' in df.columns:
                metrics[q_type]['accuracy'] = type_df['is_correct'].mean()
            
            # T√≠nh th·ªùi gian trung b√¨nh
            if 'latency' in df.columns:
                metrics[q_type]['avg_latency'] = type_df['latency'].mean()
            
            # T√≠nh s·ªë l∆∞·ª£ng c√¢u h·ªèi
            metrics[q_type]['count'] = len(type_df)
        
        return metrics
    
    def evaluate_reasoning_quality(self, 
                               results_df: pd.DataFrame, 
                               sample_size: int = 10,
                               random_seed: int = 42) -> pd.DataFrame:
        """
        ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng suy lu·∫≠n c·ªßa c√°c c√¢u tr·∫£ l·ªùi LLM.
        
        Args:
            results_df (pd.DataFrame): DataFrame k·∫øt qu·∫£ ƒë·ªÉ ƒë√°nh gi√°
            sample_size (int): S·ªë l∆∞·ª£ng m·∫´u ƒë·ªÉ ƒë√°nh gi√°
            random_seed (int): Seed ng·∫´u nhi√™n cho vi·ªác l·∫•y m·∫´u
            
        Returns:
            pd.DataFrame: DataFrame ƒë√£ b·ªï sung ƒë√°nh gi√° suy lu·∫≠n
        """
        # Ki·ªÉm tra xem c√≥ c√°c c·ªôt c·∫ßn thi·∫øt kh√¥ng
        required_cols = ['question_text', 'response', 'correct_answer']
        for col in required_cols:
            if col not in results_df.columns:
                logger.warning(f"Kh√¥ng th·ªÉ ƒë√°nh gi√° suy lu·∫≠n: thi·∫øu c·ªôt '{col}'")
                return results_df
                
        # ƒê·∫£m b·∫£o c√≥ c√°c c·ªôt c·∫ßn thi·∫øt cho ƒë√°nh gi√° suy lu·∫≠n
        if 'reasoning_avg_score' not in results_df.columns:
            results_df['reasoning_avg_score'] = np.nan
            
        if 'reasoning_evaluation' not in results_df.columns:
            results_df['reasoning_evaluation'] = ''
        
        # ƒê·∫£m b·∫£o c√≥ c·ªôt ƒë√°nh gi√° suy lu·∫≠n
        result_cols = [
            'reasoning_logical_flow', 
            'reasoning_mathematical_correctness', 
            'reasoning_clarity', 
            'reasoning_completeness', 
            'reasoning_relevance',
            'reasoning_avg_score',
            'reasoning_evaluation'
        ]
        
        for col in result_cols:
            if col not in results_df.columns:
                results_df[col] = np.nan
                
        # L·ªçc c√°c m·∫´u c√≥ ƒë√°p √°n ƒë√∫ng v√† c√≥ s·ª≠ d·ª•ng prompt y√™u c·∫ßu l·∫≠p lu·∫≠n
        has_reasoning = results_df['prompt_type'].str.contains('thought|cot|reasoning|react', case=False, na=False)
        valid_rows = (
            has_reasoning & 
            results_df['reasoning_avg_score'].isna() &  # Ch∆∞a ƒë∆∞·ª£c ƒë√°nh gi√°
            ~results_df['correct_answer'].isna() &  # C√≥ ƒë√°p √°n ƒë√∫ng
            ~results_df['response'].isna()  # C√≥ c√¢u tr·∫£ l·ªùi
        )
        
        valid_indices = results_df[valid_rows].index.tolist()
        
        if not valid_indices:
            logger.warning("Kh√¥ng c√≥ m·∫´u ph√π h·ª£p ƒë·ªÉ ƒë√°nh gi√° suy lu·∫≠n")
            return results_df
            
        # L·∫•y m·∫´u ng·∫´u nhi√™n n·∫øu c·∫ßn
        np.random.seed(random_seed)
        if len(valid_indices) > sample_size:
            sample_indices = np.random.choice(valid_indices, size=sample_size, replace=False)
        else:
            sample_indices = valid_indices
            
        logger.info(f"ƒê√°nh gi√° suy lu·∫≠n cho {len(sample_indices)} m·∫´u")
        
        # ƒê√°nh gi√° t·ª´ng m·∫´u
        for i, idx in enumerate(sample_indices):
            row = results_df.loc[idx]
            
            question = row['question_text']
            correct_answer = row['correct_answer']
            model_answer = row['response']
            
            if self.verbose:
                logger.info(f"ƒê√°nh gi√° m·∫´u {i+1}/{len(sample_indices)}: model={row['model_name']}, prompt={row['prompt_type']}")
                
            try:
                # ƒê√°nh gi√° suy lu·∫≠n
                eval_result = self._evaluate_single_reasoning(question, correct_answer, model_answer)
                
                # C·∫≠p nh·∫≠t DataFrame
                for criterion, score in eval_result.items():
                    if criterion != 'explanation':
                        col_name = f'reasoning_{criterion}'
                        if col_name in results_df.columns:
                            results_df.at[idx, col_name] = score
                
                # T√≠nh ƒëi·ªÉm trung b√¨nh
                criteria_scores = [v for k, v in eval_result.items() if k in self.reasoning_criteria]
                avg_score = sum(criteria_scores) / len(criteria_scores) if criteria_scores else 0
                results_df.at[idx, 'reasoning_avg_score'] = avg_score
                
                # L∆∞u gi·∫£i th√≠ch ƒë√°nh gi√°
                if 'explanation' in eval_result:
                    results_df.at[idx, 'reasoning_evaluation'] = eval_result['explanation']
                    
            except Exception as e:
                logger.error(f"L·ªói khi ƒë√°nh gi√° suy lu·∫≠n cho m·∫´u {idx}: {str(e)}")
                logger.error(traceback.format_exc())
        
        return results_df
    
    def _compute_reasoning_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        T√≠nh to√°n c√°c metrics ƒë√°nh gi√° suy lu·∫≠n.
        
        Args:
            df (pd.DataFrame): DataFrame ƒë√£ c√≥ k·∫øt qu·∫£ ƒë√°nh gi√° suy lu·∫≠n
            
        Returns:
            Dict: C√°c metrics c·ªßa ƒë√°nh gi√° suy lu·∫≠n
        """
        metrics = {}
        
        # Ki·ªÉm tra c√°c c·ªôt reasoning c√≥ t·ªìn t·∫°i kh√¥ng
        reasoning_cols = [col for col in df.columns if col.startswith('reasoning_') and col != 'reasoning_evaluation']
        if not reasoning_cols:
            return metrics
        
        # 1. Metrics t·ªïng th·ªÉ
        metrics["overall"] = {}
        for col in reasoning_cols:
            criterion = col.replace('reasoning_', '')
            metrics["overall"][criterion] = df[col].mean()
        
        # 2. Metrics theo model
        metrics["by_model"] = {}
        for model in df['model_name'].unique():
            metrics["by_model"][model] = {}
            model_df = df[df['model_name'] == model]
            
            for col in reasoning_cols:
                criterion = col.replace('reasoning_', '')
                metrics["by_model"][model][criterion] = model_df[col].mean()
        
        # 3. Metrics theo prompt type
        metrics["by_prompt_type"] = {}
        for prompt in df['prompt_type'].unique():
            metrics["by_prompt_type"][prompt] = {}
            prompt_df = df[df['prompt_type'] == prompt]
            
            for col in reasoning_cols:
                criterion = col.replace('reasoning_', '')
                metrics["by_prompt_type"][prompt][criterion] = prompt_df[col].mean()
        
        # 4. Metrics theo model v√† prompt type
        metrics["by_model_prompt"] = {}
        for model in df['model_name'].unique():
            metrics["by_model_prompt"][model] = {}
            model_df = df[df['model_name'] == model]
            
            for prompt in model_df['prompt_type'].unique():
                metrics["by_model_prompt"][model][prompt] = {}
                prompt_df = model_df[model_df['prompt_type'] == prompt]
                
                for col in reasoning_cols:
                    criterion = col.replace('reasoning_', '')
                    metrics["by_model_prompt"][model][prompt][criterion] = prompt_df[col].mean()
        
        return metrics
    
    def calculate_similarity(self, 
                            df: pd.DataFrame,
                            reference_column: str = 'correct_answer',
                            response_column: str = 'response') -> pd.DataFrame:
        """
        T√≠nh to√°n semantic similarity gi·ªØa c√¢u tr·∫£ l·ªùi m√¥ h√¨nh v√† ƒë√°p √°n chu·∫©n.
        
        Args:
            df (pd.DataFrame): DataFrame k·∫øt qu·∫£
            reference_column (str): T√™n c·ªôt ch·ª©a tham chi·∫øu (th∆∞·ªùng l√† ƒë√°p √°n chu·∫©n)
            response_column (str): T√™n c·ªôt ch·ª©a c√¢u tr·∫£ l·ªùi m√¥ h√¨nh
            
        Returns:
            pd.DataFrame: DataFrame v·ªõi c·ªôt similarity b·ªï sung
        """
        # Ch·ªâ t√≠nh similarity khi c√≥ m√¥ h√¨nh ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
        if not self.similarity_model:
            logger.warning("Kh√¥ng c√≥ m√¥ h√¨nh similarity ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh")
            return df
        
        if self.verbose:
            logger.info(f"üìè T√≠nh to√°n semantic similarity cho {len(df)} m·ª•c")
        
        # Copy DataFrame ƒë·ªÉ kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn d·ªØ li·ªáu g·ªëc
        result_df = df.copy()
        
        # Chu·∫©n b·ªã c·ªôt similarity
        result_df['similarity_score'] = 0.0
        
        try:
            # T√≠nh similarity cho t·ª´ng d√≤ng
            for idx, row in result_df.iterrows():
                reference = row[reference_column]
                response = row[response_column]
                
                # T√≠nh similarity score
                similarity = self._calculate_text_similarity(reference, response)
                result_df.at[idx, 'similarity_score'] = similarity
                
        except Exception as e:
            logger.error(f"L·ªói khi t√≠nh to√°n similarity: {str(e)}")
        
        return result_df
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """
        T√≠nh to√°n similarity gi·ªØa hai ƒëo·∫°n vƒÉn b·∫£n.
        Ph∆∞∆°ng th·ª©c n√†y s·∫Ω ƒë∆∞·ª£c tri·ªÉn khai khi c√≥ m√¥ h√¨nh similarity c·ª• th·ªÉ.
        
        Args:
            text1 (str): VƒÉn b·∫£n th·ª© nh·∫•t
            text2 (str): VƒÉn b·∫£n th·ª© hai
            
        Returns:
            float: ƒêi·ªÉm similarity (0-1)
        """
        # Ch·ªâ d√πng khi c√≥ m√¥ h√¨nh
        if not self.similarity_model:
            return 0.0
        
        # TODO: Tri·ªÉn khai t√≠nh similarity khi c·∫ßn thi·∫øt
        # Hi·ªán t·∫°i, tr·∫£ v·ªÅ gi√° tr·ªã m·∫∑c ƒë·ªãnh
        return 0.0
    
    def export_summary(self, analysis_results: Dict[str, Any], format: str = 'text') -> str:
        """
        Xu·∫•t b·∫£n t√≥m t·∫Øt k·∫øt qu·∫£ ph√¢n t√≠ch theo ƒë·ªãnh d·∫°ng ch·ªâ ƒë·ªãnh.
        
        Args:
            analysis_results (Dict): K·∫øt qu·∫£ ph√¢n t√≠ch t·ª´ h√†m analyze_results
            format (str): ƒê·ªãnh d·∫°ng xu·∫•t ('text', 'markdown', 'json', 'html')
            
        Returns:
            str: T√≥m t·∫Øt k·∫øt qu·∫£ ph√¢n t√≠ch theo ƒë·ªãnh d·∫°ng ch·ªâ ƒë·ªãnh
        """
        if format == 'text':
            return self._export_text_summary(analysis_results)
        elif format == 'markdown':
            return self._export_markdown_summary(analysis_results)
        elif format == 'json':
            import json
            return json.dumps(analysis_results, indent=2, ensure_ascii=False)
        elif format == 'html':
            return self._export_html_summary(analysis_results)
        else:
            return self._export_text_summary(analysis_results)
    
    def _export_text_summary(self, analysis_results: Dict[str, Any]) -> str:
        """
        Xu·∫•t b·∫£n t√≥m t·∫Øt d·∫°ng text.
        
        Args:
            analysis_results (Dict): K·∫øt qu·∫£ ph√¢n t√≠ch
            
        Returns:
            str: T√≥m t·∫Øt d·∫°ng text
        """
        summary = []
        
        # Th√¥ng tin t·ªïng quan
        summary.append("=== K·∫æT QU·∫¢ PH√ÇN T√çCH ===")
        
        # Metrics c∆° b·∫£n
        basic = analysis_results.get('basic_metrics', {})
        summary.append("\n--- METRICS C∆† B·∫¢N ---")
        
        if 'overall_accuracy' in basic:
            summary.append(f"Accuracy t·ªïng th·ªÉ: {basic['overall_accuracy']:.4f}")
        
        if 'average_latency' in basic:
            summary.append(f"Th·ªùi gian trung b√¨nh: {basic['average_latency']:.2f}s")
        
        if 'average_response_length' in basic:
            summary.append(f"ƒê·ªô d√†i ph·∫£n h·ªìi trung b√¨nh: {basic['average_response_length']:.2f} tokens")
        
        # Metrics theo model v√† prompt type
        model_metrics = analysis_results.get('model_prompt_metrics', {})
        if model_metrics:
            summary.append("\n--- METRICS THEO MODEL & PROMPT TYPE ---")
            
            for model, prompts in model_metrics.items():
                summary.append(f"\nModel: {model}")
                
                for prompt, metrics in prompts.items():
                    summary.append(f"  Prompt: {prompt}")
                    
                    if 'accuracy' in metrics:
                        summary.append(f"    - Accuracy: {metrics['accuracy']:.4f}")
                    
                    if 'avg_latency' in metrics:
                        summary.append(f"    - Th·ªùi gian TB: {metrics['avg_latency']:.2f}s")
                    
                    if 'avg_response_length' in metrics:
                        summary.append(f"    - ƒê·ªô d√†i TB: {metrics['avg_response_length']:.2f} tokens")
        
        # Metrics theo lo·∫°i c√¢u h·ªèi
        q_metrics = analysis_results.get('question_type_metrics', {})
        if q_metrics:
            summary.append("\n--- METRICS THEO LO·∫†I C√ÇU H·ªéI ---")
            
            for q_type, metrics in q_metrics.items():
                summary.append(f"\nLo·∫°i: {q_type} (s·ªë l∆∞·ª£ng: {metrics.get('count', 0)})")
                
                if 'accuracy' in metrics:
                    summary.append(f"  - Accuracy: {metrics['accuracy']:.4f}")
                
                if 'avg_latency' in metrics:
                    summary.append(f"  - Th·ªùi gian TB: {metrics['avg_latency']:.2f}s")
        
        return "\n".join(summary)
    
    def _export_markdown_summary(self, analysis_results: Dict[str, Any]) -> str:
        """
        Xu·∫•t b·∫£n t√≥m t·∫Øt d·∫°ng markdown.
        
        Args:
            analysis_results (Dict): K·∫øt qu·∫£ ph√¢n t√≠ch
            
        Returns:
            str: T√≥m t·∫Øt d·∫°ng markdown
        """
        summary = []
        
        # Th√¥ng tin t·ªïng quan
        summary.append("# K·∫æT QU·∫¢ PH√ÇN T√çCH")
        
        # Metrics c∆° b·∫£n
        basic = analysis_results.get('basic_metrics', {})
        summary.append("\n## Metrics C∆° B·∫£n")
        
        if basic:
            summary.append("| Metric | Gi√° tr·ªã |")
            summary.append("|--------|--------|")
            
            if 'overall_accuracy' in basic:
                summary.append(f"| Accuracy t·ªïng th·ªÉ | {basic['overall_accuracy']:.4f} |")
            
            if 'average_latency' in basic:
                summary.append(f"| Th·ªùi gian trung b√¨nh | {basic['average_latency']:.2f}s |")
            
            if 'average_response_length' in basic:
                summary.append(f"| ƒê·ªô d√†i ph·∫£n h·ªìi trung b√¨nh | {basic['average_response_length']:.2f} tokens |")
        
        # Metrics theo model v√† prompt type
        model_metrics = analysis_results.get('model_prompt_metrics', {})
        if model_metrics:
            summary.append("\n## Metrics Theo Model & Prompt Type")
            
            for model, prompts in model_metrics.items():
                summary.append(f"\n### Model: {model}")
                
                summary.append("| Prompt Type | Accuracy | Th·ªùi gian TB | ƒê·ªô d√†i TB |")
                summary.append("|------------|----------|--------------|-----------|")
                
                for prompt, metrics in prompts.items():
                    acc = f"{metrics.get('accuracy', 'N/A'):.4f}" if 'accuracy' in metrics else 'N/A'
                    lat = f"{metrics.get('avg_latency', 'N/A'):.2f}s" if 'avg_latency' in metrics else 'N/A'
                    len_val = f"{metrics.get('avg_response_length', 'N/A'):.2f}" if 'avg_response_length' in metrics else 'N/A'
                    
                    summary.append(f"| {prompt} | {acc} | {lat} | {len_val} |")
        
        # Metrics theo lo·∫°i c√¢u h·ªèi
        q_metrics = analysis_results.get('question_type_metrics', {})
        if q_metrics:
            summary.append("\n## Metrics Theo Lo·∫°i C√¢u H·ªèi")
            
            summary.append("| Lo·∫°i | S·ªë l∆∞·ª£ng | Accuracy | Th·ªùi gian TB |")
            summary.append("|------|----------|----------|--------------|")
            
            for q_type, metrics in q_metrics.items():
                count = metrics.get('count', 'N/A')
                acc = f"{metrics.get('accuracy', 'N/A'):.4f}" if 'accuracy' in metrics else 'N/A'
                lat = f"{metrics.get('avg_latency', 'N/A'):.2f}s" if 'avg_latency' in metrics else 'N/A'
                
                summary.append(f"| {q_type} | {count} | {acc} | {lat} |")
        
        return "\n".join(summary)
    
    def _export_html_summary(self, analysis_results: Dict[str, Any]) -> str:
        """
        Xu·∫•t b·∫£n t√≥m t·∫Øt d·∫°ng HTML.
        
        Args:
            analysis_results (Dict): K·∫øt qu·∫£ ph√¢n t√≠ch
            
        Returns:
            str: T√≥m t·∫Øt d·∫°ng HTML
        """
        # Chuy·ªÉn ƒë·ªïi t·ª´ markdown sang HTML
        try:
            import markdown
            md_summary = self._export_markdown_summary(analysis_results)
            html = markdown.markdown(md_summary, extensions=['tables'])
            
            # B·ªçc trong template HTML c∆° b·∫£n
            return f"""
            <!DOCTYPE html>
            <html>
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>K·∫øt Qu·∫£ Ph√¢n T√≠ch LLM</title>
                <style>
                    body {{ font-family: Arial, sans-serif; line-height: 1.6; padding: 20px; max-width: 1200px; margin: 0 auto; }}
                    table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
                    th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                    th {{ background-color: #f2f2f2; }}
                    tr:nth-child(even) {{ background-color: #f9f9f9; }}
                    h1, h2, h3 {{ color: #333; }}
                </style>
            </head>
            <body>
                {html}
            </body>
            </html>
            """
        except ImportError:
            # Fallback n·∫øu kh√¥ng c√≥ th∆∞ vi·ªán markdown
            return f"<pre>{self._export_text_summary(analysis_results)}</pre>"

    def evaluate_consistency(self, results_df: pd.DataFrame) -> pd.DataFrame:
        """
        ƒê√°nh gi√° t√≠nh nh·∫•t qu√°n trong c√°c self-consistency runs.
        
        Args:
            results_df (pd.DataFrame): DataFrame ch·ª©a k·∫øt qu·∫£ ƒë√°nh gi√°
            
        Returns:
            pd.DataFrame: DataFrame ƒë√£ b·ªï sung ƒë√°nh gi√° t√≠nh nh·∫•t qu√°n
        """
        # Ki·ªÉm tra xem c√≥ c·ªôt response kh√¥ng
        if 'response' not in results_df.columns:
            logger.error("L·ªói khi ƒë√°nh gi√° t√≠nh nh·∫•t qu√°n: thi·∫øu c·ªôt 'response'")
            return results_df
            
        # ƒê·∫£m b·∫£o c√≥ c√°c c·ªôt c·∫ßn thi·∫øt
        if 'consistency_score' not in results_df.columns:
            results_df['consistency_score'] = np.nan
        
        if 'consistency_agreement_rate' not in results_df.columns:
            results_df['consistency_agreement_rate'] = np.nan
        
        if 'consistency_most_common' not in results_df.columns:
            results_df['consistency_most_common'] = ''
        
        # L·ªçc c√°c prompt c√≥ ch·ª©a self-consistency
        self_consistency_mask = results_df['prompt_type'].str.contains('consistency|cot_self_consistency', case=False)
        
        if not self_consistency_mask.any():
            logger.warning("Kh√¥ng t√¨m th·∫•y self-consistency runs ƒë·ªÉ ƒë√°nh gi√° t√≠nh nh·∫•t qu√°n")
            return results_df
        
        # Nh√≥m theo model, question v√† prompt type (lo·∫°i b·ªè ph·∫ßn s·ªë runs n·∫øu c√≥)
        # V√≠ d·ª•: cot_self_consistency_3 v√† cot_self_consistency_5 s·∫Ω ƒë∆∞·ª£c nh√≥m chung
        results_df['base_prompt_type'] = results_df['prompt_type'].str.replace(r'_\d+$', '', regex=True)
        
        # S·ª≠ d·ª•ng model_name thay v√¨ model n·∫øu c√≥
        model_col = 'model_name' if 'model_name' in results_df.columns else 'model'
        
        # X√°c ƒë·ªãnh c√°c nh√≥m ch·∫°y self-consistency
        groups = results_df[self_consistency_mask].groupby([model_col, 'question_id', 'base_prompt_type'])
        
        # X·ª≠ l√Ω t·ª´ng nh√≥m
        for (model, question_id, prompt_type), group in groups:
            # B·ªè qua n·∫øu ch·ªâ c√≥ m·ªôt k·∫øt qu·∫£
            if len(group) <= 1:
                continue
            
            # L·∫•y t·∫•t c·∫£ c√°c c√¢u tr·∫£ l·ªùi trong nh√≥m
            responses = group['response'].tolist()
            final_answers = group['final_answer'].tolist() if 'final_answer' in group.columns else responses
            
            # T√≠nh to√°n t·ª∑ l·ªá nh·∫•t qu√°n
            from collections import Counter
            answer_counts = Counter(final_answers)
            
            # X√°c ƒë·ªãnh c√¢u tr·∫£ l·ªùi ph·ªï bi·∫øn nh·∫•t
            most_common_answer, most_common_count = answer_counts.most_common(1)[0]
            agreement_rate = most_common_count / len(final_answers)
            
            # T√≠nh ƒëi·ªÉm nh·∫•t qu√°n: 1 n·∫øu ho√†n to√†n nh·∫•t qu√°n, gi·∫£m d·∫ßn khi c√≥ nhi·ªÅu c√¢u tr·∫£ l·ªùi kh√°c nhau
            unique_answers = len(answer_counts)
            consistency_score = 1.0 if unique_answers == 1 else (most_common_count / len(final_answers))
            
            # C·∫≠p nh·∫≠t ƒëi·ªÉm nh·∫•t qu√°n cho t·ª´ng d√≤ng trong nh√≥m
            for idx in group.index:
                results_df.at[idx, 'consistency_score'] = consistency_score
                results_df.at[idx, 'consistency_agreement_rate'] = agreement_rate
                results_df.at[idx, 'consistency_most_common'] = most_common_answer
        
        # X√≥a c·ªôt t·∫°m base_prompt_type
        results_df = results_df.drop('base_prompt_type', axis=1)
        
        return results_df
    
    def _compute_consistency_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        T√≠nh to√°n metrics cho ƒë√°nh gi√° t√≠nh nh·∫•t qu√°n.
        
        Args:
            df (pd.DataFrame): DataFrame ƒë√£ c√≥ ƒë√°nh gi√° t√≠nh nh·∫•t qu√°n
            
        Returns:
            Dict: Metrics li√™n quan ƒë·∫øn t√≠nh nh·∫•t qu√°n
        """
        metrics = {}
        
        # L·ªçc c√°c d√≤ng c√≥ ƒë√°nh gi√° t√≠nh nh·∫•t qu√°n
        consistency_df = df[~df['consistency_score'].isna()]
        
        if len(consistency_df) == 0:
            return metrics
        
        # 1. Metrics t·ªïng th·ªÉ
        metrics["overall"] = {
            "avg_consistency_score": consistency_df['consistency_score'].mean(),
            "avg_agreement_rate": consistency_df['consistency_agreement_rate'].mean()
        }
        
        # 2. Metrics theo model
        metrics["by_model"] = {}
        
        # S·ª≠ d·ª•ng model_name thay v√¨ model n·∫øu c√≥
        model_col = 'model_name' if 'model_name' in consistency_df.columns else 'model'
        
        if model_col in consistency_df.columns:
            for model in consistency_df[model_col].unique():
                model_df = consistency_df[consistency_df[model_col] == model]
                metrics["by_model"][model] = {
                    "avg_consistency_score": model_df['consistency_score'].mean(),
                    "avg_agreement_rate": model_df['consistency_agreement_rate'].mean()
                }
        
        # 3. Metrics theo prompt type
        metrics["by_prompt_type"] = {}
        if 'prompt_type' in consistency_df.columns:
            for prompt in consistency_df['prompt_type'].unique():
                prompt_df = consistency_df[consistency_df['prompt_type'] == prompt]
                metrics["by_prompt_type"][prompt] = {
                    "avg_consistency_score": prompt_df['consistency_score'].mean(),
                    "avg_agreement_rate": prompt_df['consistency_agreement_rate'].mean()
                }
        
        # 4. Metrics theo model v√† prompt type
        metrics["by_model_prompt"] = {}
        if model_col in consistency_df.columns and 'prompt_type' in consistency_df.columns:
            for model in consistency_df[model_col].unique():
                metrics["by_model_prompt"][model] = {}
                model_df = consistency_df[consistency_df[model_col] == model]
                
                for prompt in model_df['prompt_type'].unique():
                    prompt_df = model_df[model_df['prompt_type'] == prompt]
                    metrics["by_model_prompt"][model][prompt] = {
                        "avg_consistency_score": prompt_df['consistency_score'].mean(),
                        "avg_agreement_rate": prompt_df['consistency_agreement_rate'].mean()
                    }
        
        return metrics

    def evaluate_completeness(self, 
                          results_df: pd.DataFrame, 
                          sample_size: int = 50,
                          random_seed: int = 42) -> pd.DataFrame:
        """
        ƒê√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß c·ªßa c√°c c√¢u tr·∫£ l·ªùi.
        
        Args:
            results_df (pd.DataFrame): DataFrame ch·ª©a k·∫øt qu·∫£ ƒë√°nh gi√°
            sample_size (int): S·ªë l∆∞·ª£ng m·∫´u c·∫ßn ƒë√°nh gi√°
            random_seed (int): Seed ng·∫´u nhi√™n cho vi·ªác l·∫•y m·∫´u
            
        Returns:
            pd.DataFrame: DataFrame ƒë√£ b·ªï sung ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß
        """
        # Ki·ªÉm tra xem c√≥ c√°c c·ªôt c·∫ßn thi·∫øt kh√¥ng
        required_cols = ['question_text', 'response']
        missing_cols = [col for col in required_cols if col not in results_df.columns]
        if missing_cols:
            logger.warning(f"Kh√¥ng th·ªÉ ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß: thi·∫øu c√°c c·ªôt {missing_cols}")
            return results_df
                
        # ƒê·∫£m b·∫£o c√≥ c√°c c·ªôt c·∫ßn thi·∫øt
        if 'completeness_score' not in results_df.columns:
            results_df['completeness_score'] = np.nan
            
        if 'completeness_evaluation' not in results_df.columns:
            results_df['completeness_evaluation'] = ''
            
        # L·ªçc c√°c h√†ng c√≥ c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi
        valid_rows = (
            ~results_df['question_text'].isna() & 
            ~results_df['response'].isna() &
            (results_df['response'] != '') &  # Th√™m ƒëi·ªÅu ki·ªán check chu·ªói r·ªóng
            results_df['completeness_score'].isna()  # Ch∆∞a ƒë∆∞·ª£c ƒë√°nh gi√°
        )
        
        valid_indices = results_df[valid_rows].index.tolist()
        
        if not valid_indices:
            logger.warning("Kh√¥ng c√≥ m·∫´u ph√π h·ª£p ƒë·ªÉ ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß")
            return results_df
            
        # L·∫•y m·∫´u ng·∫´u nhi√™n n·∫øu c·∫ßn
        np.random.seed(random_seed)
        if len(valid_indices) > sample_size:
            sample_indices = np.random.choice(valid_indices, size=sample_size, replace=False)
        else:
            sample_indices = valid_indices
            
        logger.info(f"ƒê√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß cho {len(sample_indices)} m·∫´u")
        
        # ƒê√°nh gi√° t·ª´ng m·∫´u
        for i, idx in enumerate(sample_indices):
            row = results_df.loc[idx]
            
            question = row['question_text']
            model_answer = row['response']
            
            if self.verbose:
                model_name = row['model_name'] if 'model_name' in row else row.get('model', 'unknown')
                prompt_type = row['prompt_type'] if 'prompt_type' in row else 'unknown'
                logger.info(f"ƒê√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß m·∫´u {i+1}/{len(sample_indices)}: model={model_name}, prompt={prompt_type}")
                
            try:
                # ƒê√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß
                eval_result = self._evaluate_single_completeness(question, model_answer)
                
                # C·∫≠p nh·∫≠t DataFrame
                results_df.at[idx, 'completeness_score'] = eval_result.get('score', 0.0)
                results_df.at[idx, 'completeness_evaluation'] = eval_result.get('explanation', '')
                    
            except Exception as e:
                logger.error(f"L·ªói khi ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß cho m·∫´u {idx}: {str(e)}")
                logger.error(traceback.format_exc())
        
        return results_df
    
    def _evaluate_single_completeness(self, question: str, model_answer: str) -> Dict[str, Any]:
        """
        ƒê√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß cho m·ªôt m·∫´u c√¢u h·ªèi/c√¢u tr·∫£ l·ªùi.
        
        Args:
            question (str): C√¢u h·ªèi
            model_answer (str): C√¢u tr·∫£ l·ªùi c·ªßa model
            
        Returns:
            Dict: K·∫øt qu·∫£ ƒë√°nh gi√° (ƒëi·ªÉm v√† gi·∫£i th√≠ch)
        """
        try:
            # C·∫Øt b·ªõt ƒë·ªô d√†i n·∫øu qu√° d√†i
            max_length = 4000  # Gi·ªõi h·∫°n ƒë·ªô d√†i ƒë·ªÉ tr√°nh v∆∞·ª£t qu√° context window
            if len(model_answer) > max_length:
                logger.warning(f"C·∫Øt b·ªõt c√¢u tr·∫£ l·ªùi ({len(model_answer)} -> {max_length} k√Ω t·ª±)")
                model_answer = model_answer[:max_length] + "..."
            
            # T·∫°o prompt ƒë√°nh gi√°
            if self.language.lower() == "vietnamese":
                eval_prompt = """
B·∫°n l√† m·ªôt chuy√™n gia ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß c·ªßa c√¢u tr·∫£ l·ªùi. H√£y ƒë√°nh gi√° xem c√¢u tr·∫£ l·ªùi c√≥ gi·∫£i quy·∫øt t·∫•t c·∫£ c√°c kh√≠a c·∫°nh c·ªßa c√¢u h·ªèi hay kh√¥ng.

C√ÇU H·ªéI:
{question}

C√ÇU TR·∫¢ L·ªúI:
{answer}

H√£y ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß c·ªßa c√¢u tr·∫£ l·ªùi theo thang ƒëi·ªÉm t·ª´ 0-10 (10 l√† ho√†n to√†n ƒë·∫ßy ƒë·ªß). X√°c ƒë·ªãnh c√°c kh√≠a c·∫°nh c·ªßa c√¢u h·ªèi v√† ki·ªÉm tra xem c√¢u tr·∫£ l·ªùi ƒë√£ ƒë·ªÅ c·∫≠p ƒë·∫øn t·∫•t c·∫£ c√°c kh√≠a c·∫°nh ƒë√≥ ch∆∞a.

ƒêi·ªÉm t√≠nh ƒë·∫ßy ƒë·ªß: ?/10

Ph√¢n t√≠ch chi ti·∫øt:
1. C√°c kh√≠a c·∫°nh c·ªßa c√¢u h·ªèi:
2. C√°c kh√≠a c·∫°nh ƒë∆∞·ª£c tr·∫£ l·ªùi:
3. C√°c kh√≠a c·∫°nh ch∆∞a ƒë∆∞·ª£c tr·∫£ l·ªùi (n·∫øu c√≥):
"""
            else:
                eval_prompt = """
You are an expert evaluating the completeness of answers. Assess whether the answer addresses all aspects of the question.

QUESTION:
{question}

ANSWER:
{answer}

Evaluate the completeness of the answer on a scale of 0-10 (10 being completely comprehensive). Identify the aspects of the question and check if the answer addresses all of them.

Completeness score: ?/10

Detailed analysis:
1. Aspects of the question:
2. Aspects addressed in the answer:
3. Aspects not addressed (if any):
"""
            
            # Format prompt
            eval_prompt = eval_prompt.format(question=question, answer=model_answer)
            
            # S·ª≠ d·ª•ng model API ƒë·ªÉ ƒë√°nh gi√°
            use_groq = self.reasoning_config.get("use_groq", True)
            if use_groq:
                # S·ª≠ d·ª•ng Groq API
                from core.model_interface import generate_text
                
                # L·∫•y t√™n model Groq
                model_name = "groq"
                config = {
                    "model": self.reasoning_config.get("models", {}).get(
                        "completeness_evaluation", "llama3-70b-8192"
                    ),
                    "temperature": 0.2,  # Th·∫•p ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n
                    "max_tokens": 1024
                }
                
                # G·ªçi API
                logger.debug("ƒê√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß b·∫±ng Groq API")
                response_text, stats = generate_text(model_name, eval_prompt, config)
                
                if stats.get("has_error", False):
                    logger.error(f"L·ªói khi g·ªçi Groq API: {stats.get('error_message')}")
                    # Fallback v·ªÅ gi√° tr·ªã m·∫∑c ƒë·ªãnh
                    return {
                        "score": 5.0,
                        "explanation": f"[L·ªói ƒë√°nh gi√°: {stats.get('error_message')}]"
                    }
            else:
                # TODO: S·ª≠ d·ª•ng model kh√°c n·∫øu c·∫ßn
                logger.warning("Ch·ªâ h·ªó tr·ª£ Groq API ƒë·ªÉ ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß")
                response_text = ""
                
            # Ph√¢n t√≠ch k·∫øt qu·∫£ ƒë√°nh gi√°
            eval_result = self._parse_completeness_evaluation(response_text)
            
            return eval_result
                
        except Exception as e:
            logger.error(f"L·ªói khi ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß: {str(e)}")
            logger.error(traceback.format_exc())
            
            # Tr·∫£ v·ªÅ gi√° tr·ªã m·∫∑c ƒë·ªãnh
            return {
                "score": 5.0,
                "explanation": f"[L·ªói ƒë√°nh gi√°: {str(e)}]"
            }
    
    def _parse_completeness_evaluation(self, eval_response: str) -> Dict[str, Any]:
        """
        Ph√¢n t√≠ch k·∫øt qu·∫£ ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß t·ª´ API.
        
        Args:
            eval_response (str): Ph·∫£n h·ªìi t·ª´ API
            
        Returns:
            Dict: Dictionary ch·ª©a ƒëi·ªÉm s·ªë v√† gi·∫£i th√≠ch
        """
        result = {
            "score": 5.0,  # Gi√° tr·ªã m·∫∑c ƒë·ªãnh
            "explanation": eval_response
        }
        
        if not eval_response:
            return result
            
        # T√¨m ƒëi·ªÉm ƒë√°nh gi√° t·ª´ ph·∫£n h·ªìi
        score_patterns = [
            r"(?:ƒëi·ªÉm|score)[^\d]*(\d+(?:\.\d+)?)/10",
            r"(?:ƒëi·ªÉm|score)[^\d]*:?[^\d]*(\d+(?:\.\d+)?)"
        ]
        
        # T√¨m ƒëi·ªÉm s·ªë
        for pattern in score_patterns:
            match = re.search(pattern, eval_response, re.IGNORECASE)
            if match:
                try:
                    score = float(match.group(1))
                    # Chuy·ªÉn v·ªÅ thang ƒëi·ªÉm 0-1
                    normalized_score = score / 10.0
                    result["score"] = max(0.0, min(1.0, normalized_score))
                    break
                except (ValueError, IndexError):
                    continue
        
        return result
    
    def _compute_completeness_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        T√≠nh to√°n metrics cho ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß.
        
        Args:
            df (pd.DataFrame): DataFrame ƒë√£ c√≥ ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß
            
        Returns:
            Dict: Metrics li√™n quan ƒë·∫øn t√≠nh ƒë·∫ßy ƒë·ªß
        """
        metrics = {}
        
        # L·ªçc c√°c d√≤ng c√≥ ƒë√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß
        completeness_df = df[~df['completeness_score'].isna()]
        
        if len(completeness_df) == 0:
            return metrics
        
        # 1. Metrics t·ªïng th·ªÉ
        metrics["overall"] = {
            "avg_completeness_score": completeness_df['completeness_score'].mean(),
            "high_completeness_rate": (completeness_df['completeness_score'] >= 0.8).mean()
        }
        
        # 2. Metrics theo model
        metrics["by_model"] = {}
        
        # X√°c ƒë·ªãnh c·ªôt model (model_name ho·∫∑c model)
        model_col = 'model_name' if 'model_name' in completeness_df.columns else 'model'
        
        if model_col in completeness_df.columns:
            for model in completeness_df[model_col].unique():
                model_df = completeness_df[completeness_df[model_col] == model]
                metrics["by_model"][model] = {
                    "avg_completeness_score": model_df['completeness_score'].mean(),
                    "high_completeness_rate": (model_df['completeness_score'] >= 0.8).mean()
                }
        
        # 3. Metrics theo prompt type
        metrics["by_prompt_type"] = {}
        if 'prompt_type' in completeness_df.columns:
            for prompt in completeness_df['prompt_type'].unique():
                prompt_df = completeness_df[completeness_df['prompt_type'] == prompt]
                metrics["by_prompt_type"][prompt] = {
                    "avg_completeness_score": prompt_df['completeness_score'].mean(),
                    "high_completeness_rate": (prompt_df['completeness_score'] >= 0.8).mean()
                }
        
        # 4. Metrics theo model v√† prompt type
        metrics["by_model_prompt"] = {}
        if model_col in completeness_df.columns and 'prompt_type' in completeness_df.columns:
            for model in completeness_df[model_col].unique():
                metrics["by_model_prompt"][model] = {}
                model_df = completeness_df[completeness_df[model_col] == model]
                
                for prompt in model_df['prompt_type'].unique():
                    prompt_df = model_df[model_df['prompt_type'] == prompt]
                    metrics["by_model_prompt"][model][prompt] = {
                        "avg_completeness_score": prompt_df['completeness_score'].mean(),
                        "high_completeness_rate": (prompt_df['completeness_score'] >= 0.8).mean()
                    }
        
        return metrics

    def evaluate_similarity(self, results_df: pd.DataFrame) -> pd.DataFrame:
        """
        T√≠nh to√°n c√°c metrics ƒëo l∆∞·ªùng ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa c√¢u tr·∫£ l·ªùi c·ªßa model v√† ƒë√°p √°n chu·∫©n.
        S·ª≠ d·ª•ng c√°c ph∆∞∆°ng ph√°p:
        - ROUGE (ƒëo l∆∞·ªùng s·ª± tr√πng l·∫∑p n-gram)
        - BLEU (ƒëo l∆∞·ªùng ƒë·ªô ch√≠nh x√°c n-gram)
        - Cosine similarity c·ªßa embeddings (n·∫øu c√≥ similarity_model)
        
        Args:
            results_df (pd.DataFrame): DataFrame ch·ª©a k·∫øt qu·∫£ ƒë√°nh gi√°
            
        Returns:
            pd.DataFrame: DataFrame ƒë√£ b·ªï sung c√°c metrics ƒëo l∆∞·ªùng ƒë·ªô t∆∞∆°ng ƒë·ªìng
        """
        # ƒê·∫£m b·∫£o c√≥ c·ªôt ƒë√°p √°n chu·∫©n v√† c√¢u tr·∫£ l·ªùi
        if 'correct_answer' not in results_df.columns or 'response' not in results_df.columns:
            logger.warning("Thi·∫øu c·ªôt 'correct_answer' ho·∫∑c 'response' ƒë·ªÉ t√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng")
            return results_df
        
        # Th√™m c√°c c·ªôt c·∫ßn thi·∫øt n·∫øu ch∆∞a c√≥
        for col in ['rouge_score', 'bleu_score', 'embedding_similarity']:
            if col not in results_df.columns:
                results_df[col] = np.nan
        
        # L·ªçc c√°c h√†ng c√≥ ƒë√°p √°n chu·∫©n v√† c√¢u tr·∫£ l·ªùi
        valid_rows = ~results_df['correct_answer'].isna() & ~results_df['response'].isna()
        
        if not valid_rows.any():
            logger.warning("Kh√¥ng c√≥ m·∫´u ph√π h·ª£p ƒë·ªÉ t√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng")
            return results_df
        
        # T√≠nh to√°n ROUGE v√† BLEU scores
        try:
            # N·∫øu ch∆∞a c√≥ th∆∞ vi·ªán ROUGE ho·∫∑c BLEU, c·∫ßn th√™m v√†o requirements.txt
            from rouge import Rouge
            import nltk
            from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
            
            # Download NLTK data n·∫øu c·∫ßn
            try:
                nltk.data.find('tokenizers/punkt')
            except LookupError:
                nltk.download('punkt')
            
            # Kh·ªüi t·∫°o ROUGE
            rouge = Rouge()
            smoothing = SmoothingFunction().method1
            
            # X·ª≠ l√Ω t·ª´ng h√†ng
            for idx in results_df[valid_rows].index:
                reference = results_df.at[idx, 'correct_answer']
                hypothesis = results_df.at[idx, 'response']
                
                # T√≠nh ROUGE score
                try:
                    rouge_scores = rouge.get_scores(hypothesis, reference)
                    # L·∫•y trung b√¨nh rouge-1, rouge-2, rouge-l f1-scores
                    rouge_f1 = (rouge_scores[0]['rouge-1']['f'] + 
                               rouge_scores[0]['rouge-2']['f'] + 
                               rouge_scores[0]['rouge-l']['f']) / 3
                    results_df.at[idx, 'rouge_score'] = rouge_f1
                except Exception as e:
                    logger.debug(f"L·ªói khi t√≠nh ROUGE score: {str(e)}")
                
                # T√≠nh BLEU score
                try:
                    # Tokenize c√¢u
                    reference_tokens = nltk.word_tokenize(reference.lower())
                    hypothesis_tokens = nltk.word_tokenize(hypothesis.lower())
                    
                    # T√≠nh BLEU (s·ª≠ d·ª•ng smoothing ƒë·ªÉ tr√°nh l·ªói khi kh√¥ng c√≥ n-gram kh·ªõp)
                    bleu_score = sentence_bleu([reference_tokens], hypothesis_tokens, 
                                              smoothing_function=smoothing)
                    results_df.at[idx, 'bleu_score'] = bleu_score
                except Exception as e:
                    logger.debug(f"L·ªói khi t√≠nh BLEU score: {str(e)}")
                
        except ImportError as e:
            logger.warning(f"Kh√¥ng th·ªÉ t√≠nh ROUGE/BLEU scores do thi·∫øu th∆∞ vi·ªán: {str(e)}")
        
        # T√≠nh to√°n embedding similarity n·∫øu c√≥ similarity model
        if self.similarity_model:
            # Th·ª±c hi·ªán t√≠nh to√°n embedding similarity
            try:
                for idx in results_df[valid_rows].index:
                    reference = results_df.at[idx, 'correct_answer']
                    hypothesis = results_df.at[idx, 'response']
                    
                    similarity = self._calculate_embedding_similarity(reference, hypothesis)
                    results_df.at[idx, 'embedding_similarity'] = similarity
            except Exception as e:
                logger.warning(f"L·ªói khi t√≠nh embedding similarity: {str(e)}")
        
        return results_df
    
    def _calculate_embedding_similarity(self, text1: str, text2: str) -> float:
        """
        T√≠nh to√°n cosine similarity gi·ªØa embeddings c·ªßa hai ƒëo·∫°n vƒÉn b·∫£n.
        
        Args:
            text1 (str): VƒÉn b·∫£n th·ª© nh·∫•t
            text2 (str): VƒÉn b·∫£n th·ª© hai
            
        Returns:
            float: Cosine similarity (0-1)
        """
        if not self.similarity_model:
            return 0.0
            
        try:
            # ƒê√¢y l√† ph·∫ßn tri·ªÉn khai t√πy thu·ªôc v√†o model v√† framework s·ª≠ d·ª•ng
            # V√≠ d·ª• v·ªõi sentence-transformers
            from sentence_transformers import SentenceTransformer
            import numpy as np
            from sklearn.metrics.pairwise import cosine_similarity
            
            # Ki·ªÉm tra xem similarity_model c√≥ ph·∫£i l√† ƒë∆∞·ªùng d·∫´n ho·∫∑c t√™n model kh√¥ng
            if isinstance(self.similarity_model, str):
                # Lazy loading model
                if not hasattr(self, '_embedding_model'):
                    self._embedding_model = SentenceTransformer(self.similarity_model)
                
                # T√≠nh embeddings
                embedding1 = self._embedding_model.encode([text1])[0]
                embedding2 = self._embedding_model.encode([text2])[0]
                
                # T√≠nh cosine similarity
                similarity = cosine_similarity([embedding1], [embedding2])[0][0]
                return float(similarity)
            else:
                # N·∫øu similarity_model ƒë√£ l√† instance c·ªßa model
                embedding1 = self.similarity_model.encode([text1])[0]
                embedding2 = self.similarity_model.encode([text2])[0]
                
                similarity = cosine_similarity([embedding1], [embedding2])[0][0]
                return float(similarity)
                
        except Exception as e:
            logger.error(f"L·ªói khi t√≠nh embedding similarity: {str(e)}")
            return 0.0
    
    def _compute_similarity_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        T√≠nh to√°n c√°c metrics li√™n quan ƒë·∫øn ƒë·ªô t∆∞∆°ng ƒë·ªìng v·ªõi ƒë√°p √°n chu·∫©n.
        
        Args:
            df (pd.DataFrame): DataFrame ƒë√£ c√≥ c√°c metrics ƒëo l∆∞·ªùng ƒë·ªô t∆∞∆°ng ƒë·ªìng
            
        Returns:
            Dict: Metrics li√™n quan ƒë·∫øn ƒë·ªô t∆∞∆°ng ƒë·ªìng
        """
        metrics = {}
        
        # Danh s√°ch c√°c c·ªôt similarity metrics
        similarity_cols = ['rouge_score', 'bleu_score', 'embedding_similarity']
        
        # L·ªçc c√°c h√†ng c√≥ √≠t nh·∫•t m·ªôt metric similarity ƒë∆∞·ª£c t√≠nh to√°n
        similarity_df = df[df[similarity_cols].notna().any(axis=1)]
        
        if len(similarity_df) == 0:
            return metrics
        
        # 1. Metrics t·ªïng th·ªÉ
        metrics["overall"] = {}
        for col in similarity_cols:
            if col in similarity_df.columns and similarity_df[col].notna().any():
                metrics["overall"][col] = similarity_df[col].mean()
        
        # 2. Metrics theo model
        metrics["by_model"] = {}
        for model in similarity_df['model'].unique():
            metrics["by_model"][model] = {}
            model_df = similarity_df[similarity_df['model'] == model]
            
            for col in similarity_cols:
                if col in model_df.columns and model_df[col].notna().any():
                    metrics["by_model"][model][col] = model_df[col].mean()
        
        # 3. Metrics theo prompt type
        metrics["by_prompt_type"] = {}
        for prompt in similarity_df['prompt_type'].unique():
            metrics["by_prompt_type"][prompt] = {}
            prompt_df = similarity_df[similarity_df['prompt_type'] == prompt]
            
            for col in similarity_cols:
                if col in prompt_df.columns and prompt_df[col].notna().any():
                    metrics["by_prompt_type"][prompt][col] = prompt_df[col].mean()
        
        # 4. Metrics theo model v√† prompt type
        metrics["by_model_prompt"] = {}
        for model in similarity_df['model'].unique():
            metrics["by_model_prompt"][model] = {}
            model_df = similarity_df[similarity_df['model'] == model]
            
            for prompt in model_df['prompt_type'].unique():
                metrics["by_model_prompt"][model][prompt] = {}
                prompt_df = model_df[model_df['prompt_type'] == prompt]
                
                for col in similarity_cols:
                    if col in prompt_df.columns and prompt_df[col].notna().any():
                        metrics["by_model_prompt"][model][prompt][col] = prompt_df[col].mean()
        
        # 5. T∆∞∆°ng quan gi·ªØa accuracy v√† c√°c metrics similarity
        if 'is_correct' in similarity_df.columns:
            metrics["correlation"] = {}
            
            for col in similarity_cols:
                if col in similarity_df.columns and similarity_df[col].notna().any():
                    corr = similarity_df[['is_correct', col]].corr().iloc[0, 1]
                    metrics["correlation"][f"{col}_vs_accuracy"] = corr
        
        return metrics
